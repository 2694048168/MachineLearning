{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第 8 章 提升方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boost\n",
    "**“装袋”（bagging）和“提升”（boost）**是构建组合模型的两种最主要的方法，所谓的组合模型是由多个基本模型构成的模型，组合模型的预测效果往往比任意一个基本模型的效果都要好。\n",
    "- 装袋：每个基本模型由从总体样本中随机抽样得到的不同数据集进行训练得到，通过重抽样得到不同训练数据集的过程称为装袋。\n",
    "- 提升：每个基本模型训练时的数据集采用不同权重，针对上一个基本模型分类错误的样本增加权重，使得新的模型重点关注误分类样本\n",
    "## AdaBoost\n",
    "AdaBoost是AdaptiveBoost的缩写，表明该算法是具有适应性的提升算法。\n",
    "1. 给每个训练样本（$x_{1},x_{2},….,x_{N}$）分配权重，初始权重$w_{1}$均为1/N。\n",
    "\n",
    "2. 针对带有权值的样本进行训练，得到模型$G_m$（初始模型为G1）。\n",
    "\n",
    "3. 计算模型$G_m$的误分率$e_m=\\sum_{i=1}^Nw_iI(y_i\\not= G_m(x_i))$\n",
    "\n",
    "4. 计算模型$G_m$的系数$\\alpha_m=0.5\\log[(1-e_m)/e_m]$\n",
    "\n",
    "5. 根据误分率e和当前权重向量$w_m$更新权重向量$w_{m+1}$。\n",
    "\n",
    "6. 计算组合模型$f(x)=\\sum_{m=1}^M\\alpha_mG_m(x_i)$的误分率。\n",
    "\n",
    "7.  当组合模型的误分率或迭代次数低于一定阈值，停止迭代；否则，回到步骤2）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection  import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of train data: X-(70, 2), y-(70,)\n",
      "The shape of test data: X-(30, 2), y-(30,)\n"
     ]
    }
   ],
   "source": [
    "# processing data for model.\n",
    "def create_data():\n",
    "  iris = load_iris()\n",
    "  df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "  df['label'] = iris.target\n",
    "  df.columns = ['sepal length', 'sepal width', 'petal length', 'petal width', 'label']\n",
    "  data = np.array(df.iloc[:100, [0,1,-1]])\n",
    "  return data[:,:2], data[:,-1]\n",
    "\n",
    "# loadding data\n",
    "X, y = create_data()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "print(\"The shape of train data: X-{}, y-{}\".format(X_train.shape, y_train.shape))\n",
    "print(\"The shape of test data: X-{}, y-{}\".format(X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of AdaBoost model: 0.45454545454545453\n"
     ]
    }
   ],
   "source": [
    "# the class of AdaBoost\n",
    "class AdaBoost:\n",
    "  def __init__(self, n_estimators=50, learning_rate=1.0):\n",
    "      self.clf_num = n_estimators\n",
    "      self.learning_rate = learning_rate\n",
    "\n",
    "  def init_args(self, datasets, labels):\n",
    "      self.X = datasets\n",
    "      self.Y = labels\n",
    "      self.M, self.N = datasets.shape\n",
    "      # 弱分类器数目和集合\n",
    "      self.clf_sets = []\n",
    "      # 初始化weights\n",
    "      self.weights = [1.0 / self.M] * self.M\n",
    "      # G(x)系数 alpha\n",
    "      self.alpha = []\n",
    "\n",
    "  def _G(self, features, labels, weights):\n",
    "      m = len(features)\n",
    "      error = 100000.0  # 无穷大\n",
    "      best_v = 0.0\n",
    "      # 单维features\n",
    "      features_min = min(features)\n",
    "      features_max = max(features)\n",
    "      n_step = (features_max - features_min + self.learning_rate) // self.learning_rate\n",
    "      # print('n_step:{}'.format(n_step))\n",
    "      direct, compare_array = None, None\n",
    "      for i in range(1, int(n_step)):\n",
    "          v = features_min + self.learning_rate * i\n",
    "          if v not in features:\n",
    "              # 误分类计算\n",
    "              compare_array_positive = np.array([1 if features[k] > v else -1 for k in range(m)])\n",
    "              weight_error_positive = sum([ weights[k] for k in range(m) if compare_array_positive[k] != labels[k] ])\n",
    "              compare_array_nagetive = np.array([-1 if features[k] > v else 1 for k in range(m)])\n",
    "              weight_error_nagetive = sum([ weights[k] for k in range(m) if compare_array_nagetive[k] != labels[k] ])\n",
    "              if weight_error_positive < weight_error_nagetive:\n",
    "                  weight_error = weight_error_positive\n",
    "                  _compare_array = compare_array_positive\n",
    "                  direct = 'positive'\n",
    "              else:\n",
    "                  weight_error = weight_error_nagetive\n",
    "                  _compare_array = compare_array_nagetive\n",
    "                  direct = 'nagetive'\n",
    "\n",
    "              # print('v:{} error:{}'.format(v, weight_error))\n",
    "              if weight_error < error:\n",
    "                  error = weight_error\n",
    "                  compare_array = _compare_array\n",
    "                  best_v = v\n",
    "      return best_v, direct, error, compare_array\n",
    "\n",
    "  # 计算alpha\n",
    "  def _alpha(self, error):\n",
    "      return 0.5 * np.log((1 - error) / error)\n",
    "\n",
    "  # 规范化因子\n",
    "  def _Z(self, weights, a, clf):\n",
    "      return sum([ weights[i] * np.exp(-1 * a * self.Y[i] * clf[i]) for i in range(self.M) ])\n",
    "\n",
    "  # 权值更新\n",
    "  def _w(self, a, clf, Z):\n",
    "      for i in range(self.M):\n",
    "          self.weights[i] = self.weights[i] * np.exp(-1 * a * self.Y[i] * clf[i]) / Z\n",
    "\n",
    "  # G(x)的线性组合\n",
    "  def _f(self, alpha, clf_sets):\n",
    "      pass\n",
    "\n",
    "  def G(self, x, v, direct):\n",
    "      if direct == 'positive':\n",
    "          return 1 if x > v else -1\n",
    "      else:\n",
    "          return -1 if x > v else 1\n",
    "\n",
    "  def fit(self, X, y):\n",
    "      self.init_args(X, y)\n",
    "      for epoch in range(self.clf_num):\n",
    "          best_clf_error, best_v, clf_result = 100000, None, None\n",
    "          # 根据特征维度, 选择误差最小的\n",
    "          for j in range(self.N):\n",
    "              features = self.X[:, j]\n",
    "              # 分类阈值，分类误差，分类结果\n",
    "              v, direct, error, compare_array = self._G(features, self.Y, self.weights)\n",
    "              if error < best_clf_error:\n",
    "                  best_clf_error = error\n",
    "                  best_v = v\n",
    "                  final_direct = direct\n",
    "                  clf_result = compare_array\n",
    "                  axis = j\n",
    "              # print('epoch:{}/{} feature:{} error:{} v:{}'.format(epoch, self.clf_num, j, error, best_v))\n",
    "              if best_clf_error == 0:\n",
    "                  break\n",
    "          # 计算G(x)系数a\n",
    "          a = self._alpha(best_clf_error)\n",
    "          self.alpha.append(a)\n",
    "          # 记录分类器\n",
    "          self.clf_sets.append((axis, best_v, final_direct))\n",
    "          # 规范化因子\n",
    "          Z = self._Z(self.weights, a, clf_result)\n",
    "          # 权值更新\n",
    "          self._w(a, clf_result, Z)\n",
    "          # print('classifier:{}/{} error:{:.3f} v:{} direct:{} a:{:.5f}'.format(epoch+1, self.clf_num, error, best_v, final_direct, a))\n",
    "          # print('weight:{}'.format(self.weights))\n",
    "          # print('\\n')\n",
    "\n",
    "  def predict(self, feature):\n",
    "      result = 0.0\n",
    "      for i in range(len(self.clf_sets)):\n",
    "          axis, clf_v, direct = self.clf_sets[i]\n",
    "          f_input = feature[axis]\n",
    "          result += self.alpha[i] * self.G(f_input, clf_v, direct)\n",
    "      # sign\n",
    "      return 1 if result > 0 else -1\n",
    "\n",
    "  def accuracy(self, X_test, y_test):\n",
    "      num_correct = 0\n",
    "      for i in range(len(X_test)):\n",
    "          feature = X_test[i]\n",
    "          if self.predict(feature) == y_test[i]:\n",
    "              num_correct += 1\n",
    "\n",
    "      return num_correct / len(X_test)\n",
    "    \n",
    "# -----------------------------------------\n",
    "# TEST 例 8.1\n",
    "X = np.arange(10).reshape(10, 1)\n",
    "y = np.array([1, 1, 1, -1, -1, -1, 1, 1, 1, -1])\n",
    "\n",
    "clf = AdaBoost(n_estimators=3, learning_rate=0.5)\n",
    "clf.fit(X, y)\n",
    "X, y = create_data()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "clf = AdaBoost(n_estimators=10, learning_rate=0.2)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"The accuracy of AdaBoost model: {}\".format(clf.accuracy(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of AdaBoost model: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "clf = AdaBoostClassifier(n_estimators=100, learning_rate=0.5)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"The accuracy of AdaBoost model: {}\".format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 习题 8.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始输出: [-1 -1 -1 -1 -1 -1  1  1 -1 -1]\n",
      "预测输出: [-1 -1 -1 -1 -1 -1  1  1 -1 -1]\n",
      "预测正确率：100.00%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# 加载训练数据\n",
    "X = np.array([[0, 1, 3], [0, 3, 1], [1, 2, 2], [1, 1, 3], [1, 2, 3], [0, 1, 2],\n",
    "              [1, 1, 2], [1, 1, 1], [1, 3, 1], [0, 2, 1]])\n",
    "y = np.array([-1, -1, -1, -1, -1, -1, 1, 1, -1, -1])\n",
    "\n",
    "clf = AdaBoostClassifier()\n",
    "clf.fit(X, y)\n",
    "y_predict = clf.predict(X)\n",
    "score = clf.score(X, y)\n",
    "print(\"原始输出:\", y)\n",
    "print(\"预测输出:\", y_predict)\n",
    "print(\"预测正确率：{:.2%}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "迭代次数: 8\n",
      "原始输出: [-1 -1 -1 -1 -1 -1  1  1 -1 -1]\n",
      "预测输出: [-1 -1 -1 -1 -1 -1  1  1 -1 -1]\n",
      "预测正确率：100.00%\n"
     ]
    }
   ],
   "source": [
    "# 习题 8.1\n",
    "import numpy as np\n",
    "\n",
    "# the class of AdaBoost\n",
    "class AdaBoost:\n",
    "  def __init__(self, X, y, tol=0.05, max_iter=10):\n",
    "      # 训练数据 实例\n",
    "      self.X = X\n",
    "      # 训练数据 标签\n",
    "      self.y = y\n",
    "      # 训练中止条件 right_rate>self.tol\n",
    "      self.tol = tol\n",
    "      # 最大迭代次数\n",
    "      self.max_iter = max_iter\n",
    "      # 初始化样本权重w\n",
    "      self.w = np.full((X.shape[0]), 1 / X.shape[0])\n",
    "      self.G = []  # 弱分类器\n",
    "\n",
    "  def build_stump(self):\n",
    "      \"\"\"\n",
    "      以带权重的分类误差最小为目标，选择最佳分类阈值\n",
    "      best_stump['dim'] 合适的特征所在维度\n",
    "      best_stump['thresh']  合适特征的阈值\n",
    "      best_stump['ineq']  树桩分类的标识lt,rt\n",
    "      \"\"\"\n",
    "      m, n = np.shape(self.X)\n",
    "      # 分类误差\n",
    "      e_min = np.inf\n",
    "      # 小于分类阈值的样本属于的标签类别\n",
    "      sign = None\n",
    "      # 最优分类树桩\n",
    "      best_stump = {}\n",
    "      for i in range(n):\n",
    "          range_min = self.X[:, i].min()  # 求每一种特征的最大最小值\n",
    "          range_max = self.X[:, i].max()\n",
    "          step_size = (range_max - range_min) / n\n",
    "          for j in range(-1, int(n) + 1):\n",
    "              thresh_val = range_min + j * step_size\n",
    "              # 计算左子树和右子树的误差\n",
    "              for inequal in ['lt', 'rt']:\n",
    "                  predict_vals = self.base_estimator(self.X, i, thresh_val, inequal)\n",
    "                  err_arr = np.array(np.ones(m))\n",
    "                  err_arr[predict_vals.T == self.y.T] = 0\n",
    "                  weighted_error = np.dot(self.w, err_arr)\n",
    "                  if weighted_error < e_min:\n",
    "                      e_min = weighted_error\n",
    "                      sign = predict_vals\n",
    "                      best_stump['dim'] = i\n",
    "                      best_stump['thresh'] = thresh_val\n",
    "                      best_stump['ineq'] = inequal\n",
    "      return best_stump, sign, e_min\n",
    "\n",
    "  def updata_w(self, alpha, predict):\n",
    "      \"\"\"\n",
    "      更新样本权重w\n",
    "      \"\"\"\n",
    "      # 以下2行根据公式8.4 8.5 更新样本权重\n",
    "      P = self.w * np.exp(-alpha * self.y * predict)\n",
    "      self.w = P / P.sum()\n",
    "\n",
    "  @staticmethod\n",
    "  def base_estimator(X, dimen, threshVal, threshIneq):\n",
    "      \"\"\"\n",
    "      计算单个弱分类器（决策树桩）预测输出\n",
    "      \"\"\"\n",
    "      ret_array = np.ones(np.shape(X)[0])  # 预测矩阵\n",
    "      # 左叶子 ，整个矩阵的样本进行比较赋值\n",
    "      if threshIneq == 'lt':\n",
    "          ret_array[X[:, dimen] <= threshVal] = -1.0\n",
    "      else:\n",
    "          ret_array[X[:, dimen] > threshVal] = -1.0\n",
    "      return ret_array\n",
    "\n",
    "  def fit(self):\n",
    "      \"\"\"\n",
    "      对训练数据进行学习\n",
    "      \"\"\"\n",
    "      G = 0\n",
    "      for i in range(self.max_iter):\n",
    "          best_stump, sign, error = self.build_stump()  # 获取当前迭代最佳分类阈值\n",
    "          alpha = 1 / 2 * np.log((1 - error) / error)  # 计算本轮弱分类器的系数\n",
    "          # 弱分类器权重\n",
    "          best_stump['alpha'] = alpha\n",
    "          # 保存弱分类器\n",
    "          self.G.append(best_stump)\n",
    "          # 以下3行计算当前总分类器（之前所有弱分类器加权和）分类效率\n",
    "          G += alpha * sign\n",
    "          y_predict = np.sign(G)\n",
    "          error_rate = np.sum(np.abs(y_predict - self.y)) / 2 / self.y.shape[0]\n",
    "          if error_rate < self.tol:  # 满足中止条件 则跳出循环\n",
    "              print(\"迭代次数:\", i + 1)\n",
    "              break\n",
    "          else:\n",
    "              self.updata_w(alpha, y_predict)  # 若不满足，更新权重，继续迭代\n",
    "\n",
    "  def predict(self, X):\n",
    "      \"\"\"\n",
    "      对新数据进行预测\n",
    "      \"\"\"\n",
    "      m = np.shape(X)[0]\n",
    "      G = np.zeros(m)\n",
    "      for i in range(len(self.G)):\n",
    "          stump = self.G[i]\n",
    "          # 遍历每一个弱分类器，进行加权\n",
    "          _G = self.base_estimator(X, stump['dim'], stump['thresh'], stump['ineq'])\n",
    "          alpha = stump['alpha']\n",
    "          G += alpha * _G\n",
    "      y_predict = np.sign(G)\n",
    "      return y_predict.astype(int)\n",
    "\n",
    "  def score(self, X, y):\n",
    "      \"\"\"对训练效果进行评价\"\"\"\n",
    "      y_predict = self.predict(X)\n",
    "      error_rate = np.sum(np.abs(y_predict - y)) / 2 / y.shape[0]\n",
    "      return 1 - error_rate\n",
    "    \n",
    "# --------------------------------\n",
    "# TEST\n",
    "clf = AdaBoost(X, y)\n",
    "clf.fit()\n",
    "y_predict = clf.predict(X)\n",
    "score = clf.score(X, y)\n",
    "print(\"原始输出:\", y)\n",
    "print(\"预测输出:\", y_predict)\n",
    "print(\"预测正确率：{:.2%}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
