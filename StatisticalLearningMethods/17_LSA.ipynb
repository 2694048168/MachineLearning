{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第十七章 潜在语义分析 Latent Semantic Analysis, LSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.单词向量空间模型通过单词的向量表示文本的语义内容。以单词-文本矩阵$X$为输入，其中每一行对应一个单词，每一列对应一个文本，每一个元素表示单词在文本中的频数或权值（如TF-IDF）\n",
    "$$X = \\left[ \\begin{array} { c c c c } { x _ { 11 } } & { x _ { 12 } } & { \\cdots } & { x _ { 1 n } } \\\\ { x _ { 21 } } & { x _ { 22 } } & { \\cdots } & { x _ { 2 n } } \\\\ { \\vdots } & { \\vdots } & { } & { \\vdots } \\\\ { x _ { m 1 } } & { x _ { m 2 } } & { \\cdots } & { x _ { m n } } \\end{array} \\right]$$\n",
    "单词向量空间模型认为，这个矩阵的每一列向量是单词向量，表示一个文本，两个单词向量的内积或标准化内积表示文本之间的语义相似度。\n",
    "\n",
    "2.话题向量空间模型通过话题的向量表示文本的语义内容。假设有话题文本矩阵$$Y = \\left[ \\begin{array} { c c c c } { y _ { 11 } } & { y _ { 12 } } & { \\cdots } & { y _ { 1 n } } \\\\ { y _ { 21 } } & { y _ { 22 } } & { \\cdots } & { y _ { 2 n } } \\\\ { \\vdots } & { \\vdots } & { } & { \\vdots } \\\\ { y _ { k 1 } } & { y _ { k 2 } } & { \\cdots } & { y _ { k n } } \\end{array} \\right]$$\n",
    "其中每一行对应一个话题，每一列对应一个文本每一个元素表示话题在文本中的权值。话题向量空间模型认为，这个矩阵的每一列向量是话题向量，表示一个文本，两个话题向量的内积或标准化内积表示文本之间的语义相似度。假设有单词话题矩阵$T$\n",
    "$$T = \\left[ \\begin{array} { c c c c } { t _ { 11 } } & { t _ { 12 } } & { \\cdots } & { t _ { 1 k } } \\\\ { t _ { 21 } } & { t _ { 22 } } & { \\cdots } & { t _ { 2 k } } \\\\ { \\vdots } & { \\vdots } & { } & { \\vdots } \\\\ { t _ { m 1 } } & { t _ { m 2 } } & { \\cdots } & { t _ { m k } } \\end{array} \\right]$$ \n",
    "其中每一行对应一个单词，每一列对应一个话题，每一个元素表示单词在话题中的权值。\n",
    "\n",
    "给定一个单词文本矩阵$X$\n",
    "$$X = \\left[ \\begin{array} { c c c c } { x _ { 11 } } & { x _ { 12 } } & { \\cdots } & { x _ { 1 n } } \\\\ { x _ { 21 } } & { x _ { 22 } } & { \\cdots } & { x _ { 2 n } } \\\\ { \\vdots } & { \\vdots } & { } & { \\vdots } \\\\ { x _ { m 1 } } & { x _ { m 2 } } & { \\cdots } & { x _ { m n } } \\end{array} \\right]$$\n",
    "\n",
    "潜在语义分析的目标是，找到合适的单词-话题矩阵$T$与话题文本矩阵$Y$，将单词文本矩阵$X$近似的表示为$T$与$Y$的乘积形式。\n",
    "$$X \\approx T Y$$\n",
    "\n",
    "等价地，潜在语义分析将文本在单词向量空间的表示X通过线性变换$T$转换为话题向量空间中的表示$Y$。\n",
    "\n",
    "潜在语义分析的关键是对单词-文本矩阵进行以上的矩阵因子分解（话题分析）\n",
    "\n",
    "3.潜在语义分析的算法是奇异值分解。通过对单词文本矩阵进行截断奇异值分解，得到\n",
    "$$X \\approx U _ { k } \\Sigma _ { k } V _ { k } ^ { T } = U _ { k } ( \\Sigma _ { k } V _ { k } ^ { T } )$$\n",
    "\n",
    "矩阵$U_k$表示话题空间，矩阵$( \\Sigma _ { k } V _ { k } ^ { T } )$是文本在话题空间的表示。\n",
    "\n",
    "4.非负矩阵分解也可以用于话题分析。非负矩阵分解将非负的单词文本矩阵近似分解成两个非负矩阵$W$和$H$的乘积，得到\n",
    "$$X \\approx W H$$\n",
    "\n",
    "矩阵$W$表示话题空间，矩阵$H$是文本在话题空间的表示。\n",
    "\n",
    "**非负矩阵分解**可以表为以下的最优化问题：(**推荐系统**中也常用到)\n",
    "\n",
    "**非负矩阵分解**（non-negative matrix factorization, NMF）是另一种矩阵的因子分解方法，其特点是分解的矩阵非负\n",
    "$$\\left. \\begin{array} { l } { \\operatorname { min } _ { W , H } \\| X - W H \\| ^ { 2 } } \\\\ { \\text { s.t. } W , H \\geq 0 } \\end{array} \\right.$$\n",
    "非负矩阵分解的算法是迭代算法。乘法更新规则的迭代算法，交替地对$W$和$H$进行更新。本质是梯度下降法，通过定义特殊的步长和非负的初始值，保证迭代过程及结果的矩阵$W$和$H$均为非负。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 0 0 0]\n",
      " [0 2 0 0]\n",
      " [0 0 1 0]\n",
      " [0 0 2 3]\n",
      " [0 0 0 1]\n",
      " [1 2 2 1]]\n",
      "-------------------------------------\n",
      "[[-7.84368672e-02  2.84423033e-01  8.94427191e-01  2.15138396e-01\n",
      "  -2.68931121e-02 -2.56794523e-01]\n",
      " [-1.56873734e-01  5.68846066e-01 -4.47213595e-01  4.30276793e-01\n",
      "  -5.37862243e-02 -5.13589047e-01]\n",
      " [-1.42622354e-01 -1.37930417e-02 -6.93889390e-17 -6.53519444e-01\n",
      "   4.77828945e-01 -5.69263078e-01]\n",
      " [-7.28804669e-01 -5.53499910e-01  5.55111512e-17  1.56161345e-01\n",
      "  -2.92700697e-01 -2.28957508e-01]\n",
      " [-1.47853320e-01 -1.75304609e-01 -6.93889390e-18  4.87733411e-01\n",
      "   8.24315866e-01  1.73283476e-01]\n",
      " [-6.29190197e-01  5.08166890e-01  1.11022302e-16 -2.81459486e-01\n",
      "   5.37862243e-02  5.13589047e-01]]\n",
      "-------------------------------------\n",
      "[[-1.75579600e-01 -3.51159201e-01 -6.38515454e-01 -6.61934313e-01]\n",
      " [ 3.91361272e-01  7.82722545e-01 -3.79579831e-02 -4.82432341e-01]\n",
      " [ 8.94427191e-01 -4.47213595e-01 -5.55111512e-17 -8.32667268e-17]\n",
      " [ 1.26523351e-01  2.53046702e-01 -7.68672366e-01  5.73674125e-01]]\n",
      "-------------------------------------\n",
      "[4.47696617 2.7519661  2.         1.17620428]\n",
      "-------------------------------------\n",
      "TruncatedSVD(n_components=3, n_iter=7, random_state=42)\n",
      "[0.39945801 0.34585056 0.18861789]\n",
      "0.9339264600284488\n",
      "[4.47696617 2.7519661  2.        ]\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "X = [[2, 0, 0, 0], [0, 2, 0, 0], [0, 0, 1, 0], [0, 0, 2, 3], [0, 0, 0, 1], [1, 2, 2, 1]]\n",
    "X = np.asarray(X);\n",
    "print(X)\n",
    "print(\"-------------------------------------\")\n",
    "U,sigma,VT=np.linalg.svd(X)\n",
    "print(U)\n",
    "print(\"-------------------------------------\")\n",
    "print(VT)\n",
    "print(\"-------------------------------------\")\n",
    "print(sigma)\n",
    "print(\"-------------------------------------\")\n",
    "\n",
    "# 截断奇异值分解\n",
    "svd = TruncatedSVD(n_components=3, n_iter=7, random_state=42)\n",
    "print(svd.fit(X))\n",
    "print(svd.explained_variance_ratio_)\n",
    "print(svd.explained_variance_ratio_.sum())\n",
    "print(svd.singular_values_)\n",
    "print(\"------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 非负矩阵分解 Non-negative matrix factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The W of Non-negative matrix factorization:\n",
      "[[0.00000000e+00 1.42285083e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 1.13699328e+00]\n",
      " [3.13991022e-01 4.96837125e-02 9.37183858e-02]\n",
      " [1.97590724e+00 3.88861164e-09 7.71791461e-09]\n",
      " [4.38949768e-01 0.00000000e+00 0.00000000e+00]\n",
      " [8.83492360e-01 7.51728001e-01 1.26925658e+00]]\n",
      "---------------------------------------------------------\n",
      "The W of Non-negative matrix factorization:\n",
      "[[5.69911553e-10 2.40782589e-09 1.09381312e+00 1.45701612e+00]\n",
      " [1.38785965e+00 1.32283338e-06 1.57038279e-01 1.08561689e-24]\n",
      " [3.12245864e-06 1.65233209e+00 4.19872298e-01 1.85556796e-25]]\n",
      "---------------------------------------------------------\n",
      "The Reconstruction matrix:\n",
      "[[1.97471726e+00 1.88219458e-06 2.23442046e-01 1.54467090e-24]\n",
      " [3.55021450e-06 1.87869049e+00 4.77391983e-01 2.10976831e-25]\n",
      " [6.89543128e-02 1.54853963e-01 3.90599498e-01 4.57489981e-01]\n",
      " [6.52296366e-09 1.75102037e-08 2.16127327e+00 2.87892870e+00]\n",
      " [2.50162544e-10 1.05691461e-09 4.80129015e-01 6.39556889e-01]\n",
      " [1.04329693e+00 2.09723437e+00 1.61735128e+00 1.28726261e+00]]\n",
      "---------------------------------------------------------\n",
      "The Original matrix:\n",
      "[[2 0 0 0]\n",
      " [0 2 0 0]\n",
      " [0 0 1 0]\n",
      " [0 0 2 3]\n",
      " [0 0 0 1]\n",
      " [1 2 2 1]]\n",
      "---------------------------------------------------------\n",
      "The Reconstruction error: 1.543964598235621\n"
     ]
    }
   ],
   "source": [
    "def inverse_transform(W, H):\n",
    "  # 重构\n",
    "  return W.dot(H)\n",
    "\n",
    "def loss(X, X_):\n",
    "  #计算重构误差\n",
    "  return ((X - X_) * (X - X_)).sum()\n",
    "\n",
    "# 算法 17.1\n",
    "# Non-negative matrix factorization\n",
    "class MyNMF:\n",
    "  def fit(self, X, k, t):\n",
    "    m, n = X.shape\n",
    "    W = np.random.rand(m, k)\n",
    "    W = W/W.sum(axis=0)\n",
    "    H = np.random.rand(k, n)\n",
    "    i = 1\n",
    "    while i < t:\n",
    "      W = W * X.dot(H.T) / W.dot(H).dot(H.T)\n",
    "      H = H * (W.T).dot(X) / (W.T).dot(W).dot(H)\n",
    "      i += 1\n",
    "    return W, H\n",
    "  \n",
    "# ----------------------------------------------\n",
    "# TEST\n",
    "model = MyNMF()\n",
    "W, H = model.fit(X, 3, 200)\n",
    "print(\"The W of Non-negative matrix factorization:\\n{}\".format(W))\n",
    "print(\"---------------------------------------------------------\")\n",
    "print(\"The W of Non-negative matrix factorization:\\n{}\".format(H))\n",
    "print(\"---------------------------------------------------------\")\n",
    "\n",
    "X_hat = inverse_transform(W, H)\n",
    "print(\"The Reconstruction matrix:\\n{}\".format(X_hat))\n",
    "print(\"---------------------------------------------------------\")\n",
    "print(\"The Original matrix:\\n{}\".format(X))\n",
    "print(\"---------------------------------------------------------\")\n",
    "\n",
    "print(\"The Reconstruction error: {}\".format(loss(X, X_hat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The W of Non-negative matrix factorization:\n",
      "[[0.         0.53849498 0.        ]\n",
      " [0.         1.07698996 0.        ]\n",
      " [0.69891361 0.         0.        ]\n",
      " [1.39782972 0.         1.97173859]\n",
      " [0.         0.         0.65783848]\n",
      " [1.39783002 1.34623756 0.65573258]]\n",
      "---------------------------------------------------------\n",
      "The W of Non-negative matrix factorization:\n",
      "[[0.00000000e+00 0.00000000e+00 1.43078959e+00 1.71761682e-03]\n",
      " [7.42810976e-01 1.48562195e+00 0.00000000e+00 3.30264644e-04]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 1.52030365e+00]]\n",
      "---------------------------------------------------------\n",
      "The Reconstruction matrix:\n",
      "[[3.99999983e-01 7.99999966e-01 0.00000000e+00 1.77845853e-04]\n",
      " [7.99999966e-01 1.59999993e+00 0.00000000e+00 3.55691707e-04]\n",
      " [0.00000000e+00 0.00000000e+00 9.99998311e-01 1.20046577e-03]\n",
      " [0.00000000e+00 0.00000000e+00 2.00000021e+00 3.00004230e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00011424e+00]\n",
      " [1.00000003e+00 2.00000007e+00 2.00000064e+00 9.99758185e-01]]\n",
      "---------------------------------------------------------\n",
      "The Original matrix:\n",
      "[[2 0 0 0]\n",
      " [0 2 0 0]\n",
      " [0 0 1 0]\n",
      " [0 0 2 3]\n",
      " [0 0 0 1]\n",
      " [1 2 2 1]]\n",
      "---------------------------------------------------------\n",
      "The Reconstruction error: 4.0000016725824565\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "model = NMF(n_components=3, init='random', max_iter=200, random_state=0)\n",
    "W = model.fit_transform(X)\n",
    "H = model.components_\n",
    "X_hat = inverse_transform(W, H)\n",
    "\n",
    "print(\"The W of Non-negative matrix factorization:\\n{}\".format(W))\n",
    "print(\"---------------------------------------------------------\")\n",
    "print(\"The W of Non-negative matrix factorization:\\n{}\".format(H))\n",
    "print(\"---------------------------------------------------------\")\n",
    "print(\"The Reconstruction matrix:\\n{}\".format(X_hat))\n",
    "print(\"---------------------------------------------------------\")\n",
    "print(\"The Original matrix:\\n{}\".format(X))\n",
    "print(\"---------------------------------------------------------\")\n",
    "print(\"The Reconstruction error: {}\".format(loss(X, X_hat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
