{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow\n",
    "### Deep Learning\n",
    "### Neural Networks\n",
    "### 环境搭建详细教程\n",
    "- Windows 10\n",
    "- Ubuntu 18.04/16.04/20.04\n",
    "- TensorFlow\n",
    "- PyTorch\n",
    "- CUDA cuDNN\n",
    "- Jupyter Lab & Jupyter Notebook\n",
    "- PyCharm\n",
    "\n",
    "[环境搭建详细教程——GitHub-Blog](https://2694048168.github.io/)\n",
    "\n",
    "[环境搭建详细教程——Gitee-Blog](http://weili_yzzcq.gitee.io/)\n",
    "\n",
    "[环境搭建详细教程——CSDN-Blog](https://blog.csdn.net/weixin_46782218)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent at b = 0, w = 0, error = 5565.107834483211\n",
      "Running\n",
      "After 1000 iterations b = 0.08893651993741346, w = 1.4777440851894448, error = 112.61481011613473\n"
     ]
    }
   ],
   "source": [
    "# 线性回归\n",
    "# make decisions: discrete 离散值     continuous 连续值\n",
    "# continuous prediction：input data（x）；prediction（f，参数 θ theta）；real data，ground-truth（y）\n",
    "# linear equation 线性方程组 y = w * x + b + epsilon\n",
    "# with noise  epsilon 噪声 高斯分布\n",
    "# find the W and b\n",
    "# loss = (WX + b - Y)**2\n",
    "# miniize loss \n",
    "# gradient descent GD 梯度下降 一维和二维可视化\n",
    "# loss surface\n",
    "# linear regression\n",
    "# logistic regression\n",
    "# classification\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# 生成 csv 文件里面的数据\n",
    "# data = []\n",
    "# for i in range(100):\n",
    "# \tx = np.random.uniform(3., 12.)\n",
    "# \t# mean=0, std=0.1\n",
    "# \teps = np.random.normal(0., 0.1)\n",
    "# \ty = 1.477 * x + 0.089 + eps\n",
    "# \tdata.append([x, y])\n",
    "# data = np.array(data)\n",
    "# print(data.shape, data)\n",
    "\n",
    "# cpmputer loss\n",
    "# Y = W*X +b\n",
    "def computer_error_for_line_given_points(b, w, points):\n",
    "    total_error = 0\n",
    "    for i in range(0, len(points)):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        # computer mean-squared-error\n",
    "        total_error += ((w * x + b) - y) ** 2\n",
    "        # total_error += (y - (w * x + b)) ** 2\n",
    "    # average loss for each point\n",
    "    return total_error / float(len(points))\n",
    "\n",
    "# computer Gradient and update\n",
    "def step_gradient(b_current, w_current, points, learning_rate):\n",
    "    b_gradient = 0\n",
    "    w_gradient = 0\n",
    "    N = float(len(points))\n",
    "    for i in range(0, len(points)):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        # grad_b = 2(wx + b - y)\n",
    "        b_gradient += (2/N) * ((w_current * x + b_current) - y)\n",
    "        # grad_w = 2(wx + b - y) * x\n",
    "        w_gradient += (2/N) * x * ((w_current * x + b_current) - y)\n",
    "    # update the grad_w, and the grad_b\n",
    "    new_b = b_current - (learning_rate * b_gradient)\n",
    "    new_w = w_current - (learning_rate * w_gradient)\n",
    "    return [new_b, new_w]\n",
    "\n",
    "# set w = w_new and loop\n",
    "def gradient_descent_runner(points, starting_b, starting_w, learning_rate, num_iterations):\n",
    "    b = starting_b\n",
    "    w = starting_w\n",
    "    # update for several times\n",
    "    for i in range(num_iterations):\n",
    "        b, w = step_gradient(b, w, np.array(points), learning_rate)\n",
    "    return [b, w]\n",
    "\n",
    "\n",
    "# running the linear regression model\n",
    "# loading the data for numpy from the data.cvs file\n",
    "# the data shape is 100 * 2 (row * column)\n",
    "points = np.genfromtxt(\"./datasets/data.csv\", delimiter=\",\")\n",
    "# init the Hyperparameter\n",
    "learning_rate = 0.0001\n",
    "initial_b = 0\n",
    "initial_w = 0\n",
    "num_iterations = 1000\n",
    "# show the initial Hyperparameter for linear regression model and the error\n",
    "print(\"Starting gradient descent at b = {0}, w = {1}, error = {2}\"\n",
    "      .format(initial_b, initial_w, computer_error_for_line_given_points(initial_b, initial_w, points)))\n",
    "\n",
    "print(\"Running\")\n",
    "# iterations = 1000,computer the Hyperparameter for linear regression model\n",
    "[b, w] = gradient_descent_runner(points, initial_b, initial_w, learning_rate, num_iterations)\n",
    "\n",
    "# and show the Hyperparameter for linear regression model and the error\n",
    "print(\"After {0} iterations b = {1}, w = {2}, error = {3}\"\n",
    "      .format(num_iterations, b, w, computer_error_for_line_given_points(b, w, points)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000, 10)\n",
      "0 0 2.145007\n",
      "0 100 0.9779064\n",
      "0 200 0.8011861\n",
      "1 0 0.682101\n",
      "1 100 0.68953323\n",
      "1 200 0.59687996\n",
      "2 0 0.5544969\n",
      "2 100 0.60021234\n",
      "2 200 0.5208171\n",
      "3 0 0.49560317\n",
      "3 100 0.5510172\n",
      "3 200 0.4770015\n",
      "4 0 0.45830703\n",
      "4 100 0.51574916\n",
      "4 200 0.44660082\n",
      "5 0 0.4310113\n",
      "5 100 0.48866108\n",
      "5 200 0.42332447\n",
      "6 0 0.40955436\n",
      "6 100 0.46667328\n",
      "6 200 0.40453193\n",
      "7 0 0.3919871\n",
      "7 100 0.44810033\n",
      "7 200 0.38870093\n",
      "8 0 0.37698773\n",
      "8 100 0.4321654\n",
      "8 200 0.3749433\n",
      "9 0 0.3639257\n",
      "9 100 0.41827995\n",
      "9 200 0.3629041\n",
      "10 0 0.35239592\n",
      "10 100 0.40607452\n",
      "10 200 0.35226172\n",
      "11 0 0.34209\n",
      "11 100 0.39520663\n",
      "11 200 0.34277108\n",
      "12 0 0.33278885\n",
      "12 100 0.38533977\n",
      "12 200 0.33422118\n",
      "13 0 0.3244313\n",
      "13 100 0.37634808\n",
      "13 200 0.32637382\n",
      "14 0 0.3167131\n",
      "14 100 0.3681502\n",
      "14 200 0.3191441\n",
      "15 0 0.30964595\n",
      "15 100 0.36062518\n",
      "15 200 0.31245127\n",
      "16 0 0.30311885\n",
      "16 100 0.3536573\n",
      "16 200 0.30619872\n",
      "17 0 0.2970287\n",
      "17 100 0.34722012\n",
      "17 200 0.3003191\n",
      "18 0 0.29140472\n",
      "18 100 0.34124565\n",
      "18 200 0.29482606\n",
      "19 0 0.28617287\n",
      "19 100 0.33563197\n",
      "19 200 0.28969684\n",
      "20 0 0.28127086\n",
      "20 100 0.33038154\n",
      "20 200 0.28484973\n",
      "21 0 0.27660605\n",
      "21 100 0.32542306\n",
      "21 200 0.280265\n",
      "22 0 0.27215695\n",
      "22 100 0.32068706\n",
      "22 200 0.27590075\n",
      "23 0 0.26799193\n",
      "23 100 0.31619698\n",
      "23 200 0.27178058\n",
      "24 0 0.26402566\n",
      "24 100 0.3118977\n",
      "24 200 0.26785937\n",
      "25 0 0.26028073\n",
      "25 100 0.307815\n",
      "25 200 0.26411417\n",
      "26 0 0.25673878\n",
      "26 100 0.3039063\n",
      "26 200 0.26061907\n",
      "27 0 0.25335112\n",
      "27 100 0.3001855\n",
      "27 200 0.25728583\n",
      "28 0 0.25014195\n",
      "28 100 0.29665297\n",
      "28 200 0.25410765\n",
      "29 0 0.2471003\n",
      "29 100 0.29328233\n",
      "29 200 0.25104755\n"
     ]
    }
   ],
   "source": [
    "# Image Classification\n",
    "# hand-written digits recognition\n",
    "# MNIST: 7,000 images per category = 70,000\n",
    "# train / test splitting = 60k / 10k\n",
    "# [28, 28, 1] = [rows,columns, gray_value]\n",
    "# 28 * 28 = 784 转换为一维数组\n",
    "# input and output\n",
    "# coding the features for data: one-hot\n",
    "# regression VS classification\n",
    "# y = w * x + b\n",
    "# y 属于空间 R^d\n",
    "# out = X @ W + b\n",
    "# out:[0.1, 0.8, 0.02,.0.08] = 1 概率，最大值概率称之为置信度\n",
    "# pred = argmax(out)\n",
    "\n",
    "# out = X @ W + b\n",
    "# X = [b, 784]\n",
    "# W = [784, 10]\n",
    "# b = [10]\n",
    "\n",
    "# it is linear !\n",
    "# out = X @ W + b\n",
    "# 变成 non-linear !\n",
    "# out = f(X @ W + b)\n",
    "# 引入非线性因子，这个 f 函数称之为激活函数，\n",
    "# f = ReLu function and sigmoid function\n",
    "# out = relu(X @ W + b)\n",
    "\n",
    "# it is simple\n",
    "# out = relu(X @ W + b)\n",
    "# 添加隐藏层 hide layer\n",
    "# h1 = relu(X @ W1 + b1)\n",
    "# h2 = relu(X @ W2 + b2)\n",
    "# out = relu(X @ W3 + b3)\n",
    "\n",
    "# particularly\n",
    "# 每一层都是类似降维的过程，直到从 [1, 784] ---- [1, 10]\n",
    "# X = [v1, v2, ... , v784]\n",
    "# h1 = relu(X @ W1 + b1)\n",
    "#    W1: [784, 512]\n",
    "#    b1: [1, 512]\n",
    "# h2 = relu(X @ W2 + b2)\n",
    "#    W2: [512, 256]\n",
    "#    b2: [1, 256]\n",
    "# out = relu(X @ W3 + b3)\n",
    "#    W2: [256, 10]\n",
    "#    b2: [1, 10]\n",
    "\n",
    "# loss\n",
    "# out:[1, 10]\n",
    "# Y/label: 0-9 （one-hot）\n",
    "# euclidean distance 计算欧式距离 使得 out 接近 label   MSE\n",
    "# loss = (Y - out) **2\n",
    "\n",
    "# in a nutshell\n",
    "# out = relu{relu{relu[X @ W1 + b1] @ W2 + b2 } @ W3 + b3}\n",
    "# pred = argmax(out)\n",
    "# loss = MSE(out, label)\n",
    "# minimize loss\n",
    "# [w1_new, b2_new, w2_new, b2_new, w3_new, b3_new]\n",
    "\n",
    "# Deep Learning\n",
    "# classification precedure\n",
    "# step1 compute [h1, h2, out]\n",
    "# step2 compute loss\n",
    "# step3 compute gradient and update [w1_new, b2_new, w2_new, b2_new, w3_new, b3_new]\n",
    "# step4 loop\n",
    "# need TensorFlow\n",
    "\n",
    "# 以下两条语句，使得 TensorFlow 少打印出一些无关紧要的信息\n",
    "# import  os\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "\n",
    "# step 0 loading data X and Y\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, optimizers\n",
    "\n",
    "(x, y), (x_val, y_val) = datasets.mnist.load_data() \n",
    "x = tf.convert_to_tensor(x, dtype=tf.float32) / 255.\n",
    "y = tf.convert_to_tensor(y, dtype=tf.int32)\n",
    "y = tf.one_hot(y, depth=10)\n",
    "print(x.shape, y.shape)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "# batch 概念\n",
    "train_dataset = train_dataset.batch(200)\n",
    "    \n",
    "# step 0 model NN\n",
    "model = tf.keras.Sequential([layers.Dense(512, activation=\"relu\"),\n",
    "                          layers.Dense(256, activation=\"relu\"),\n",
    "                          layers.Dense(10)])\n",
    "\n",
    "optimizer = optimizers.SGD(learning_rate=0.001)\n",
    "\n",
    "# 对整个训练集 训练 30 次，即就是 epoch = 30 \n",
    "# 将训练集数据划分为每次读取 200 ，即就是 batch = 200\n",
    "# 那么 step = train_data / batch = 60k / 200 = 300\n",
    "# 每训练 100 个样本数据，就查看一次训练情况 loss\n",
    "# 实际显示的 loss 情况次数 = epoch * (step / 100) = 30 * (300/100) = 90\n",
    "\n",
    "# step4 loop\n",
    "# 对一个数据集 dataset 训练一次称之为 epoch\n",
    "def train_epoch(epoch):\n",
    "    # 对一个 batch 训练一次称之为 step\n",
    "    for step, (x, y) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # [b， 28， 28] => [b, 784]\n",
    "            x = tf.reshape(x, (-1, 28*28))\n",
    "            # step 1 compute output\n",
    "            # [b, 784] => [b, 10]\n",
    "            out = model(x)\n",
    "            # step2 compute loss\n",
    "            loss = tf.reduce_sum(tf.square(out - y)) / x.shape[0]\n",
    "            \n",
    "        # step 3 optimize and update w1, w2, w3, b1, b2, b3\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        # w_new = w - lr * grad\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables)) \n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            # print(epoch, step, 'loss:', loss.numpy())\n",
    "            print(\"第 {0} 次 epoch，第 {1} step，该次训练 loss = {2}\".format(epoch, step+100, loss.numpy()))\n",
    "\n",
    "# 对整个数据集训练多次 30 次\n",
    "for epoch in range(30):\n",
    "    train_epoch(epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=1>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TensorFlow 数据类型\n",
    "# data container: list, np.array, tf.Tensor\n",
    "# what is Tensor\n",
    "# - scalar 标量 1.1\n",
    "# - vector 向量 [1.1], [1.1, 2.2]\n",
    "# - matrix 矩阵 [[1.1, 2.2], [3.3, 4.4], [5.5, 6.6]]\n",
    "# - tensor rank > 2 矩阵的秩\n",
    "# TF is a computing lib\n",
    "# - int, float, double, bool, string\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# create the int, float, double, bool, string\n",
    "tf.constant(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=1.0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.constant(1.2, dtype=tf.int32)\n",
    "# TypeError: Cannot convert 1.2 to EagerTensor of dtype int32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True, False])>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant([True, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'Hello World'>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/job:localhost/replica:0/task:0/device:CPU:0'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensor Property 常见属性\n",
    "with tf.device(\"cpu\"):\n",
    "    const_cpu = tf.constant([1])\n",
    "\n",
    "with tf.device(\"gpu\"):\n",
    "    const_gpu = tf.range(4)\n",
    "    \n",
    "# device 属性，查看 Tensor 在哪一个设备上 CPU or GPU\n",
    "const_cpu.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/job:localhost/replica:0/task:0/device:CPU:0'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const_gpu.device\n",
    "# 因为本地没有安装 GPU 加速驱动 CUDA 和 cuDNN\n",
    "# 无法提供 GPU 加速环境，故此工作环境依旧还在 CPU 上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/job:localhost/replica:0/task:0/device:CPU:0'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 相互转换 \n",
    "# const_cpu_to_gpu = const_cpu.gpu()\n",
    "# const_cpu_to_gpu.device\n",
    "\n",
    "const_gpu_to_cpu = tf.identity(const_gpu)\n",
    "const_gpu_to_cpu.device\n",
    "\n",
    "# tensor 的计算必须在同一设备进行，全部都是 CPU 或者全部都是 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy 属性，将 Tensor 转换为 numpy\n",
    "# numpy 是在 CPU 上的\n",
    "const_gpu.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看 Tensor 的 shape 维度\n",
    "# ndim 属性，返回 Tensor 的维度信息\n",
    "# rank 方法，返回 Tensor 的维度信息\n",
    "const_cpu.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=1>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.rank(const_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=3>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.rank(tf.ones([3, 2, 4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 判断是否 Tensor\n",
    "tf.is_tensor(const_cpu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=int32, numpy=array([0, 1, 2, 3, 4])>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 转换为 Tensor\n",
    "const = np.arange(5)\n",
    "const_tensor = tf.convert_to_tensor(const)\n",
    "const_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf.int32"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert dtype\n",
    "# 数值类型之间的转换\n",
    "const_cpu.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=float32, numpy=array([0., 1., 2., 3., 4.], dtype=float32)>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.cast(const_tensor, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=float64, numpy=array([0., 1., 2., 3., 4.])>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.cast(const_tensor, dtype=tf.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=int32, numpy=array([0, 1, 2, 3, 4])>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.cast(const_tensor, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=bool, numpy=array([False,  True])>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# int <==> bool\n",
    "bool_num = tf.constant([0, 1])\n",
    "bool_num_bool = tf.cast(bool_num, dtype=tf.bool)\n",
    "bool_num_bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=int32, numpy=array([0, 1])>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.cast(bool_num_bool, tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=int32, numpy=array([0, 1, 2, 3, 4])>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf.Variable\n",
    "# 一般是需要优化的变量参数\n",
    "a = tf.range(5)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(5,) dtype=int32, numpy=array([0, 1, 2, 3, 4])>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = tf.Variable(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf.int32"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Variable:0'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.trainable\n",
    "# 需要梯度信息，可以自动求导\n",
    "# 神经网络的参数需求"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.is_tensor(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建 Tensor\n",
    "# from numpy, list\n",
    "# zeros, ones\n",
    "# fill\n",
    "# random\n",
    "# constant\n",
    "# Applicatin\n",
    "\n",
    "# meta-learning\n",
    "\n",
    "# Tensor 索引和切片\n",
    "# selecetive indexing\n",
    "# tf.gather() , tf.gather_nd(), tf.boolean_mask()\n",
    "\n",
    "# 维度变换\n",
    "# shape, ndim\n",
    "# reshape\n",
    "# expand_dims, squeeze\n",
    "# transpose\n",
    "# broadcast_to\n",
    "\n",
    "# 数学计算\n",
    "# +, -, *, /, **, pow, square, sqrt, //, %, exp, log, @, matmul\n",
    "# element_wise, matrix_wise, dim_wise, \n",
    "# 元素级计算，矩阵级计算，维度级计算\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets: (60000, 28, 28) (60000,) 0 255\n",
      "(60000, 28, 28) (60000,) <dtype: 'float32'> <dtype: 'int32'>\n",
      "tf.Tensor(0.0, shape=(), dtype=float32) tf.Tensor(255.0, shape=(), dtype=float32)\n",
      "tf.Tensor(0, shape=(), dtype=int32) tf.Tensor(9, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# import  os\n",
    "# 复制 0， 1， 2，打印 c++ 一些信息\n",
    "# 0 打印全部信息\n",
    "# 2 只打印 error 信息\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, optimizers, Sequential, metrics\n",
    "\n",
    "# loading the dataset MNIST\n",
    "(x, y), _ = datasets.mnist.load_data()\n",
    "print('datasets:', x.shape, y.shape, x.min(), x.max())\n",
    "\n",
    "# x : [0-255] -> [0-1.]\n",
    "x = tf.convert_to_tensor(x, dtype=tf.float32) / 255.\n",
    "y = tf.convert_to_tensor(y, dtype=tf.int32)\n",
    "\n",
    "print(x.shape, y.shape, x.dtype, y.dtype)\n",
    "print(tf.reduce_min(x), tf.reduce_max(x))\n",
    "print(tf.reduce_min(y), tf.reduce_max(y))\n",
    "\n",
    "# set the batch\n",
    "train_db = tf.data.Dataset.from_tensor_slices((x, y)).batch(128)\n",
    "\n",
    "# set the iteration\n",
    "train_iter = iter(train_db)\n",
    "sample = next(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,) <dtype: 'float32'> <dtype: 'int32'>\n",
      "tf.Tensor(0.0, shape=(), dtype=float32) tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "tf.Tensor(0, shape=(), dtype=int32) tf.Tensor(9, shape=(), dtype=int32)\n",
      "batch (128, 28, 28) (128,)\n",
      "0 0 loss 0.42511042952537537\n",
      "0 100 loss 0.1789942979812622\n",
      "0 200 loss 0.16580988466739655\n",
      "0 300 loss 0.1560211181640625\n",
      "0 400 loss 0.1542573869228363\n",
      "1 0 loss 0.1479978859424591\n",
      "1 100 loss 0.1321682631969452\n",
      "1 200 loss 0.13824641704559326\n",
      "1 300 loss 0.13372421264648438\n",
      "1 400 loss 0.13392488658428192\n",
      "2 0 loss 0.12848277390003204\n",
      "2 100 loss 0.11817771196365356\n",
      "2 200 loss 0.12388703972101212\n",
      "2 300 loss 0.11938042938709259\n",
      "2 400 loss 0.12050290405750275\n",
      "3 0 loss 0.11577292531728745\n",
      "3 100 loss 0.10877491533756256\n",
      "3 200 loss 0.11369021236896515\n",
      "3 300 loss 0.1092468723654747\n",
      "3 400 loss 0.11090860515832901\n",
      "4 0 loss 0.10678441822528839\n",
      "4 100 loss 0.10189224779605865\n",
      "4 200 loss 0.10604660212993622\n",
      "4 300 loss 0.10167856514453888\n",
      "4 400 loss 0.10369563102722168\n",
      "5 0 loss 0.0999135747551918\n",
      "5 100 loss 0.09662076085805893\n",
      "5 200 loss 0.10006420314311981\n",
      "5 300 loss 0.0957786813378334\n",
      "5 400 loss 0.09803612530231476\n",
      "6 0 loss 0.0944289118051529\n",
      "6 100 loss 0.09243079274892807\n",
      "6 200 loss 0.09519816935062408\n",
      "6 300 loss 0.09105338156223297\n",
      "6 400 loss 0.09347577393054962\n",
      "7 0 loss 0.08995360136032104\n",
      "7 100 loss 0.08898767083883286\n",
      "7 200 loss 0.09114708006381989\n",
      "7 300 loss 0.08714453876018524\n",
      "7 400 loss 0.0896892100572586\n",
      "8 0 loss 0.08621992915868759\n",
      "8 100 loss 0.08607572317123413\n",
      "8 200 loss 0.08771105855703354\n",
      "8 300 loss 0.08386079967021942\n",
      "8 400 loss 0.08645957708358765\n",
      "9 0 loss 0.08304877579212189\n",
      "9 100 loss 0.08354615420103073\n",
      "9 200 loss 0.08477957546710968\n",
      "9 300 loss 0.08105111122131348\n",
      "9 400 loss 0.08372876793146133\n"
     ]
    }
   ],
   "source": [
    "# 前向传播 forward\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets\n",
    "\n",
    "# loading data\n",
    "(x, y), _ = datasets.mnist.load_data()\n",
    "\n",
    "# convert to Tensor\n",
    "# x : [0-255] -> [0-1.]\n",
    "x = tf.convert_to_tensor(x, dtype=tf.float32) / 255.\n",
    "y = tf.convert_to_tensor(y, dtype=tf.int32)\n",
    "\n",
    "# show the info\n",
    "print(x.shape, y.shape, x.dtype, y.dtype)\n",
    "print(tf.reduce_min(x), tf.reduce_max(x))\n",
    "print(tf.reduce_min(y), tf.reduce_max(y))\n",
    "\n",
    "# set the dataset and split the batch\n",
    "train_db = tf.data.Dataset.from_tensor_slices((x, y)).batch(128)\n",
    "train_iter = iter(train_db)\n",
    "sample = next(train_iter)\n",
    "print(\"batch\", sample[0].shape, sample[1].shape)\n",
    "\n",
    "# 权值\n",
    "# [b, 784] => [b, 256] => [b, 128] => [b, 10]\n",
    "# [dim_in, dim_out], [dim_out]\n",
    "# tf.Variable 才会自动记录梯度信息，tf.Tensor 不会记录梯度信息，故此值为 None，类型为 NoneType\n",
    "# 需要给一个好点的初始值，否则容易 loss 爆炸，出现 nan(not a number) 情况,stddev=0.1 方差给小一点\n",
    "w1 = tf.Variable(tf.random.truncated_normal([784, 256], stddev=0.1))\n",
    "b1 = tf.Variable(tf.zeros([256]))\n",
    "w2 = tf.Variable(tf.random.truncated_normal([256, 128], stddev=0.1))\n",
    "b2 = tf.Variable(tf.zeros([128]))\n",
    "w3 = tf.Variable(tf.random.truncated_normal([128, 10], stddev=0.1))\n",
    "b3 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# 学习率 ，步长 10^-3\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# 对数据集迭代 10 次\n",
    "for epoch in range(10):\n",
    "    # 对数据集一次循环\n",
    "    for step, (x, y) in enumerate(train_db):\n",
    "        # x [128, 28, 28]\n",
    "        # y [128]\n",
    "        # 进行维度转换 [128, 28, 28] => [b, 28*28]\n",
    "        x = tf.reshape(x, [-1, 28*28])   \n",
    "    \n",
    "        # 自动求导过程,梯度\n",
    "        with tf.GradientTape() as tape:\n",
    "            # x [b, 28*28]\n",
    "            # h1 = x@w1 + b1\n",
    "            # [b, 784]@[784, 256] + [256] => [b, 256] + [256] => [b, 256] + [b, 256]\n",
    "            h1 = x@w1 + tf.broadcast_to(b1, [x.shape[0], 256])\n",
    "            # 添加非线性，relu 激活函数\n",
    "            h1 = tf.nn.relu(h1)\n",
    "            # [b, 256] => [b, 128]\n",
    "            h2 = h1@w2 + b2\n",
    "            h2 = tf.nn.relu(h2)\n",
    "            # [b, 128] => [b, 10]\n",
    "            out = h2@w3 +b3\n",
    "        \n",
    "            # compute loss\n",
    "            # out [b, 10]\n",
    "            # y [b] => [b, 10]\n",
    "            y_one_hot = tf.one_hot(y, depth=10)\n",
    "        \n",
    "            # MSE = mean((y - out)**2)\n",
    "            loss = tf.square(y_one_hot - out)\n",
    "            # mean: scalar\n",
    "            loss = tf.reduce_mean(loss)\n",
    "    \n",
    "        # compute gradients\n",
    "        grads = tape.gradient(loss, [w1, b1, w2, b2, w3, b3])\n",
    "        # print(grads)\n",
    "        # w1 = w1 - lr * w1_grad\n",
    "        # 原地更新，否则就返回 tf.Tensor 类型了，无法进行下一个梯度更新\n",
    "        # w1 = w1 - learning_rate * grads[0]\n",
    "        w1.assign_sub(learning_rate * grads[0])\n",
    "        b1.assign_sub(learning_rate * grads[1])\n",
    "        w2.assign_sub(learning_rate * grads[2])\n",
    "        b2.assign_sub(learning_rate * grads[3])\n",
    "        w3.assign_sub(learning_rate * grads[4])\n",
    "        b3.assign_sub(learning_rate * grads[5])\n",
    "    \n",
    "        if step % 100 == 0:\n",
    "            print(epoch, step, \"loss\", float(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor 张量的合并和分割\n",
    "# tf.concat 合并\n",
    "# tf.split  分割\n",
    "# tf.stack 合并\n",
    "# tf.unstack 分割\n",
    "\n",
    "# 数据统计\n",
    "# tf.norm 范数概念\n",
    "# tf.reduce_min/max/mean\n",
    "# tf.argmax/argminx\n",
    "# tf.equal\n",
    "# tf.unique\n",
    "\n",
    "# 张量排序 Tensor 排序\n",
    "# tf.sort、tf.argsort\n",
    "# top_k\n",
    "# top_k compute of accuracy\n",
    "\n",
    "# 填充与复制\n",
    "# pad\n",
    "# tile\n",
    "# broadcast_to\n",
    "\n",
    "# 张量限幅\n",
    "# clip_by_value\n",
    "# relu\n",
    "# clip_by_norm\n",
    "# gradient clipping\n",
    "\n",
    "# 高阶操作\n",
    "# where\n",
    "# scatter_nd\n",
    "# meshgrid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,) <dtype: 'float32'> <dtype: 'int32'>\n",
      "tf.Tensor(0.0, shape=(), dtype=float32) tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "tf.Tensor(0, shape=(), dtype=int32) tf.Tensor(9, shape=(), dtype=int32)\n",
      "batch (128, 28, 28) (128,)\n",
      "0 0 loss 0.38115018606185913\n",
      "0 100 loss 0.2140667885541916\n",
      "0 200 loss 0.17352941632270813\n",
      "0 300 loss 0.17252981662750244\n",
      "0 400 loss 0.1654943823814392\n",
      "test acc :  0.1447\n",
      "1 0 loss 0.155382439494133\n",
      "1 100 loss 0.1564989537000656\n",
      "1 200 loss 0.1401979923248291\n",
      "1 300 loss 0.14494988322257996\n",
      "1 400 loss 0.1413814276456833\n",
      "test acc :  0.1958\n",
      "2 0 loss 0.13282153010368347\n",
      "2 100 loss 0.13779962062835693\n",
      "2 200 loss 0.12248848378658295\n",
      "2 300 loss 0.12837831676006317\n",
      "2 400 loss 0.12546643614768982\n",
      "test acc :  0.2617\n",
      "3 0 loss 0.11781013011932373\n",
      "3 100 loss 0.1250159740447998\n",
      "3 200 loss 0.11026396602392197\n",
      "3 300 loss 0.11668787151575089\n",
      "3 400 loss 0.11417224258184433\n",
      "test acc :  0.3339\n",
      "4 0 loss 0.10710453987121582\n",
      "4 100 loss 0.11570942401885986\n",
      "4 200 loss 0.10131652653217316\n",
      "4 300 loss 0.10808267444372177\n",
      "4 400 loss 0.105748750269413\n",
      "test acc :  0.3906\n",
      "5 0 loss 0.09909947216510773\n",
      "5 100 loss 0.10840018093585968\n",
      "5 200 loss 0.09452857077121735\n",
      "5 300 loss 0.10141358524560928\n",
      "5 400 loss 0.09917517006397247\n",
      "test acc :  0.4394\n",
      "6 0 loss 0.0928255096077919\n",
      "6 100 loss 0.10247411578893661\n",
      "6 200 loss 0.08917931467294693\n",
      "6 300 loss 0.09608200192451477\n",
      "6 400 loss 0.09400992095470428\n",
      "test acc :  0.4765\n",
      "7 0 loss 0.08775044977664948\n",
      "7 100 loss 0.09758851677179337\n",
      "7 200 loss 0.08487435430288315\n",
      "7 300 loss 0.0917344018816948\n",
      "7 400 loss 0.08983373641967773\n",
      "test acc :  0.5093\n",
      "8 0 loss 0.08355022966861725\n",
      "8 100 loss 0.09345561265945435\n",
      "8 200 loss 0.08127716183662415\n",
      "8 300 loss 0.08807353675365448\n",
      "8 400 loss 0.08636192977428436\n",
      "test acc :  0.5352\n",
      "9 0 loss 0.08004598319530487\n",
      "9 100 loss 0.08989448845386505\n",
      "9 200 loss 0.07824313640594482\n",
      "9 300 loss 0.08493747562170029\n",
      "9 400 loss 0.08345197141170502\n",
      "test acc :  0.5571\n",
      "10 0 loss 0.07705165445804596\n",
      "10 100 loss 0.08678595721721649\n",
      "10 200 loss 0.0756436213850975\n",
      "10 300 loss 0.08224496245384216\n",
      "10 400 loss 0.08092234283685684\n",
      "test acc :  0.5747\n",
      "11 0 loss 0.07441703230142593\n",
      "11 100 loss 0.08403900265693665\n",
      "11 200 loss 0.07335633039474487\n",
      "11 300 loss 0.07990600913763046\n",
      "11 400 loss 0.07867644727230072\n",
      "test acc :  0.5941\n",
      "12 0 loss 0.07211394608020782\n",
      "12 100 loss 0.08158200979232788\n",
      "12 200 loss 0.07134275138378143\n",
      "12 300 loss 0.07783723622560501\n",
      "12 400 loss 0.07668007165193558\n",
      "test acc :  0.6114\n",
      "13 0 loss 0.0700608417391777\n",
      "13 100 loss 0.0793977752327919\n",
      "13 200 loss 0.06954559683799744\n",
      "13 300 loss 0.07597361505031586\n",
      "13 400 loss 0.07488047331571579\n",
      "test acc :  0.6253\n",
      "14 0 loss 0.06824599206447601\n",
      "14 100 loss 0.07743702828884125\n",
      "14 200 loss 0.06793345510959625\n",
      "14 300 loss 0.07425446063280106\n",
      "14 400 loss 0.07327115535736084\n",
      "test acc :  0.6381\n",
      "15 0 loss 0.06660370528697968\n",
      "15 100 loss 0.07566101104021072\n",
      "15 200 loss 0.06647977977991104\n",
      "15 300 loss 0.07267196476459503\n",
      "15 400 loss 0.07180460542440414\n",
      "test acc :  0.6469\n",
      "16 0 loss 0.06510435044765472\n",
      "16 100 loss 0.07405555993318558\n",
      "16 200 loss 0.06515373289585114\n",
      "16 300 loss 0.07121490687131882\n",
      "16 400 loss 0.07045447081327438\n",
      "test acc :  0.6563\n",
      "17 0 loss 0.06374146044254303\n",
      "17 100 loss 0.07258890569210052\n",
      "17 200 loss 0.06391996145248413\n",
      "17 300 loss 0.06987211108207703\n",
      "17 400 loss 0.06921040266752243\n",
      "test acc :  0.6664\n",
      "18 0 loss 0.06248990446329117\n",
      "18 100 loss 0.07123509794473648\n",
      "18 200 loss 0.0627775564789772\n",
      "18 300 loss 0.06864247471094131\n",
      "18 400 loss 0.0680684894323349\n",
      "test acc :  0.6768\n",
      "19 0 loss 0.06133512407541275\n",
      "19 100 loss 0.06997664272785187\n",
      "19 200 loss 0.06172039359807968\n",
      "19 300 loss 0.06750530749559402\n",
      "19 400 loss 0.06701908260583878\n",
      "test acc :  0.6854\n",
      "20 0 loss 0.060278356075286865\n",
      "20 100 loss 0.06880421936511993\n",
      "20 200 loss 0.06074480339884758\n",
      "20 300 loss 0.06643931567668915\n",
      "20 400 loss 0.06604793667793274\n",
      "test acc :  0.6929\n",
      "21 0 loss 0.05930075794458389\n",
      "21 100 loss 0.06770424544811249\n",
      "21 200 loss 0.05983210355043411\n",
      "21 300 loss 0.06543797254562378\n",
      "21 400 loss 0.06512323766946793\n",
      "test acc :  0.7007\n",
      "22 0 loss 0.05838370323181152\n",
      "22 100 loss 0.06667597591876984\n",
      "22 200 loss 0.05897524207830429\n",
      "22 300 loss 0.06450088322162628\n",
      "22 400 loss 0.06424987316131592\n",
      "test acc :  0.7059\n",
      "23 0 loss 0.05752246454358101\n",
      "23 100 loss 0.06571599841117859\n",
      "23 200 loss 0.05816458538174629\n",
      "23 300 loss 0.06362030655145645\n",
      "23 400 loss 0.06343315541744232\n",
      "test acc :  0.7116\n",
      "24 0 loss 0.05671171098947525\n",
      "24 100 loss 0.06480778008699417\n",
      "24 200 loss 0.057399481534957886\n",
      "24 300 loss 0.06278723478317261\n",
      "24 400 loss 0.06265544146299362\n",
      "test acc :  0.717\n",
      "25 0 loss 0.05595047399401665\n",
      "25 100 loss 0.0639498233795166\n",
      "25 200 loss 0.05667787790298462\n",
      "25 300 loss 0.061995841562747955\n",
      "25 400 loss 0.061922479420900345\n",
      "test acc :  0.7237\n",
      "26 0 loss 0.05523594468832016\n",
      "26 100 loss 0.06314895302057266\n",
      "26 200 loss 0.05599508434534073\n",
      "26 300 loss 0.06124423071742058\n",
      "26 400 loss 0.06122912839055061\n",
      "test acc :  0.7278\n",
      "27 0 loss 0.05456118658185005\n",
      "27 100 loss 0.0623927041888237\n",
      "27 200 loss 0.0553477481007576\n",
      "27 300 loss 0.060527123510837555\n",
      "27 400 loss 0.06057377904653549\n",
      "test acc :  0.7328\n",
      "28 0 loss 0.05391637235879898\n",
      "28 100 loss 0.06168699264526367\n",
      "28 200 loss 0.05472429469227791\n",
      "28 300 loss 0.05984686687588692\n",
      "28 400 loss 0.05994900315999985\n",
      "test acc :  0.7373\n",
      "29 0 loss 0.05330028384923935\n",
      "29 100 loss 0.06101897358894348\n",
      "29 200 loss 0.054127972573041916\n",
      "29 300 loss 0.059199970215559006\n",
      "29 400 loss 0.059349846094846725\n",
      "test acc :  0.7416\n",
      "30 0 loss 0.05270889401435852\n",
      "30 100 loss 0.060382891446352005\n",
      "30 200 loss 0.053558506071567535\n",
      "30 300 loss 0.05858476087450981\n",
      "30 400 loss 0.0587819442152977\n",
      "test acc :  0.7467\n",
      "31 0 loss 0.052144359797239304\n",
      "31 100 loss 0.059779126197099686\n",
      "31 200 loss 0.05301126837730408\n",
      "31 300 loss 0.057996414601802826\n",
      "31 400 loss 0.05823702737689018\n",
      "test acc :  0.7501\n",
      "32 0 loss 0.05160559341311455\n",
      "32 100 loss 0.05920390412211418\n",
      "32 200 loss 0.052489589899778366\n",
      "32 300 loss 0.05743228271603584\n",
      "32 400 loss 0.05772050470113754\n",
      "test acc :  0.7536\n",
      "33 0 loss 0.05108598619699478\n",
      "33 100 loss 0.05865227058529854\n",
      "33 200 loss 0.05199173837900162\n",
      "33 300 loss 0.0568876676261425\n",
      "33 400 loss 0.05722380802035332\n",
      "test acc :  0.7571\n",
      "34 0 loss 0.050588082522153854\n",
      "34 100 loss 0.05812184885144234\n",
      "34 200 loss 0.05151171609759331\n",
      "34 300 loss 0.05636397749185562\n",
      "34 400 loss 0.05674329400062561\n",
      "test acc :  0.7603\n",
      "35 0 loss 0.05010852962732315\n",
      "35 100 loss 0.05761328339576721\n",
      "35 200 loss 0.05105356127023697\n",
      "35 300 loss 0.05586067587137222\n",
      "35 400 loss 0.056281525641679764\n",
      "test acc :  0.7635\n",
      "36 0 loss 0.0496474988758564\n",
      "36 100 loss 0.05712847039103508\n",
      "36 200 loss 0.0506146065890789\n",
      "36 300 loss 0.055375099182128906\n",
      "36 400 loss 0.05583874136209488\n",
      "test acc :  0.7668\n",
      "37 0 loss 0.0492032915353775\n",
      "37 100 loss 0.05666247755289078\n",
      "37 200 loss 0.05019441992044449\n",
      "37 300 loss 0.05490853264927864\n",
      "37 400 loss 0.055410586297512054\n",
      "test acc :  0.7705\n",
      "38 0 loss 0.048772990703582764\n",
      "38 100 loss 0.05620960518717766\n",
      "38 200 loss 0.04978957772254944\n",
      "38 300 loss 0.05445799231529236\n",
      "38 400 loss 0.05500040203332901\n",
      "test acc :  0.7738\n",
      "39 0 loss 0.04836076498031616\n",
      "39 100 loss 0.055771779268980026\n",
      "39 200 loss 0.04939812421798706\n",
      "39 300 loss 0.05402276664972305\n",
      "39 400 loss 0.05460397154092789\n",
      "test acc :  0.7762\n",
      "40 0 loss 0.04796552658081055\n",
      "40 100 loss 0.055350832641124725\n",
      "40 200 loss 0.04902079701423645\n",
      "40 300 loss 0.05360036343336105\n",
      "40 400 loss 0.054220229387283325\n",
      "test acc :  0.7791\n",
      "41 0 loss 0.04758107289671898\n",
      "41 100 loss 0.0549440011382103\n",
      "41 200 loss 0.04865718260407448\n",
      "41 300 loss 0.053191959857940674\n",
      "41 400 loss 0.05384797602891922\n",
      "test acc :  0.7822\n",
      "42 0 loss 0.047212012112140656\n",
      "42 100 loss 0.0545472726225853\n",
      "42 200 loss 0.048302870243787766\n",
      "42 300 loss 0.05279573053121567\n",
      "42 400 loss 0.05348651856184006\n",
      "test acc :  0.7845\n",
      "43 0 loss 0.04685242474079132\n",
      "43 100 loss 0.054162196815013885\n",
      "43 200 loss 0.047960467636585236\n",
      "43 300 loss 0.05241231992840767\n",
      "43 400 loss 0.05313428118824959\n",
      "test acc :  0.7867\n",
      "44 0 loss 0.0465020090341568\n",
      "44 100 loss 0.053790878504514694\n",
      "44 200 loss 0.04762934893369675\n",
      "44 300 loss 0.05204085633158684\n",
      "44 400 loss 0.05279231071472168\n",
      "test acc :  0.7886\n",
      "45 0 loss 0.04616357758641243\n",
      "45 100 loss 0.053434282541275024\n",
      "45 200 loss 0.047307755798101425\n",
      "45 300 loss 0.051681987941265106\n",
      "45 400 loss 0.05246217921376228\n",
      "test acc :  0.7908\n",
      "46 0 loss 0.045835208147764206\n",
      "46 100 loss 0.05308990553021431\n",
      "46 200 loss 0.04699523746967316\n",
      "46 300 loss 0.05133068561553955\n",
      "46 400 loss 0.05214263126254082\n",
      "test acc :  0.7934\n",
      "47 0 loss 0.04551561921834946\n",
      "47 100 loss 0.05275673791766167\n",
      "47 200 loss 0.04669106751680374\n",
      "47 300 loss 0.050989311188459396\n",
      "47 400 loss 0.05183122679591179\n",
      "test acc :  0.7954\n",
      "48 0 loss 0.04520464316010475\n",
      "48 100 loss 0.05243523046374321\n",
      "48 200 loss 0.0463934987783432\n",
      "48 300 loss 0.0506555549800396\n",
      "48 400 loss 0.05152641981840134\n",
      "test acc :  0.797\n",
      "49 0 loss 0.04490112140774727\n",
      "49 100 loss 0.05212181806564331\n",
      "49 200 loss 0.046104203909635544\n",
      "49 300 loss 0.050329022109508514\n",
      "49 400 loss 0.05123021453619003\n",
      "test acc :  0.7991\n",
      "50 0 loss 0.0446031391620636\n",
      "50 100 loss 0.051818251609802246\n",
      "50 200 loss 0.04582319036126137\n",
      "50 300 loss 0.050009746104478836\n",
      "50 400 loss 0.05093911290168762\n",
      "test acc :  0.8004\n",
      "51 0 loss 0.04431120306253433\n",
      "51 100 loss 0.051523495465517044\n",
      "51 200 loss 0.04554922506213188\n",
      "51 300 loss 0.04969814419746399\n",
      "51 400 loss 0.05065541714429855\n",
      "test acc :  0.8021\n",
      "52 0 loss 0.04402463510632515\n",
      "52 100 loss 0.05123831704258919\n",
      "52 200 loss 0.04528244584798813\n",
      "52 300 loss 0.04939335584640503\n",
      "52 400 loss 0.050379008054733276\n",
      "test acc :  0.8032\n",
      "53 0 loss 0.043743181973695755\n",
      "53 100 loss 0.05095933750271797\n",
      "53 200 loss 0.045021165162324905\n",
      "53 300 loss 0.04909534007310867\n",
      "53 400 loss 0.050108153373003006\n",
      "test acc :  0.8047\n",
      "54 0 loss 0.04347008094191551\n",
      "54 100 loss 0.05068633705377579\n",
      "54 200 loss 0.04476510360836983\n",
      "54 300 loss 0.04880417138338089\n",
      "54 400 loss 0.04984188824892044\n",
      "test acc :  0.8064\n",
      "55 0 loss 0.043203700333833694\n",
      "55 100 loss 0.05042009800672531\n",
      "55 200 loss 0.04451506584882736\n",
      "55 300 loss 0.04851969704031944\n",
      "55 400 loss 0.04958141967654228\n",
      "test acc :  0.8085\n",
      "56 0 loss 0.0429452583193779\n",
      "56 100 loss 0.0501626618206501\n",
      "56 200 loss 0.044269781559705734\n",
      "56 300 loss 0.04824180528521538\n",
      "56 400 loss 0.049327123910188675\n",
      "test acc :  0.8096\n",
      "57 0 loss 0.04269274324178696\n",
      "57 100 loss 0.049908775836229324\n",
      "57 200 loss 0.04403039067983627\n",
      "57 300 loss 0.04796919971704483\n",
      "57 400 loss 0.04907992109656334\n",
      "test acc :  0.8111\n",
      "58 0 loss 0.042445700615644455\n",
      "58 100 loss 0.049658287316560745\n",
      "58 200 loss 0.04379665106534958\n",
      "58 300 loss 0.04770303890109062\n",
      "58 400 loss 0.048837676644325256\n",
      "test acc :  0.813\n",
      "59 0 loss 0.04220323637127876\n",
      "59 100 loss 0.04941178485751152\n",
      "59 200 loss 0.04356915503740311\n",
      "59 300 loss 0.047442320734262466\n",
      "59 400 loss 0.04860005900263786\n",
      "test acc :  0.8145\n",
      "60 0 loss 0.041965849697589874\n",
      "60 100 loss 0.04916984960436821\n",
      "60 200 loss 0.043345287442207336\n",
      "60 300 loss 0.04718870669603348\n",
      "60 400 loss 0.04836668074131012\n",
      "test acc :  0.8164\n",
      "61 0 loss 0.04173411801457405\n",
      "61 100 loss 0.04893278330564499\n",
      "61 200 loss 0.04312619939446449\n",
      "61 300 loss 0.04694286733865738\n",
      "61 400 loss 0.04813831299543381\n",
      "test acc :  0.8188\n",
      "62 0 loss 0.041506774723529816\n",
      "62 100 loss 0.048700422048568726\n",
      "62 200 loss 0.042910367250442505\n",
      "62 300 loss 0.04670284315943718\n",
      "62 400 loss 0.04791618138551712\n",
      "test acc :  0.8197\n",
      "63 0 loss 0.041284989565610886\n",
      "63 100 loss 0.04847363755106926\n",
      "63 200 loss 0.042699359357357025\n",
      "63 300 loss 0.046468932181596756\n",
      "63 400 loss 0.04769790917634964\n",
      "test acc :  0.8206\n",
      "64 0 loss 0.04106815531849861\n",
      "64 100 loss 0.04825229197740555\n",
      "64 200 loss 0.0424925796687603\n",
      "64 300 loss 0.04624093323945999\n",
      "64 400 loss 0.04748294875025749\n",
      "test acc :  0.8214\n",
      "65 0 loss 0.04085569828748703\n",
      "65 100 loss 0.04803464561700821\n",
      "65 200 loss 0.042288888245821\n",
      "65 300 loss 0.046017251908779144\n",
      "65 400 loss 0.047272127121686935\n",
      "test acc :  0.8223\n",
      "66 0 loss 0.04064695164561272\n",
      "66 100 loss 0.047819096595048904\n",
      "66 200 loss 0.04208820313215256\n",
      "66 300 loss 0.045797593891620636\n",
      "66 400 loss 0.04706548526883125\n",
      "test acc :  0.8232\n",
      "67 0 loss 0.04044150561094284\n",
      "67 100 loss 0.04760883003473282\n",
      "67 200 loss 0.04189068451523781\n",
      "67 300 loss 0.045583002269268036\n",
      "67 400 loss 0.046863920986652374\n",
      "test acc :  0.825\n",
      "68 0 loss 0.04024072736501694\n",
      "68 100 loss 0.04740343615412712\n",
      "68 200 loss 0.041696738451719284\n",
      "68 300 loss 0.04537288099527359\n",
      "68 400 loss 0.04666696488857269\n",
      "test acc :  0.8256\n",
      "69 0 loss 0.040043849498033524\n",
      "69 100 loss 0.04720109701156616\n",
      "69 200 loss 0.04150695353746414\n",
      "69 300 loss 0.045166872441768646\n",
      "69 400 loss 0.04647374898195267\n",
      "test acc :  0.8263\n",
      "70 0 loss 0.03984982892870903\n",
      "70 100 loss 0.047002095729112625\n",
      "70 200 loss 0.041320763528347015\n",
      "70 300 loss 0.04496481269598007\n",
      "70 400 loss 0.04628382995724678\n",
      "test acc :  0.8275\n",
      "71 0 loss 0.03965771943330765\n",
      "71 100 loss 0.046806275844573975\n",
      "71 200 loss 0.04113922268152237\n",
      "71 300 loss 0.04476593807339668\n",
      "71 400 loss 0.046097539365291595\n",
      "test acc :  0.8282\n",
      "72 0 loss 0.03946869820356369\n",
      "72 100 loss 0.04661364108324051\n",
      "72 200 loss 0.04096105694770813\n",
      "72 300 loss 0.044569823890924454\n",
      "72 400 loss 0.04591444134712219\n",
      "test acc :  0.8296\n",
      "73 0 loss 0.039283305406570435\n",
      "73 100 loss 0.04642434045672417\n",
      "73 200 loss 0.040784623473882675\n",
      "73 300 loss 0.04437648504972458\n",
      "73 400 loss 0.045734308660030365\n",
      "test acc :  0.8302\n",
      "74 0 loss 0.03910214453935623\n",
      "74 100 loss 0.04623822122812271\n",
      "74 200 loss 0.04061014577746391\n",
      "74 300 loss 0.04418756440281868\n",
      "74 400 loss 0.045556701719760895\n",
      "test acc :  0.8316\n",
      "75 0 loss 0.03892552852630615\n",
      "75 100 loss 0.04605536535382271\n",
      "75 200 loss 0.04043920710682869\n",
      "75 300 loss 0.044002119451761246\n",
      "75 400 loss 0.04538055881857872\n",
      "test acc :  0.8324\n",
      "76 0 loss 0.03875264525413513\n",
      "76 100 loss 0.045875005424022675\n",
      "76 200 loss 0.04027188569307327\n",
      "76 300 loss 0.043820206075906754\n",
      "76 400 loss 0.04520733281970024\n",
      "test acc :  0.8335\n",
      "77 0 loss 0.038582075387239456\n",
      "77 100 loss 0.04569671303033829\n",
      "77 200 loss 0.04010870307683945\n",
      "77 300 loss 0.04364117234945297\n",
      "77 400 loss 0.04503738135099411\n",
      "test acc :  0.8346\n",
      "78 0 loss 0.03841397911310196\n",
      "78 100 loss 0.04552104324102402\n",
      "78 200 loss 0.03994744271039963\n",
      "78 300 loss 0.043464936316013336\n",
      "78 400 loss 0.04486938938498497\n",
      "test acc :  0.8358\n",
      "79 0 loss 0.03824803605675697\n",
      "79 100 loss 0.04534788802266121\n",
      "79 200 loss 0.039787981659173965\n",
      "79 300 loss 0.043291401118040085\n",
      "79 400 loss 0.04470363259315491\n",
      "test acc :  0.8364\n",
      "80 0 loss 0.03808467835187912\n",
      "80 100 loss 0.04517688602209091\n",
      "80 200 loss 0.03963100165128708\n",
      "80 300 loss 0.0431220643222332\n",
      "80 400 loss 0.04453931376338005\n",
      "test acc :  0.8375\n",
      "81 0 loss 0.037923943251371384\n",
      "81 100 loss 0.04500853270292282\n",
      "81 200 loss 0.03947613388299942\n",
      "81 300 loss 0.042955975979566574\n",
      "81 400 loss 0.04437790811061859\n",
      "test acc :  0.8379\n",
      "82 0 loss 0.03776607662439346\n",
      "82 100 loss 0.044842664152383804\n",
      "82 200 loss 0.039324451237916946\n",
      "82 300 loss 0.042791374027729034\n",
      "82 400 loss 0.044217996299266815\n",
      "test acc :  0.8382\n",
      "83 0 loss 0.03761017322540283\n",
      "83 100 loss 0.04467971995472908\n",
      "83 200 loss 0.03917527571320534\n",
      "83 300 loss 0.042628318071365356\n",
      "83 400 loss 0.044059887528419495\n",
      "test acc :  0.8388\n",
      "84 0 loss 0.03745704144239426\n",
      "84 100 loss 0.04451969265937805\n",
      "84 200 loss 0.03902864828705788\n",
      "84 300 loss 0.04246807098388672\n",
      "84 400 loss 0.043904971331357956\n",
      "test acc :  0.8397\n",
      "85 0 loss 0.03730637580156326\n",
      "85 100 loss 0.04436231404542923\n",
      "85 200 loss 0.038883477449417114\n",
      "85 300 loss 0.04231100529432297\n",
      "85 400 loss 0.04375187307596207\n",
      "test acc :  0.8412\n",
      "86 0 loss 0.03715896978974342\n",
      "86 100 loss 0.04420659691095352\n",
      "86 200 loss 0.03874034434556961\n",
      "86 300 loss 0.042156580835580826\n",
      "86 400 loss 0.04360025376081467\n",
      "test acc :  0.8421\n",
      "87 0 loss 0.037014275789260864\n",
      "87 100 loss 0.044052623212337494\n",
      "87 200 loss 0.03859835863113403\n",
      "87 300 loss 0.04200438782572746\n",
      "87 400 loss 0.0434495247900486\n",
      "test acc :  0.8428\n",
      "88 0 loss 0.03687151521444321\n",
      "88 100 loss 0.043899957090616226\n",
      "88 200 loss 0.03845846652984619\n",
      "88 300 loss 0.041854195296764374\n",
      "88 400 loss 0.04330062493681908\n",
      "test acc :  0.8436\n",
      "89 0 loss 0.036731164902448654\n",
      "89 100 loss 0.04374951124191284\n",
      "89 200 loss 0.03832138702273369\n",
      "89 300 loss 0.04170690476894379\n",
      "89 400 loss 0.04315396025776863\n",
      "test acc :  0.844\n",
      "90 0 loss 0.036591917276382446\n",
      "90 100 loss 0.043600987643003464\n",
      "90 200 loss 0.03818712383508682\n",
      "90 300 loss 0.041561465710401535\n",
      "90 400 loss 0.04300937056541443\n",
      "test acc :  0.8452\n",
      "91 0 loss 0.03645489737391472\n",
      "91 100 loss 0.04345426708459854\n",
      "91 200 loss 0.038055308163166046\n",
      "91 300 loss 0.041418105363845825\n",
      "91 400 loss 0.04286690801382065\n",
      "test acc :  0.8457\n",
      "92 0 loss 0.036319632083177567\n",
      "92 100 loss 0.04330841824412346\n",
      "92 200 loss 0.03792550414800644\n",
      "92 300 loss 0.04127715900540352\n",
      "92 400 loss 0.04272657632827759\n",
      "test acc :  0.8471\n",
      "93 0 loss 0.036186523735523224\n",
      "93 100 loss 0.04316403344273567\n",
      "93 200 loss 0.03779716417193413\n",
      "93 300 loss 0.04113849624991417\n",
      "93 400 loss 0.04258804768323898\n",
      "test acc :  0.8483\n",
      "94 0 loss 0.036055319011211395\n",
      "94 100 loss 0.043021492660045624\n",
      "94 200 loss 0.037670254707336426\n",
      "94 300 loss 0.041001882404088974\n",
      "94 400 loss 0.04245080426335335\n",
      "test acc :  0.8492\n",
      "95 0 loss 0.03592623397707939\n",
      "95 100 loss 0.04288052022457123\n",
      "95 200 loss 0.03754490613937378\n",
      "95 300 loss 0.04086753726005554\n",
      "95 400 loss 0.042315687984228134\n",
      "test acc :  0.8498\n",
      "96 0 loss 0.03579932451248169\n",
      "96 100 loss 0.04274066537618637\n",
      "96 200 loss 0.03742077201604843\n",
      "96 300 loss 0.040735382586717606\n",
      "96 400 loss 0.04218202829360962\n",
      "test acc :  0.8505\n",
      "97 0 loss 0.03567362204194069\n",
      "97 100 loss 0.042602285742759705\n",
      "97 200 loss 0.0372978039085865\n",
      "97 300 loss 0.0406050905585289\n",
      "97 400 loss 0.04204951971769333\n",
      "test acc :  0.8515\n",
      "98 0 loss 0.03555028885602951\n",
      "98 100 loss 0.0424656942486763\n",
      "98 200 loss 0.03717616945505142\n",
      "98 300 loss 0.04047643020749092\n",
      "98 400 loss 0.04191843792796135\n",
      "test acc :  0.8523\n",
      "99 0 loss 0.035428889095783234\n",
      "99 100 loss 0.042330287396907806\n",
      "99 200 loss 0.03705455735325813\n",
      "99 300 loss 0.04034944623708725\n",
      "99 400 loss 0.04178903251886368\n",
      "test acc :  0.8531\n"
     ]
    }
   ],
   "source": [
    "# 数据集加载\n",
    "# keras.datasets\n",
    "# tf.data.Dataset.from_tensor_sclices : shuffle, map, batch, repeat\n",
    "# input pipeline\n",
    "# such as : boston housing, mnist/fashion mnist, cifar10/100, imdb\n",
    "\n",
    "# 前向传播 forward\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets\n",
    "\n",
    "# loading data\n",
    "(x, y), (x_test, y_test) = datasets.mnist.load_data()\n",
    "\n",
    "# convert to Tensor\n",
    "# x : [0-255] -> [0-1.]\n",
    "x = tf.convert_to_tensor(x, dtype=tf.float32) / 255.\n",
    "y = tf.convert_to_tensor(y, dtype=tf.int32)\n",
    "\n",
    "x_test = tf.convert_to_tensor(x_test, dtype=tf.float32) / 255.\n",
    "y_test = tf.convert_to_tensor(y_test, dtype=tf.int32)\n",
    "\n",
    "# show the info\n",
    "print(x.shape, y.shape, x.dtype, y.dtype)\n",
    "print(tf.reduce_min(x), tf.reduce_max(x))\n",
    "print(tf.reduce_min(y), tf.reduce_max(y))\n",
    "\n",
    "# set the dataset and split the batch\n",
    "train_db = tf.data.Dataset.from_tensor_slices((x, y)).batch(128)\n",
    "test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(128)\n",
    "\n",
    "train_iter = iter(train_db)\n",
    "sample = next(train_iter)\n",
    "print(\"batch\", sample[0].shape, sample[1].shape)\n",
    "\n",
    "# 权值\n",
    "# [b, 784] => [b, 256] => [b, 128] => [b, 10]\n",
    "# [dim_in, dim_out], [dim_out]\n",
    "# tf.Variable 才会自动记录梯度信息，tf.Tensor 不会记录梯度信息，故此值为 None，类型为 NoneType\n",
    "# 需要给一个好点的初始值，否则容易 loss 爆炸，出现 nan(not a number) 情况,stddev=0.1 方差给小一点\n",
    "w1 = tf.Variable(tf.random.truncated_normal([784, 256], stddev=0.1))\n",
    "b1 = tf.Variable(tf.zeros([256]))\n",
    "w2 = tf.Variable(tf.random.truncated_normal([256, 128], stddev=0.1))\n",
    "b2 = tf.Variable(tf.zeros([128]))\n",
    "w3 = tf.Variable(tf.random.truncated_normal([128, 10], stddev=0.1))\n",
    "b3 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# 学习率 ，步长 10^-3\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# 对数据集迭代 100 次\n",
    "# for epoch in range(10): # test acc = 0.5566\n",
    "for epoch in range(100): # test acc = 0.8531\n",
    "    # 对数据集一次循环\n",
    "    for step, (x, y) in enumerate(train_db):\n",
    "        # x [128, 28, 28]\n",
    "        # y [128]\n",
    "        # 进行维度转换 [128, 28, 28] => [b, 28*28]\n",
    "        x = tf.reshape(x, [-1, 28*28])   \n",
    "    \n",
    "        # 自动求导过程,梯度\n",
    "        with tf.GradientTape() as tape:\n",
    "            # x [b, 28*28]\n",
    "            # h1 = x@w1 + b1\n",
    "            # [b, 784]@[784, 256] + [256] => [b, 256] + [256] => [b, 256] + [b, 256]\n",
    "            h1 = x@w1 + tf.broadcast_to(b1, [x.shape[0], 256])\n",
    "            # 添加非线性，relu 激活函数\n",
    "            h1 = tf.nn.relu(h1)\n",
    "            # [b, 256] => [b, 128]\n",
    "            h2 = h1@w2 + b2\n",
    "            h2 = tf.nn.relu(h2)\n",
    "            # [b, 128] => [b, 10]\n",
    "            out = h2@w3 +b3\n",
    "        \n",
    "            # compute loss\n",
    "            # out [b, 10]\n",
    "            # y [b] => [b, 10]\n",
    "            y_one_hot = tf.one_hot(y, depth=10)\n",
    "        \n",
    "            # MSE = mean((y - out)**2)\n",
    "            loss = tf.square(y_one_hot - out)\n",
    "            # mean: scalar\n",
    "            loss = tf.reduce_mean(loss)\n",
    "    \n",
    "        # compute gradients\n",
    "        grads = tape.gradient(loss, [w1, b1, w2, b2, w3, b3])\n",
    "        # print(grads)\n",
    "        # w1 = w1 - lr * w1_grad\n",
    "        # 原地更新，否则就返回 tf.Tensor 类型了，无法进行下一个梯度更新\n",
    "        # w1 = w1 - learning_rate * grads[0]\n",
    "        w1.assign_sub(learning_rate * grads[0])\n",
    "        b1.assign_sub(learning_rate * grads[1])\n",
    "        w2.assign_sub(learning_rate * grads[2])\n",
    "        b2.assign_sub(learning_rate * grads[3])\n",
    "        w3.assign_sub(learning_rate * grads[4])\n",
    "        b3.assign_sub(learning_rate * grads[5])\n",
    "    \n",
    "        if step % 100 == 0:\n",
    "            print(epoch, step, \"loss\", float(loss))\n",
    "    \n",
    "    # test/evluation\n",
    "    total_correct, total_num = 0, 0\n",
    "    for step, (x, y) in enumerate(test_db):\n",
    "        x = tf.reshape(x, [-1, 28*28])\n",
    "        \n",
    "        h1 = tf.nn.relu(x@w1 + b1)\n",
    "        h2 = tf.nn.relu(h1@w2 + b2)        \n",
    "        out = h2@w3 + b3\n",
    "        \n",
    "        prob = tf.nn.softmax(out, axis=1)\n",
    "        pred = tf.argmax(prob, axis=1)\n",
    "        pred = tf.cast(pred, dtype=tf.int32)\n",
    "        correct = tf.cast(tf.equal(pred, y), dtype=tf.int32)\n",
    "        correct = tf.reduce_sum(correct)\n",
    "        \n",
    "        total_correct += int(correct)\n",
    "        total_num += x.shape[0]\n",
    "        \n",
    "    acc = total_correct / total_num\n",
    "    print(\"test acc : \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                multiple                  8         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  6         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  6         \n",
      "=================================================================\n",
      "Total params: 20\n",
      "Trainable params: 20\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "dense/kernel:0 (3, 2)\n",
      "dense/bias:0 (2,)\n",
      "dense_1/kernel:0 (2, 2)\n",
      "dense_1/bias:0 (2,)\n",
      "dense_2/kernel:0 (2, 2)\n",
      "dense_2/bias:0 (2,)\n"
     ]
    }
   ],
   "source": [
    "# 全连接层\n",
    "# matmul\n",
    "# neural network\n",
    "# deep learning\n",
    "# multi-layer\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "x = tf.random.normal([2, 3])\n",
    "\n",
    "model = keras.Sequential([\n",
    "        keras.layers.Dense(2, activation=\"relu\"),\n",
    "        keras.layers.Dense(2, activation=\"relu\"),\n",
    "        keras.layers.Dense(2)]) # 没有加 relu 非线性的这一层，称之为 logits\n",
    "\n",
    "model.build(input_shape=[None, 3])\n",
    "model.summary()\n",
    "\n",
    "for p in model.trainable_variables:\n",
    "    print(p.name, p.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出方式\n",
    "# such as : R 整个实数集，[0-1]输出概率值，[0-1]&所有概率和为1，[-1-1]范围\n",
    "# tf.sigmoid => [0-1] => sigmoid 激活函数以及压缩\n",
    "# tf.nn.softmax => [0-1]&所有概率之和为 1 => softmax 函数\n",
    "# tf.tanh => [-1 -1] => 通过 sigmoid 函数扩大到 [0-2],然后平移到 [-1 -1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 损失函数\n",
    "# MSE\n",
    "# Cross Entropy Loss 熵 交叉熵 散度\n",
    "# Hinge Loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(3.0, shape=(), dtype=float32)\n",
      "tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 梯度下降\n",
    "# gradient descent\n",
    "# auto gradient\n",
    "# with tf.GradientTape() as tape:\n",
    "# 二阶求导  2nd-order\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "w = tf.Variable(1.0)\n",
    "b = tf.Variable(2.0)\n",
    "x = tf.Variable(3.0)\n",
    "\n",
    "# 二阶求导\n",
    "with tf.GradientTape() as tape1:\n",
    "    with tf.GradientTape() as tape2:\n",
    "        y = x * w + b\n",
    "    dy_dw, dy_db = tape2.gradient(y, [w, b])\n",
    "d2y_dw2 = tape1.gradient(dy_dw, w)\n",
    "\n",
    "print(dy_dw)\n",
    "print(dy_db)\n",
    "print(d2y_dw2)\n",
    "\n",
    "assert dy_dw.numpy() == 3.0\n",
    "assert d2y_dw2 is None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation functions 激活函数\n",
    "# 激活函数及其梯度\n",
    "# 温水煮青蛙\n",
    "# sigmoid logistic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
