{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow\n",
    "### Deep Learning\n",
    "### Neural Networks\n",
    "### 环境搭建详细教程\n",
    "- Windows 10\n",
    "- Ubuntu 18.04/16.04/20.04\n",
    "- TensorFlow\n",
    "- PyTorch\n",
    "- CUDA cuDNN\n",
    "- Jupyter Lab & Jupyter Notebook\n",
    "- PyCharm\n",
    "\n",
    "[环境搭建详细教程——GitHub-Blog](https://2694048168.github.io/)\n",
    "\n",
    "[环境搭建详细教程——Gitee-Blog](http://weili_yzzcq.gitee.io/)\n",
    "\n",
    "[环境搭建详细教程——CSDN-Blog](https://blog.csdn.net/weixin_46782218)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent at b = 0, w = 0, error = 5565.107834483211\n",
      "Running\n",
      "After 1000 iterations b = 0.08893651993741346, w = 1.4777440851894448, error = 112.61481011613473\n"
     ]
    }
   ],
   "source": [
    "# 线性回归\n",
    "# make decisions: discrete 离散值     continuous 连续值\n",
    "# continuous prediction：input data（x）；prediction（f，参数 θ theta）；real data，ground-truth（y）\n",
    "# linear equation 线性方程组 y = w * x + b + epsilon\n",
    "# with noise  epsilon 噪声 高斯分布\n",
    "# find the W and b\n",
    "# loss = (WX + b - Y)**2\n",
    "# miniize loss \n",
    "# gradient descent GD 梯度下降 一维和二维可视化\n",
    "# loss surface\n",
    "# linear regression\n",
    "# logistic regression\n",
    "# classification\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# 生成 csv 文件里面的数据\n",
    "# data = []\n",
    "# for i in range(100):\n",
    "# \tx = np.random.uniform(3., 12.)\n",
    "# \t# mean=0, std=0.1\n",
    "# \teps = np.random.normal(0., 0.1)\n",
    "# \ty = 1.477 * x + 0.089 + eps\n",
    "# \tdata.append([x, y])\n",
    "# data = np.array(data)\n",
    "# print(data.shape, data)\n",
    "\n",
    "# cpmputer loss\n",
    "# Y = W*X +b\n",
    "def computer_error_for_line_given_points(b, w, points):\n",
    "    total_error = 0\n",
    "    for i in range(0, len(points)):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        # computer mean-squared-error\n",
    "        total_error += ((w * x + b) - y) ** 2\n",
    "        # total_error += (y - (w * x + b)) ** 2\n",
    "    # average loss for each point\n",
    "    return total_error / float(len(points))\n",
    "\n",
    "# computer Gradient and update\n",
    "def step_gradient(b_current, w_current, points, learning_rate):\n",
    "    b_gradient = 0\n",
    "    w_gradient = 0\n",
    "    N = float(len(points))\n",
    "    for i in range(0, len(points)):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        # grad_b = 2(wx + b - y)\n",
    "        b_gradient += (2/N) * ((w_current * x + b_current) - y)\n",
    "        # grad_w = 2(wx + b - y) * x\n",
    "        w_gradient += (2/N) * x * ((w_current * x + b_current) - y)\n",
    "    # update the grad_w, and the grad_b\n",
    "    new_b = b_current - (learning_rate * b_gradient)\n",
    "    new_w = w_current - (learning_rate * w_gradient)\n",
    "    return [new_b, new_w]\n",
    "\n",
    "# set w = w_new and loop\n",
    "def gradient_descent_runner(points, starting_b, starting_w, learning_rate, num_iterations):\n",
    "    b = starting_b\n",
    "    w = starting_w\n",
    "    # update for several times\n",
    "    for i in range(num_iterations):\n",
    "        b, w = step_gradient(b, w, np.array(points), learning_rate)\n",
    "    return [b, w]\n",
    "\n",
    "\n",
    "# running the linear regression model\n",
    "# loading the data for numpy from the data.cvs file\n",
    "# the data shape is 100 * 2 (row * column)\n",
    "points = np.genfromtxt(\"./datasets/data.csv\", delimiter=\",\")\n",
    "# init the Hyperparameter\n",
    "learning_rate = 0.0001\n",
    "initial_b = 0\n",
    "initial_w = 0\n",
    "num_iterations = 1000\n",
    "# show the initial Hyperparameter for linear regression model and the error\n",
    "print(\"Starting gradient descent at b = {0}, w = {1}, error = {2}\"\n",
    "      .format(initial_b, initial_w, computer_error_for_line_given_points(initial_b, initial_w, points)))\n",
    "\n",
    "print(\"Running\")\n",
    "# iterations = 1000,computer the Hyperparameter for linear regression model\n",
    "[b, w] = gradient_descent_runner(points, initial_b, initial_w, learning_rate, num_iterations)\n",
    "\n",
    "# and show the Hyperparameter for linear regression model and the error\n",
    "print(\"After {0} iterations b = {1}, w = {2}, error = {3}\"\n",
    "      .format(num_iterations, b, w, computer_error_for_line_given_points(b, w, points)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000, 10)\n",
      "第 0 次 epoch，第 100 step，该次训练 loss = 1.6492801904678345\n",
      "第 0 次 epoch，第 200 step，该次训练 loss = 0.9574925899505615\n",
      "第 0 次 epoch，第 300 step，该次训练 loss = 0.7756418585777283\n",
      "第 1 次 epoch，第 100 step，该次训练 loss = 0.677655816078186\n",
      "第 1 次 epoch，第 200 step，该次训练 loss = 0.7041688561439514\n",
      "第 1 次 epoch，第 300 step，该次训练 loss = 0.5958806872367859\n",
      "第 2 次 epoch，第 100 step，该次训练 loss = 0.5593937039375305\n",
      "第 2 次 epoch，第 200 step，该次训练 loss = 0.6061217188835144\n",
      "第 2 次 epoch，第 300 step，该次训练 loss = 0.5171711444854736\n",
      "第 3 次 epoch，第 100 step，该次训练 loss = 0.500190794467926\n",
      "第 3 次 epoch，第 200 step，该次训练 loss = 0.5500799417495728\n",
      "第 3 次 epoch，第 300 step，该次训练 loss = 0.4704623818397522\n",
      "第 4 次 epoch，第 100 step，该次训练 loss = 0.4624771773815155\n",
      "第 4 次 epoch，第 200 step，该次训练 loss = 0.5116350054740906\n",
      "第 4 次 epoch，第 300 step，该次训练 loss = 0.43804916739463806\n",
      "第 5 次 epoch，第 100 step，该次训练 loss = 0.4350418448448181\n",
      "第 5 次 epoch，第 200 step，该次训练 loss = 0.48301219940185547\n",
      "第 5 次 epoch，第 300 step，该次训练 loss = 0.41369202733039856\n",
      "第 6 次 epoch，第 100 step，该次训练 loss = 0.41353902220726013\n",
      "第 6 次 epoch，第 200 step，该次训练 loss = 0.46057674288749695\n",
      "第 6 次 epoch，第 300 step，该次训练 loss = 0.39432698488235474\n",
      "第 7 次 epoch，第 100 step，该次训练 loss = 0.39585599303245544\n",
      "第 7 次 epoch，第 200 step，该次训练 loss = 0.442291796207428\n",
      "第 7 次 epoch，第 300 step，该次训练 loss = 0.3781718909740448\n",
      "第 8 次 epoch，第 100 step，该次训练 loss = 0.3807425796985626\n",
      "第 8 次 epoch，第 200 step，该次训练 loss = 0.42688846588134766\n",
      "第 8 次 epoch，第 300 step，该次训练 loss = 0.36439192295074463\n",
      "第 9 次 epoch，第 100 step，该次训练 loss = 0.367557168006897\n",
      "第 9 次 epoch，第 200 step，该次训练 loss = 0.4134664833545685\n",
      "第 9 次 epoch，第 300 step，该次训练 loss = 0.35241368412971497\n",
      "第 10 次 epoch，第 100 step，该次训练 loss = 0.3557971119880676\n",
      "第 10 次 epoch，第 200 step，该次训练 loss = 0.40170928835868835\n",
      "第 10 次 epoch，第 300 step，该次训练 loss = 0.3418275713920593\n",
      "第 11 次 epoch，第 100 step，该次训练 loss = 0.34523069858551025\n",
      "第 11 次 epoch，第 200 step，该次训练 loss = 0.39130812883377075\n",
      "第 11 次 epoch，第 300 step，该次训练 loss = 0.33241990208625793\n",
      "第 12 次 epoch，第 100 step，该次训练 loss = 0.33555713295936584\n",
      "第 12 次 epoch，第 200 step，该次训练 loss = 0.3819326162338257\n",
      "第 12 次 epoch，第 300 step，该次训练 loss = 0.3239107131958008\n",
      "第 13 次 epoch，第 100 step，该次训练 loss = 0.3267011344432831\n",
      "第 13 次 epoch，第 200 step，该次训练 loss = 0.37346935272216797\n",
      "第 13 次 epoch，第 300 step，该次训练 loss = 0.3161661624908447\n",
      "第 14 次 epoch，第 100 step，该次训练 loss = 0.31853535771369934\n",
      "第 14 次 epoch，第 200 step，该次训练 loss = 0.3657045364379883\n",
      "第 14 次 epoch，第 300 step，该次训练 loss = 0.309009313583374\n",
      "第 15 次 epoch，第 100 step，该次训练 loss = 0.31101059913635254\n",
      "第 15 次 epoch，第 200 step，该次训练 loss = 0.3585987091064453\n",
      "第 15 次 epoch，第 300 step，该次训练 loss = 0.30239829421043396\n",
      "第 16 次 epoch，第 100 step，该次训练 loss = 0.3040561378002167\n",
      "第 16 次 epoch，第 200 step，该次训练 loss = 0.35202449560165405\n",
      "第 16 次 epoch，第 300 step，该次训练 loss = 0.2962380647659302\n",
      "第 17 次 epoch，第 100 step，该次训练 loss = 0.2976039946079254\n",
      "第 17 次 epoch，第 200 step，该次训练 loss = 0.34592434763908386\n",
      "第 17 次 epoch，第 300 step，该次训练 loss = 0.29047834873199463\n",
      "第 18 次 epoch，第 100 step，该次训练 loss = 0.2915274500846863\n",
      "第 18 次 epoch，第 200 step，该次训练 loss = 0.3402198851108551\n",
      "第 18 次 epoch，第 300 step，该次训练 loss = 0.2851108908653259\n",
      "第 19 次 epoch，第 100 step，该次训练 loss = 0.28578639030456543\n",
      "第 19 次 epoch，第 200 step，该次训练 loss = 0.3348488509654999\n",
      "第 19 次 epoch，第 300 step，该次训练 loss = 0.28010454773902893\n",
      "第 20 次 epoch，第 100 step，该次训练 loss = 0.2803708016872406\n",
      "第 20 次 epoch，第 200 step，该次训练 loss = 0.3297297954559326\n",
      "第 20 次 epoch，第 300 step，该次训练 loss = 0.2754082977771759\n",
      "第 21 次 epoch，第 100 step，该次训练 loss = 0.2752252519130707\n",
      "第 21 次 epoch，第 200 step，该次训练 loss = 0.3248599171638489\n",
      "第 21 次 epoch，第 300 step，该次训练 loss = 0.27102020382881165\n",
      "第 22 次 epoch，第 100 step，该次训练 loss = 0.27042156457901\n",
      "第 22 次 epoch，第 200 step，该次训练 loss = 0.32023605704307556\n",
      "第 22 次 epoch，第 300 step，该次训练 loss = 0.2669133245944977\n",
      "第 23 次 epoch，第 100 step，该次训练 loss = 0.2658783793449402\n"
     ]
    }
   ],
   "source": [
    "# Image Classification\n",
    "# hand-written digits recognition\n",
    "# MNIST: 7,000 images per category = 70,000\n",
    "# train / test splitting = 60k / 10k\n",
    "# [28, 28, 1] = [rows,columns, gray_value]\n",
    "# 28 * 28 = 784 转换为一维数组\n",
    "# input and output\n",
    "# coding the features for data: one-hot\n",
    "# regression VS classification\n",
    "# y = w * x + b\n",
    "# y 属于空间 R^d\n",
    "# out = X @ W + b\n",
    "# out:[0.1, 0.8, 0.02,.0.08] = 1 概率，最大值概率称之为置信度\n",
    "# pred = argmax(out)\n",
    "\n",
    "# out = X @ W + b\n",
    "# X = [b, 784]\n",
    "# W = [784, 10]\n",
    "# b = [10]\n",
    "\n",
    "# it is linear !\n",
    "# out = X @ W + b\n",
    "# 变成 non-linear !\n",
    "# out = f(X @ W + b)\n",
    "# 引入非线性因子，这个 f 函数称之为激活函数，\n",
    "# f = ReLu function and sigmoid function\n",
    "# out = relu(X @ W + b)\n",
    "\n",
    "# it is simple\n",
    "# out = relu(X @ W + b)\n",
    "# 添加隐藏层 hide layer\n",
    "# h1 = relu(X @ W1 + b1)\n",
    "# h2 = relu(X @ W2 + b2)\n",
    "# out = relu(X @ W3 + b3)\n",
    "\n",
    "# particularly\n",
    "# 每一层都是类似降维的过程，直到从 [1, 784] ---- [1, 10]\n",
    "# X = [v1, v2, ... , v784]\n",
    "# h1 = relu(X @ W1 + b1)\n",
    "#    W1: [784, 512]\n",
    "#    b1: [1, 512]\n",
    "# h2 = relu(X @ W2 + b2)\n",
    "#    W2: [512, 256]\n",
    "#    b2: [1, 256]\n",
    "# out = relu(X @ W3 + b3)\n",
    "#    W2: [256, 10]\n",
    "#    b2: [1, 10]\n",
    "\n",
    "# loss\n",
    "# out:[1, 10]\n",
    "# Y/label: 0-9 （one-hot）\n",
    "# euclidean distance 计算欧式距离 使得 out 接近 label   MSE\n",
    "# loss = (Y - out) **2\n",
    "\n",
    "# in a nutshell\n",
    "# out = relu{relu{relu[X @ W1 + b1] @ W2 + b2 } @ W3 + b3}\n",
    "# pred = argmax(out)\n",
    "# loss = MSE(out, label)\n",
    "# minimize loss\n",
    "# [w1_new, b2_new, w2_new, b2_new, w3_new, b3_new]\n",
    "\n",
    "# Deep Learning\n",
    "# classification precedure\n",
    "# step1 compute [h1, h2, out]\n",
    "# step2 compute loss\n",
    "# step3 compute gradient and update [w1_new, b2_new, w2_new, b2_new, w3_new, b3_new]\n",
    "# step4 loop\n",
    "# need TensorFlow\n",
    "\n",
    "# 以下两条语句，使得 TensorFlow 少打印出一些无关紧要的信息\n",
    "# import  os\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "\n",
    "# step 0 loading data X and Y\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, optimizers\n",
    "\n",
    "(x, y), (x_val, y_val) = datasets.mnist.load_data() \n",
    "x = tf.convert_to_tensor(x, dtype=tf.float32) / 255.\n",
    "y = tf.convert_to_tensor(y, dtype=tf.int32)\n",
    "y = tf.one_hot(y, depth=10)\n",
    "print(x.shape, y.shape)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "# batch 概念\n",
    "train_dataset = train_dataset.batch(200)\n",
    "    \n",
    "# step 0 model NN\n",
    "model = tf.keras.Sequential([layers.Dense(512, activation=\"relu\"),\n",
    "                          layers.Dense(256, activation=\"relu\"),\n",
    "                          layers.Dense(10)])\n",
    "\n",
    "optimizer = optimizers.SGD(learning_rate=0.001)\n",
    "\n",
    "# 对整个训练集 训练 30 次，即就是 epoch = 30 \n",
    "# 将训练集数据划分为每次读取 200 ，即就是 batch = 200\n",
    "# 那么 step = train_data / batch = 60k / 200 = 300\n",
    "# 每训练 100 个样本数据，就查看一次训练情况 loss\n",
    "# 实际显示的 loss 情况次数 = epoch * (step / 100) = 30 * (300/100) = 90\n",
    "\n",
    "# step4 loop\n",
    "# 对一个数据集 dataset 训练一次称之为 epoch\n",
    "def train_epoch(epoch):\n",
    "    # 对一个 batch 训练一次称之为 step\n",
    "    for step, (x, y) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # [b， 28， 28] => [b, 784]\n",
    "            x = tf.reshape(x, (-1, 28*28))\n",
    "            # step 1 compute output\n",
    "            # [b, 784] => [b, 10]\n",
    "            out = model(x)\n",
    "            # step2 compute loss\n",
    "            loss = tf.reduce_sum(tf.square(out - y)) / x.shape[0]\n",
    "            \n",
    "        # step 3 optimize and update w1, w2, w3, b1, b2, b3\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        # w_new = w - lr * grad\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables)) \n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            # print(epoch, step, 'loss:', loss.numpy())\n",
    "            print(\"第 {0} 次 epoch，第 {1} step，该次训练 loss = {2}\".format(epoch, step+100, loss.numpy()))\n",
    "\n",
    "# 对整个数据集训练多次 30 次\n",
    "for epoch in range(30):\n",
    "    train_epoch(epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow 数据类型\n",
    "# data container: list, np.array, tf.Tensor\n",
    "# what is Tensor\n",
    "# - scalar 标量 1.1\n",
    "# - vector 向量 [1.1], [1.1, 2.2]\n",
    "# - matrix 矩阵 [[1.1, 2.2], [3.3, 4.4], [5.5, 6.6]]\n",
    "# - tensor rank > 2 矩阵的秩\n",
    "# TF is a computing lib\n",
    "# - int, float, double, bool, string\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# create the int, float, double, bool, string\n",
    "tf.constant(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.constant(1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.constant(1.2, dtype=tf.int32)\n",
    "# TypeError: Cannot convert 1.2 to EagerTensor of dtype int32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.constant([True, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.constant(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor Property 常见属性\n",
    "with tf.device(\"cpu\"):\n",
    "    const_cpu = tf.constant([1])\n",
    "\n",
    "with tf.device(\"gpu\"):\n",
    "    const_gpu = tf.range(4)\n",
    "    \n",
    "# device 属性，查看 Tensor 在哪一个设备上 CPU or GPU\n",
    "const_cpu.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const_gpu.device\n",
    "# 因为本地没有安装 GPU 加速驱动 CUDA 和 cuDNN\n",
    "# 无法提供 GPU 加速环境，故此工作环境依旧还在 CPU 上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 相互转换 \n",
    "# const_cpu_to_gpu = const_cpu.gpu()\n",
    "# const_cpu_to_gpu.device\n",
    "\n",
    "const_gpu_to_cpu = tf.identity(const_gpu)\n",
    "const_gpu_to_cpu.device\n",
    "\n",
    "# tensor 的计算必须在同一设备进行，全部都是 CPU 或者全部都是 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy 属性，将 Tensor 转换为 numpy\n",
    "# numpy 是在 CPU 上的\n",
    "const_gpu.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看 Tensor 的 shape 维度\n",
    "# ndim 属性，返回 Tensor 的维度信息\n",
    "# rank 方法，返回 Tensor 的维度信息\n",
    "const_cpu.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.rank(const_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.rank(tf.ones([3, 2, 4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 判断是否 Tensor\n",
    "tf.is_tensor(const_cpu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转换为 Tensor\n",
    "const = np.arange(5)\n",
    "const_tensor = tf.convert_to_tensor(const)\n",
    "const_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dtype\n",
    "# 数值类型之间的转换\n",
    "const_cpu.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.cast(const_tensor, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.cast(const_tensor, dtype=tf.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.cast(const_tensor, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int <==> bool\n",
    "bool_num = tf.constant([0, 1])\n",
    "bool_num_bool = tf.cast(bool_num, dtype=tf.bool)\n",
    "bool_num_bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.cast(bool_num_bool, tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.Variable\n",
    "# 一般是需要优化的变量参数\n",
    "a = tf.range(5)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = tf.Variable(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.trainable\n",
    "# 需要梯度信息，可以自动求导\n",
    "# 神经网络的参数需求"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.is_tensor(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建 Tensor\n",
    "# from numpy, list\n",
    "# zeros, ones\n",
    "# fill\n",
    "# random\n",
    "# constant\n",
    "# Applicatin\n",
    "\n",
    "# meta-learning\n",
    "\n",
    "# Tensor 索引和切片\n",
    "# selecetive indexing\n",
    "# tf.gather() , tf.gather_nd(), tf.boolean_mask()\n",
    "\n",
    "# 维度变换\n",
    "# shape, ndim\n",
    "# reshape\n",
    "# expand_dims, squeeze\n",
    "# transpose\n",
    "# broadcast_to\n",
    "\n",
    "# 数学计算\n",
    "# +, -, *, /, **, pow, square, sqrt, //, %, exp, log, @, matmul\n",
    "# element_wise, matrix_wise, dim_wise, \n",
    "# 元素级计算，矩阵级计算，维度级计算\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import  os\n",
    "# 复制 0， 1， 2，打印 c++ 一些信息\n",
    "# 0 打印全部信息\n",
    "# 2 只打印 error 信息\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, optimizers, Sequential, metrics\n",
    "\n",
    "# loading the dataset MNIST\n",
    "(x, y), _ = datasets.mnist.load_data()\n",
    "print('datasets:', x.shape, y.shape, x.min(), x.max())\n",
    "\n",
    "# x : [0-255] -> [0-1.]\n",
    "x = tf.convert_to_tensor(x, dtype=tf.float32) / 255.\n",
    "y = tf.convert_to_tensor(y, dtype=tf.int32)\n",
    "\n",
    "print(x.shape, y.shape, x.dtype, y.dtype)\n",
    "print(tf.reduce_min(x), tf.reduce_max(x))\n",
    "print(tf.reduce_min(y), tf.reduce_max(y))\n",
    "\n",
    "# set the batch\n",
    "train_db = tf.data.Dataset.from_tensor_slices((x, y)).batch(128)\n",
    "\n",
    "# set the iteration\n",
    "train_iter = iter(train_db)\n",
    "sample = next(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前向传播 forward\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets\n",
    "\n",
    "# loading data\n",
    "(x, y), _ = datasets.mnist.load_data()\n",
    "\n",
    "# convert to Tensor\n",
    "# x : [0-255] -> [0-1.]\n",
    "x = tf.convert_to_tensor(x, dtype=tf.float32) / 255.\n",
    "y = tf.convert_to_tensor(y, dtype=tf.int32)\n",
    "\n",
    "# show the info\n",
    "print(x.shape, y.shape, x.dtype, y.dtype)\n",
    "print(tf.reduce_min(x), tf.reduce_max(x))\n",
    "print(tf.reduce_min(y), tf.reduce_max(y))\n",
    "\n",
    "# set the dataset and split the batch\n",
    "train_db = tf.data.Dataset.from_tensor_slices((x, y)).batch(128)\n",
    "train_iter = iter(train_db)\n",
    "sample = next(train_iter)\n",
    "print(\"batch\", sample[0].shape, sample[1].shape)\n",
    "\n",
    "# 权值\n",
    "# [b, 784] => [b, 256] => [b, 128] => [b, 10]\n",
    "# [dim_in, dim_out], [dim_out]\n",
    "# tf.Variable 才会自动记录梯度信息，tf.Tensor 不会记录梯度信息，故此值为 None，类型为 NoneType\n",
    "# 需要给一个好点的初始值，否则容易 loss 爆炸，出现 nan(not a number) 情况,stddev=0.1 方差给小一点\n",
    "w1 = tf.Variable(tf.random.truncated_normal([784, 256], stddev=0.1))\n",
    "b1 = tf.Variable(tf.zeros([256]))\n",
    "w2 = tf.Variable(tf.random.truncated_normal([256, 128], stddev=0.1))\n",
    "b2 = tf.Variable(tf.zeros([128]))\n",
    "w3 = tf.Variable(tf.random.truncated_normal([128, 10], stddev=0.1))\n",
    "b3 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# 学习率 ，步长 10^-3\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# 对数据集迭代 10 次\n",
    "for epoch in range(10):\n",
    "    # 对数据集一次循环\n",
    "    for step, (x, y) in enumerate(train_db):\n",
    "        # x [128, 28, 28]\n",
    "        # y [128]\n",
    "        # 进行维度转换 [128, 28, 28] => [b, 28*28]\n",
    "        x = tf.reshape(x, [-1, 28*28])   \n",
    "    \n",
    "        # 自动求导过程,梯度\n",
    "        with tf.GradientTape() as tape:\n",
    "            # x [b, 28*28]\n",
    "            # h1 = x@w1 + b1\n",
    "            # [b, 784]@[784, 256] + [256] => [b, 256] + [256] => [b, 256] + [b, 256]\n",
    "            h1 = x@w1 + tf.broadcast_to(b1, [x.shape[0], 256])\n",
    "            # 添加非线性，relu 激活函数\n",
    "            h1 = tf.nn.relu(h1)\n",
    "            # [b, 256] => [b, 128]\n",
    "            h2 = h1@w2 + b2\n",
    "            h2 = tf.nn.relu(h2)\n",
    "            # [b, 128] => [b, 10]\n",
    "            out = h2@w3 +b3\n",
    "        \n",
    "            # compute loss\n",
    "            # out [b, 10]\n",
    "            # y [b] => [b, 10]\n",
    "            y_one_hot = tf.one_hot(y, depth=10)\n",
    "        \n",
    "            # MSE = mean((y - out)**2)\n",
    "            loss = tf.square(y_one_hot - out)\n",
    "            # mean: scalar\n",
    "            loss = tf.reduce_mean(loss)\n",
    "    \n",
    "        # compute gradients\n",
    "        grads = tape.gradient(loss, [w1, b1, w2, b2, w3, b3])\n",
    "        # print(grads)\n",
    "        # w1 = w1 - lr * w1_grad\n",
    "        # 原地更新，否则就返回 tf.Tensor 类型了，无法进行下一个梯度更新\n",
    "        # w1 = w1 - learning_rate * grads[0]\n",
    "        w1.assign_sub(learning_rate * grads[0])\n",
    "        b1.assign_sub(learning_rate * grads[1])\n",
    "        w2.assign_sub(learning_rate * grads[2])\n",
    "        b2.assign_sub(learning_rate * grads[3])\n",
    "        w3.assign_sub(learning_rate * grads[4])\n",
    "        b3.assign_sub(learning_rate * grads[5])\n",
    "    \n",
    "        if step % 100 == 0:\n",
    "            print(epoch, step, \"loss\", float(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor 张量的合并和分割\n",
    "# tf.concat 合并\n",
    "# tf.split  分割\n",
    "# tf.stack 合并\n",
    "# tf.unstack 分割\n",
    "\n",
    "# 数据统计\n",
    "# tf.norm 范数概念\n",
    "# tf.reduce_min/max/mean\n",
    "# tf.argmax/argminx\n",
    "# tf.equal\n",
    "# tf.unique\n",
    "\n",
    "# 张量排序 Tensor 排序\n",
    "# tf.sort、tf.argsort\n",
    "# top_k\n",
    "# top_k compute of accuracy\n",
    "\n",
    "# 填充与复制\n",
    "# pad\n",
    "# tile\n",
    "# broadcast_to\n",
    "\n",
    "# 张量限幅\n",
    "# clip_by_value\n",
    "# relu\n",
    "# clip_by_norm\n",
    "# gradient clipping\n",
    "\n",
    "# 高阶操作\n",
    "# where\n",
    "# scatter_nd\n",
    "# meshgrid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集加载\n",
    "# keras.datasets\n",
    "# tf.data.Dataset.from_tensor_sclices : shuffle, map, batch, repeat\n",
    "# input pipeline\n",
    "# such as : boston housing, mnist/fashion mnist, cifar10/100, imdb\n",
    "\n",
    "# 前向传播 forward\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets\n",
    "\n",
    "# loading data\n",
    "(x, y), (x_test, y_test) = datasets.mnist.load_data()\n",
    "\n",
    "# convert to Tensor\n",
    "# x : [0-255] -> [0-1.]\n",
    "x = tf.convert_to_tensor(x, dtype=tf.float32) / 255.\n",
    "y = tf.convert_to_tensor(y, dtype=tf.int32)\n",
    "\n",
    "x_test = tf.convert_to_tensor(x_test, dtype=tf.float32) / 255.\n",
    "y_test = tf.convert_to_tensor(y_test, dtype=tf.int32)\n",
    "\n",
    "# show the info\n",
    "print(x.shape, y.shape, x.dtype, y.dtype)\n",
    "print(tf.reduce_min(x), tf.reduce_max(x))\n",
    "print(tf.reduce_min(y), tf.reduce_max(y))\n",
    "\n",
    "# set the dataset and split the batch\n",
    "train_db = tf.data.Dataset.from_tensor_slices((x, y)).batch(128)\n",
    "test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(128)\n",
    "\n",
    "train_iter = iter(train_db)\n",
    "sample = next(train_iter)\n",
    "print(\"batch\", sample[0].shape, sample[1].shape)\n",
    "\n",
    "# 权值\n",
    "# [b, 784] => [b, 256] => [b, 128] => [b, 10]\n",
    "# [dim_in, dim_out], [dim_out]\n",
    "# tf.Variable 才会自动记录梯度信息，tf.Tensor 不会记录梯度信息，故此值为 None，类型为 NoneType\n",
    "# 需要给一个好点的初始值，否则容易 loss 爆炸，出现 nan(not a number) 情况,stddev=0.1 方差给小一点\n",
    "w1 = tf.Variable(tf.random.truncated_normal([784, 256], stddev=0.1))\n",
    "b1 = tf.Variable(tf.zeros([256]))\n",
    "w2 = tf.Variable(tf.random.truncated_normal([256, 128], stddev=0.1))\n",
    "b2 = tf.Variable(tf.zeros([128]))\n",
    "w3 = tf.Variable(tf.random.truncated_normal([128, 10], stddev=0.1))\n",
    "b3 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# 学习率 ，步长 10^-3\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# 对数据集迭代 100 次\n",
    "# for epoch in range(10): # test acc = 0.5566\n",
    "for epoch in range(100): # test acc = 0.8531\n",
    "    # 对数据集一次循环\n",
    "    for step, (x, y) in enumerate(train_db):\n",
    "        # x [128, 28, 28]\n",
    "        # y [128]\n",
    "        # 进行维度转换 [128, 28, 28] => [b, 28*28]\n",
    "        x = tf.reshape(x, [-1, 28*28])   \n",
    "    \n",
    "        # 自动求导过程,梯度\n",
    "        with tf.GradientTape() as tape:\n",
    "            # x [b, 28*28]\n",
    "            # h1 = x@w1 + b1\n",
    "            # [b, 784]@[784, 256] + [256] => [b, 256] + [256] => [b, 256] + [b, 256]\n",
    "            h1 = x@w1 + tf.broadcast_to(b1, [x.shape[0], 256])\n",
    "            # 添加非线性，relu 激活函数\n",
    "            h1 = tf.nn.relu(h1)\n",
    "            # [b, 256] => [b, 128]\n",
    "            h2 = h1@w2 + b2\n",
    "            h2 = tf.nn.relu(h2)\n",
    "            # [b, 128] => [b, 10]\n",
    "            out = h2@w3 +b3\n",
    "        \n",
    "            # compute loss\n",
    "            # out [b, 10]\n",
    "            # y [b] => [b, 10]\n",
    "            y_one_hot = tf.one_hot(y, depth=10)\n",
    "        \n",
    "            # MSE = mean((y - out)**2)\n",
    "            loss = tf.square(y_one_hot - out)\n",
    "            # mean: scalar\n",
    "            loss = tf.reduce_mean(loss)\n",
    "    \n",
    "        # compute gradients\n",
    "        grads = tape.gradient(loss, [w1, b1, w2, b2, w3, b3])\n",
    "        # print(grads)\n",
    "        # w1 = w1 - lr * w1_grad\n",
    "        # 原地更新，否则就返回 tf.Tensor 类型了，无法进行下一个梯度更新\n",
    "        # w1 = w1 - learning_rate * grads[0]\n",
    "        w1.assign_sub(learning_rate * grads[0])\n",
    "        b1.assign_sub(learning_rate * grads[1])\n",
    "        w2.assign_sub(learning_rate * grads[2])\n",
    "        b2.assign_sub(learning_rate * grads[3])\n",
    "        w3.assign_sub(learning_rate * grads[4])\n",
    "        b3.assign_sub(learning_rate * grads[5])\n",
    "    \n",
    "        if step % 100 == 0:\n",
    "            print(epoch, step, \"loss\", float(loss))\n",
    "    \n",
    "    # test/evluation\n",
    "    total_correct, total_num = 0, 0\n",
    "    for step, (x, y) in enumerate(test_db):\n",
    "        x = tf.reshape(x, [-1, 28*28])\n",
    "        \n",
    "        h1 = tf.nn.relu(x@w1 + b1)\n",
    "        h2 = tf.nn.relu(h1@w2 + b2)        \n",
    "        out = h2@w3 + b3\n",
    "        \n",
    "        prob = tf.nn.softmax(out, axis=1)\n",
    "        pred = tf.argmax(prob, axis=1)\n",
    "        pred = tf.cast(pred, dtype=tf.int32)\n",
    "        correct = tf.cast(tf.equal(pred, y), dtype=tf.int32)\n",
    "        correct = tf.reduce_sum(correct)\n",
    "        \n",
    "        total_correct += int(correct)\n",
    "        total_num += x.shape[0]\n",
    "        \n",
    "    acc = total_correct / total_num\n",
    "    print(\"test acc : \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全连接层\n",
    "# matmul\n",
    "# neural network\n",
    "# deep learning\n",
    "# multi-layer\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "x = tf.random.normal([2, 3])\n",
    "\n",
    "model = keras.Sequential([\n",
    "        keras.layers.Dense(2, activation=\"relu\"),\n",
    "        keras.layers.Dense(2, activation=\"relu\"),\n",
    "        keras.layers.Dense(2)]) # 没有加 relu 非线性的这一层，称之为 logits\n",
    "\n",
    "model.build(input_shape=[None, 3])\n",
    "model.summary()\n",
    "\n",
    "for p in model.trainable_variables:\n",
    "    print(p.name, p.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出方式\n",
    "# such as : R 整个实数集，[0-1]输出概率值，[0-1]&所有概率和为1，[-1-1]范围\n",
    "# tf.sigmoid => [0-1] => sigmoid 激活函数以及压缩\n",
    "# tf.nn.softmax => [0-1]&所有概率之和为 1 => softmax 函数\n",
    "# tf.tanh => [-1 -1] => 通过 sigmoid 函数扩大到 [0-2],然后平移到 [-1 -1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 损失函数\n",
    "# MSE\n",
    "# Cross Entropy Loss 熵 交叉熵 散度\n",
    "# Hinge Loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 梯度下降\n",
    "# gradient descent\n",
    "# auto gradient\n",
    "# with tf.GradientTape() as tape:\n",
    "# 二阶求导  2nd-order\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "w = tf.Variable(1.0)\n",
    "b = tf.Variable(2.0)\n",
    "x = tf.Variable(3.0)\n",
    "\n",
    "# 二阶求导\n",
    "with tf.GradientTape() as tape1:\n",
    "    with tf.GradientTape() as tape2:\n",
    "        y = x * w + b\n",
    "    dy_dw, dy_db = tape2.gradient(y, [w, b])\n",
    "d2y_dw2 = tape1.gradient(dy_dw, w)\n",
    "\n",
    "print(dy_dw)\n",
    "print(dy_db)\n",
    "print(d2y_dw2)\n",
    "\n",
    "assert dy_dw.numpy() == 3.0\n",
    "assert d2y_dw2 is None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation functions 激活函数\n",
    "# 激活函数及其梯度\n",
    "# 温水煮青蛙\n",
    "# sigmoid logistic\n",
    "\n",
    "# loss及其梯度\n",
    "# softmax\n",
    "\n",
    "# 单输出感知器模型及其梯度\n",
    "# Single-output Perceptron\n",
    "import tensorflow as tf\n",
    "\n",
    "x = tf.random.normal([1, 3])\n",
    "w = tf.ones([3, 1])\n",
    "b = tf.ones([1])\n",
    "y = tf.constant([1])\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch([w, b])\n",
    "    logits = tf.sigmoid(x@w +b)\n",
    "    loss = tf.reduce_mean(tf.losses.MSE(y, logits))\n",
    "    \n",
    "grads = tape.gradient(loss, [w, b])\n",
    "print(grads[0])\n",
    "print(grads[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多输出感知器模型及其梯度\n",
    "# Multi-output Perceptron\n",
    "import tensorflow as tf\n",
    "\n",
    "x = tf.random.normal([2, 4])\n",
    "w = tf.random.normal([4, 3])\n",
    "b = tf.zeros([3])\n",
    "y = tf.constant([2, 0])\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch([w, b])\n",
    "    prob = tf.nn.softmax(x@w + b, axis=1)\n",
    "    loss = tf.reduce_mean(tf.losses.MSE(tf.one_hot(y, depth=3), prob))\n",
    "    \n",
    "grads = tape.gradient(loss, [w, b])\n",
    "print(grads[0])\n",
    "print(grads[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 链式法则\n",
    "# chain rule\n",
    "# 神经网络层 权值不断更新\n",
    "\n",
    "# 多层感知器 \n",
    "# Multi-Layer Perceptron\n",
    "# 反向传播算法\n",
    "\n",
    "# Himmelblau 函数优化\n",
    "# f(x, y) = (x**2 + y - 11)**2 + (x + y**2 -7)**2\n",
    "# Himmelblau function 常用于测试算法模型\n",
    "# Himmelblau 方程，是科学家们研究出来专门用于检测一个优化器效果的方程\n",
    "# 四个点（最小值）精确解\n",
    "# f(3.0, 2.0) = 0.0; f(-2.805118, 3.131312) = 0.0; f(-3.779310, -3.283186) = 0.0; f(3.584428, -1.848126) = 0.0; \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# This is important for 3d plotting \n",
    "from mpl_toolkits.mplot3d import Axes3D \n",
    "\n",
    "def himmelblau(x):\n",
    "    return (x[0] **2 + x[1] - 11) **2 + (x[0] + x[1] **2 - 7) **2\n",
    "\n",
    "x = np.arange(-6, 6, 0.1)\n",
    "y = np.arange(-6, 6, 0.1)\n",
    "print(\"x, y range: \", x.shape, y.shape)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "print(\"X, Y maps: \", X.shape, Y.shape)\n",
    "Z = himmelblau([X, Y])\n",
    "\n",
    "# 可视化 himmelblau 函数\n",
    "fig = plt.figure(\"himmelblau\")\n",
    "# ax = fig.add_subplot(1, 1, 1, projection='3d')\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.plot_surface(X, Y, Z)\n",
    "ax.view_init(60, -30)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# gradient descent\n",
    "# 初始点位置\n",
    "# x = tf.constant([-4., 0.])\n",
    "x = tf.constant([4., 0.])\n",
    "\n",
    "for step in range(200):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch([x])\n",
    "        y = himmelblau(x)\n",
    "        \n",
    "    grads = tape.gradient(y, [x])[0]\n",
    "    x -= 0.01 * grads\n",
    "    \n",
    "    if step % 20 == 0:\n",
    "        print(\"step {}: x = {}, f(x) = {}\".format(step, x.numpy(), y.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fashion MNIST\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, optimizers, Sequential, metrics\n",
    "\n",
    "# data processing 数据预处理函数\n",
    "def preprocess(x, y):\n",
    "    x = tf.cast(x, dtype=tf.float32) / 255.\n",
    "    y = tf.cast(y, dtype=tf.int32)\n",
    "    return x, y\n",
    "    \n",
    "# loading dataset\n",
    "(x, y), (x_test, y_test) = datasets.fashion_mnist.load_data()\n",
    "print(x.shape, y.shape, x_test.shape, y_test.shape)\n",
    "\n",
    "# 构造 TensorFlow 需求的数据集\n",
    "db_train = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "# 数据预处理 映射函数即可\n",
    "db_train = db_train.map(preprocess)\n",
    "# shuffle and batch 操作\n",
    "batch_size = 128\n",
    "db_train = db_train.shuffle(10000).batch(batch_size)\n",
    "\n",
    "db_test = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "# 数据预处理 映射函数即可\n",
    "db_test = db_test.map(preprocess)\n",
    "# shuffle and batch 操作\n",
    "db_test = db_test.shuffle(10000).batch(batch_size)\n",
    "\n",
    "# sample 操作\n",
    "db_train_iter = iter(db_train)\n",
    "sample = next(db_train_iter)\n",
    "print(\"batch: \", sample[0].shape, sample[1].shape)\n",
    "\n",
    "# 神经网络层构建\n",
    "model = Sequential([\n",
    "    layers.Dense(256, activation=tf.nn.relu),\n",
    "    layers.Dense(128, activation=tf.nn.relu),\n",
    "    layers.Dense(64, activation=tf.nn.relu),\n",
    "    layers.Dense(32, activation=tf.nn.relu),\n",
    "    layers.Dense(10)])\n",
    "\n",
    "model.build(input_shape=[None, 28*28])\n",
    "model.summary()\n",
    "\n",
    "# 优化器 learning_rate\n",
    "# w = w - lrarning_rete * gradient\n",
    "optimizer = optimizers.Adam(lr=1e-3)\n",
    "\n",
    "def main():\n",
    "    # 对数据集循环训练多少次 epoch\n",
    "    for epoch in range(10):\n",
    "        # 对每个 batch 循环训练\n",
    "        for step, (x, y) in enumerate(db_train):\n",
    "            # 维度转换\n",
    "            # x: [b, 28, 28] => [b, 784]\n",
    "            # y: [b]\n",
    "            x = tf.reshape(x, [-1, 28*28])\n",
    "            # 跟踪梯度信息，自动求导\n",
    "            with tf.GradientTape() as tape:\n",
    "                # [b, 784] => [b, 10]\n",
    "                logits = model(x)\n",
    "                # compute loss\n",
    "                y_one_hot = tf.one_hot(y, depth=10)\n",
    "                # [b]\n",
    "                loss_mse = tf.reduce_mean(tf.losses.MSE(y_one_hot, logits))\n",
    "                loss_ce = tf.losses.categorical_crossentropy(y_one_hot, logits, from_logits=True)\n",
    "                loss_ce = tf.reduce_mean(loss_ce)\n",
    "            \n",
    "            # 获取梯度\n",
    "            grads = tape.gradient(loss_ce, model.trainable_variables)\n",
    "            # grads = tape.gradient(loss_mse, model.trainable_variables)\n",
    "            # 更新梯度\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "            \n",
    "            # 每 100 次训练后查看 loss\n",
    "            if step % 100 == 0:\n",
    "                print(epoch, step, \" loss: \", float(loss_ce), float(loss_mse))\n",
    "        \n",
    "        # test\n",
    "        # 统计正确个数\n",
    "        total_correct = 0\n",
    "        total_num = 0\n",
    "        for x, y in db_test:\n",
    "            # 维度变换\n",
    "            x = tf.reshape(x, [-1, 28*28])\n",
    "            # [b, 10]\n",
    "            logits = model(x)\n",
    "            # 计算概率最大的位置索引\n",
    "            # logits => prob,  [b, 10]\n",
    "            prob = tf.nn.softmax(logits, axis=1)\n",
    "            # [b, 10] => [b] pred\n",
    "            pred = tf.argmax(prob, axis=1)\n",
    "            pred = tf.cast(pred, dtype=tf.int32)\n",
    "            # 不需要 one_hot 编码\n",
    "            # correct : [b], True: equal; False: not equal\n",
    "            correct = tf.equal(pred, y)\n",
    "            # 将 bool 转换为 int （1或者0）\n",
    "            correct = tf.cast(correct, dtype=tf.int32)\n",
    "            # 计算正确个数\n",
    "            correct = tf.reduce_sum(correct)\n",
    "            \n",
    "            total_correct += int(correct)\n",
    "            total_num += x.shape[0]\n",
    "            \n",
    "        # compute the accurate\n",
    "        acc = total_correct / total_num\n",
    "        print(epoch, \"test acc : \", acc)\n",
    "            \n",
    "\n",
    "# if __name__ == \" __main__\":\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow 可视化 Tensor 在数据流中 flow 过程\n",
    "# Visdom 可视化\n",
    "# 监控数据流 TensorBoard\n",
    "# tensorboard 会监听磁盘数据变换，使用 web UI 界面展示出来\n",
    "# 1、installation 2、curves 3、image visualization\n",
    "# principle：1、listen logdir 2、build summary instance 3、fed data into summary instance\n",
    "# steps：\n",
    "# step 1、run listener：cmd; cd logdir; tensorboard --logdir logs; web site:6006\n",
    "# step 2、buld summary: 代码中实现 \n",
    "# =================================================================\n",
    "# current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# log_dir = \"logs\" + current_time\n",
    "# summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "# =================================================================\n",
    "# step 3、feed scalar: 代码中实现 \n",
    "# =================================================================\n",
    "# wtih summary_writer.as_default():\n",
    "#     tf.summary.scalar(\"loss\", float(loss), step=epoch)\n",
    "#     tf.summary.scalar(\"accuracy\", float(train_accuracy), step=epoch)\n",
    "# =================================================================\n",
    "# step 4、feed single image or multi-images: 代码中实现\n",
    "# \n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, optimizers, Sequential, metrics\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import io\n",
    "\n",
    "# data processing 数据预处理函数\n",
    "def preprocess(x, y):\n",
    "    x = tf.cast(x, dtype=tf.float32) / 255.\n",
    "    y = tf.cast(y, dtype=tf.int32)\n",
    "    return x, y\n",
    "\n",
    "# plot figure to image PNG\n",
    "def plot_to_image(figure):\n",
    "    # convert the matplotlib plot specified by 'figure' to a PNG image and return it.\n",
    "    # the supplied figure is closed and inaccessible after the call.\n",
    "    \n",
    "    # save the plot to a PNG in memory\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    \n",
    "    # closing the figure prevents it from being dispalyed directly inside\n",
    "    # the notebook\n",
    "    plt.close(figure)\n",
    "    buf.seek(0)\n",
    "    \n",
    "    # convert PNG buffer to TF image\n",
    "    image = tf.image.decode_png(buf.getvalue(), channels=4)\n",
    "    # add the batch dimension\n",
    "    image = tf.expand_dims(image, 0)\n",
    "    return image\n",
    "\n",
    "# fed multi-images\n",
    "def image_grid(images):\n",
    "    # return a 5X5 grid of the MNIST images as a matplotlib figure.\n",
    "    \n",
    "    # create a figure to contain the plot\n",
    "    figure = plt.figure(figsize=(10, 10))\n",
    "    for i in range(25):\n",
    "        # start next subplot\n",
    "        plt.subplot(5, 5, i + 1, title=\"name\")\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(False)\n",
    "        plt.imshow(images[i], cmap=plt.cm.binary)\n",
    "    \n",
    "    return figure\n",
    "\n",
    "# batch\n",
    "batch_size = 128\n",
    "# loading dataset\n",
    "(x, y), (x_val, y_val) = datasets.mnist.load_data()\n",
    "print(\"datasets: \", x.shape, y.shape, x.min, y.max)\n",
    "\n",
    "# 构造 TensorFlow 需求的数据集\n",
    "db = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "# 数据预处理 映射函数即可  shuffle and batch 操作\n",
    "db = db.map(preprocess).shuffle(60000).batch(batch_size).repeat(10)\n",
    "\n",
    "# 构造 TensorFlow 需求的数据集\n",
    "ds_val = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "# 数据预处理 映射函数即可  shuffle and batch 操作\n",
    "ds_val = ds_val.map(preprocess).batch(batch_size, drop_remainder=True)\n",
    "\n",
    "# 神经网络层构建 5 layers\n",
    "network = Sequential([\n",
    "    layers.Dense(256, activation=tf.nn.relu),\n",
    "    layers.Dense(128, activation=tf.nn.relu),\n",
    "    layers.Dense(64, activation=tf.nn.relu),\n",
    "    layers.Dense(32, activation=tf.nn.relu),\n",
    "    layers.Dense(10)])\n",
    "\n",
    "network.build(input_shape=[None, 28*28])\n",
    "network.summary()\n",
    "\n",
    "# 优化器 learning_rate\n",
    "# w = w - lrarning_rete * gradient\n",
    "optimizer = optimizers.Adam(lr=0.01)\n",
    "\n",
    "# TensorBoard 可视化操作步骤，监听磁盘路径\n",
    "# logs/ 文件夹 与该程序文件是同级\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = \"logs/\" + current_time\n",
    "summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "# get x from (x, y)\n",
    "sample_img = next(iter(db))[0]\n",
    "\n",
    "# get first image instance\n",
    "sample_img = sample_img[0]\n",
    "sample_img = tf.reshape(sample_img, [1, 28, 28, 1])\n",
    "\n",
    "# TensorBoard 可视化操作步骤，写入数据 image\n",
    "with summary_writer.as_default():\n",
    "    tf.summary.image(\"Training sample : \", sample_img, step=0)\n",
    "    \n",
    "\n",
    "# \n",
    "# 对每个 batch 循环训练\n",
    "for step, (x, y) in enumerate(db):\n",
    "    # 跟踪梯度信息，自动求导\n",
    "    with tf.GradientTape() as tape:\n",
    "        # 维度转换\n",
    "        # x: [b, 28, 28] => [b, 784]\n",
    "        # y: [b]\n",
    "        x = tf.reshape(x, [-1, 28*28])\n",
    "        # [b, 784] => [b, 10]\n",
    "        out = network(x)\n",
    "        # compute loss\n",
    "        y_one_hot = tf.one_hot(y, depth=10)\n",
    "        # [b]\n",
    "        # loss_mse = tf.reduce_mean(tf.losses.MSE(y_one_hot, logits))\n",
    "        loss = tf.reduce_mean(tf.losses.categorical_crossentropy(y_one_hot, out, from_logits=True))\n",
    "            \n",
    "    # 获取梯度\n",
    "    grads = tape.gradient(loss, network.trainable_variables)\n",
    "    # grads = tape.gradient(loss_mse, network.trainable_variables)\n",
    "    # 更新梯度\n",
    "    optimizer.apply_gradients(zip(grads, network.trainable_variables))\n",
    "            \n",
    "    # 每 100 次训练后查看 loss\n",
    "    if step % 100 == 0:\n",
    "        print(step, \" loss: \", float(loss))\n",
    "        # TensorBoard 可视化操作步骤，写入数据 loss\n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.scalar(\"train-loss : \", float(loss), step=step)\n",
    "        \n",
    "    # evaluate\n",
    "    if step % 500 == 0:\n",
    "        total, total_correct = 0., 0\n",
    "        for _, (x, y) in enumerate(ds_val):\n",
    "            # 维度变换\n",
    "            x = tf.reshape(x, [-1, 28*28])\n",
    "            # [b, 10]\n",
    "            out = network(x)\n",
    "            # 计算概率最大的位置索引\n",
    "            # out => prob,  [b, 10]\n",
    "            # prob = tf.nn.softmax(out, axis=1)\n",
    "            # [b, 10] => [b] pred\n",
    "            pred = tf.argmax(out, axis=1)\n",
    "            pred = tf.cast(pred, dtype=tf.int32)\n",
    "            # 不需要 one_hot 编码\n",
    "            # correct : [b], True: equal; False: not equal\n",
    "            correct = tf.equal(pred, y)\n",
    "            # 将 bool tensor 转换为 int tensor （1或者0） 转换为 numpy\n",
    "            total_correct += tf.reduce_sum(tf.cast(correct, dtype=tf.int32)).numpy()\n",
    "            total += x.shape[0]\n",
    "            \n",
    "        # compute the accurate\n",
    "        print(step, \"Evaluate Acc : \", total_correct / total)\n",
    "        \n",
    "        # TensorBoard 可视化操作步骤，写入数据 loss and images\n",
    "        # print(x.shape)\n",
    "        val_images = x[:25]\n",
    "        val_images = tf.reshape(val_images, [-1, 28, 28, 1])\n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.scalar(\"test-acc\", float(total_correct / total), step=step)\n",
    "            tf.summary.image(\"val-onebyone-images:\", val_images, max_outputs=25, step=step)\n",
    "            \n",
    "            val_images = tf.reshape(val_images, [-1, 28, 28])\n",
    "            figure = image_grid(val_images)\n",
    "            tf.summary.image(\"val-images\", plot_to_image(figure), step=step)\n",
    "            \n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras 高层接口API 优化并且简化常规的方法代码（集成为接口 API）\n",
    "# 此处 keras 是 TensorFlow 集成的 tf.keras，而不是真正实际上讲的 Keras Keras != tf.keras\n",
    "# API: datasets, layers, losses, metrics, optimizers\n",
    "# from tensorflow.keras import datasets, layers, losses, optimizers, Sequential, metrics\n",
    "# metrics 提供计算记录清空等等一系列关于 metrics 的操作\n",
    "# compile & fit ：Compile => Fit => Evaluate => Predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义网络 (可以连接全连接层网络) 实现自己的逻辑需求\n",
    "# keras.Sequential\n",
    "# keras.layers.Layer\n",
    "# keras.Model\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, losses, optimizers, Sequential, metrics\n",
    "\n",
    "def preprocess(x, y):\n",
    "    x = tf.cast(x, dtype=tf.float32) / 255.\n",
    "    x = tf.reshape(x, [28*28])\n",
    "    y = tf.cast(y, dtype=tf.int32)\n",
    "    y = tf.one_hot(y, depth=10)\n",
    "    return x, y\n",
    "\n",
    "batchsz = 128\n",
    "(x, y), (x_val, y_val) = datasets.mnist.load_data()\n",
    "print(\"datasets: \", x.shape, y.shape, x.min, y.max)\n",
    "\n",
    "db = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "db = db.map(preprocess).shuffle(60000).batch(batchsz)\n",
    "ds_val = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "ds_val = ds_val.map(preprocess).batch(batchsz)\n",
    "\n",
    "sample = next(iter(db))\n",
    "print(sample[0].shape, sample[1].shape)\n",
    "\n",
    "network = Sequential([\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(10)])\n",
    "\n",
    "network.build(input_shape=[None, 28*28])\n",
    "network.summary()\n",
    "\n",
    "class MyDense(layers.Layer):\n",
    "    # 自定义层\n",
    "    def __init__(self, inp_dim, outp_dim):\n",
    "        super(MyDense, self).__init__()\n",
    "        \n",
    "        self.kernel = self.add_variable(\"w\", [inp_dim, outp_dim])\n",
    "        self.bias = self.add_variable(\"b\", [outp_dim])\n",
    "        \n",
    "    #\n",
    "    def call(self, inputs, training=None):\n",
    "        out = inputs @ self.kernel + self.bias\n",
    "        return out\n",
    "\n",
    "# 自定义网络模型\n",
    "class MyModel(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.fc1 = MyDense(28*28, 256)\n",
    "        self.fc2 = MyDense(256, 128)        \n",
    "        self.fc3 = MyDense(128, 64)        \n",
    "        self.fc4 = MyDense(64, 32)        \n",
    "        self.fc5 = MyDense(32, 10)\n",
    "        \n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.fc1(inputs)\n",
    "        x = tf.nn.relu(x) \n",
    "        x = self.fc2(x)        \n",
    "        x = tf.nn.relu(x)        \n",
    "        x = self.fc3(x)\n",
    "        x = tf.nn.relu(x) \n",
    "        x = self.fc4(x)        \n",
    "        x = tf.nn.relu(x)       \n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "    \n",
    "#\n",
    "network = MyModel()\n",
    "\n",
    "network.compile(optimizer = optimizers.Adam(lr=0.01),\n",
    "               loss = tf.losses.CategoricalCrossentropy(from_logits=True),\n",
    "               metrics = ['accuracy'])\n",
    "\n",
    "network.fit(db, epochs=5, validation_data=ds_val, validation_freq=2)\n",
    "\n",
    "network.evaluate(ds_val)\n",
    "\n",
    "sample = next(iter(ds_val))\n",
    "x = sample[0]\n",
    "y = sample[1]\n",
    "pred = network.predict(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型的保存和加载\n",
    "# save/load weights\n",
    "#==================================================\n",
    "# # save the weights\n",
    "# model.save_weights(\"./checkpoints/my_checkpoint\")\n",
    "# # restore the weights\n",
    "# model = create_model()\n",
    "# model.load_weights(\"./checkpoints/my_checkpoint\")\n",
    "#==================================================\n",
    "# save/load entire model\n",
    "#==================================================\n",
    "# # save entire model\n",
    "# network.save(\"model.h5\")\n",
    "# # load entire model\n",
    "# network = tf.keras.models.load_model(\"model.h5\")\n",
    "#==================================================\n",
    "# saved_model\n",
    "#==================================================\n",
    "# # 工业部署环境\n",
    "# tf.saved_model.save(m, \"./tmp/saved_model/\")\n",
    "# # 加载 loading\n",
    "# imported = tf.saved_model.load(path)\n",
    "# f = imported.signatures[\"serving_default\"]\n",
    "#==================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR10 datasets\n",
    "# 数据集下载太慢的，直接点击显示的网站链接，采用迅雷进行下载，然后手动将数据集储存到指定/user/.keras/datasets/\n",
    "# 自定义网络实战\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, losses, optimizers, Sequential, metrics\n",
    "\n",
    "def preprocess(x, y):\n",
    "    # [0-255] => [-1 -1]\n",
    "    x = 2 * tf.cast(x, dtype=tf.float32) / 255. - 1.\n",
    "    y = tf.cast(y, dtype=tf.int32)\n",
    "    return x, y\n",
    "\n",
    "batchsz = 128\n",
    "# [32, 32, 3]\n",
    "(x, y), (x_val, y_val) = datasets.cifar10.load_data()\n",
    "y = tf.squeeze(y)\n",
    "y_val = tf.squeeze(y_val)\n",
    "y = tf.one_hot(y, depth=10)\n",
    "y_val = tf.one_hot(y_val, depth=10)\n",
    "print(\"datasets: \", x.shape, y.shape, x_val.shape, y_val.shape, x.min(), x.max())\n",
    "\n",
    "train_db = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "train_db = train_db.map(preprocess).shuffle(60000).batch(batchsz)\n",
    "test_val = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "test_val = test_val.map(preprocess).batch(batchsz)\n",
    "\n",
    "sample = next(iter(train_db))\n",
    "print(\"batch: \", sample[0].shape, sample[1].shape)\n",
    "\n",
    "class MyDense(layers.Layer):\n",
    "    # to replace standard layers.Dense()\n",
    "    def __init__(self, inp_dim, outp_dim):\n",
    "        super(MyDense, self).__init__()\n",
    "        \n",
    "        self.kernel = self.add_variable(\"w\", [inp_dim, outp_dim])\n",
    "        # self.bias = self.add_variable(\"b\", [outp_dim])\n",
    "        \n",
    "    #\n",
    "    def call(self, inputs, training=None):\n",
    "        # x = inputs @ self.kernel + self.bias\n",
    "        x = inputs @ self.kernel\n",
    "        return x\n",
    "\n",
    "# 自定义网络模型\n",
    "class MyNetwork(keras.Model):\n",
    "    # \n",
    "    def __init__(self):\n",
    "        super(MyNetwork, self).__init__()\n",
    "        \n",
    "        self.fc1 = MyDense(32*32*3, 256)\n",
    "        self.fc2 = MyDense(256, 128)        \n",
    "        self.fc3 = MyDense(128, 64)        \n",
    "        self.fc4 = MyDense(64, 32)        \n",
    "        self.fc5 = MyDense(32, 10)\n",
    "        \n",
    "    def call(self, inputs, training=None):\n",
    "        # \n",
    "        x = tf.reshape(inputs, [-1, 32*32*3])\n",
    "        x = self.fc1(x)\n",
    "        x = tf.nn.relu(x) \n",
    "        x = self.fc2(x)        \n",
    "        x = tf.nn.relu(x)        \n",
    "        x = self.fc3(x)\n",
    "        x = tf.nn.relu(x) \n",
    "        x = self.fc4(x)        \n",
    "        x = tf.nn.relu(x)       \n",
    "        x = self.fc5(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "#\n",
    "network = MyNetwork()\n",
    "network.compile(optimizer = optimizers.Adam(lr=1e-3),\n",
    "               loss = tf.losses.CategoricalCrossentropy(from_logits=True),\n",
    "               metrics = ['accuracy'])\n",
    "\n",
    "network.fit(train_db, epochs=15, validation_data=test_val, validation_freq=1)\n",
    "network.evaluate(test_val)\n",
    "\n",
    "# 模型权重保存\n",
    "network.save_weights(\"ckpt/weights.ckpt\")\n",
    "del network\n",
    "print(\"saved to ckpt/weights.ckpt\")\n",
    "\n",
    "network = MyNetwork()\n",
    "network.compile(optimizer = optimizers.Adam(lr=1e-3),\n",
    "               loss = tf.losses.CategoricalCrossentropy(from_logits=True),\n",
    "               metrics = ['accuracy'])\n",
    "\n",
    "# 模型权重加载\n",
    "network.load_weights(\"ckpt/weights.ckpt\")\n",
    "print(\"loaded weights from file.\")\n",
    "network.evaluate(test_val)\n",
    "\n",
    "sample = next(iter(test_val))\n",
    "x = sample[0]\n",
    "y = sample[1]\n",
    "pred = network.predict(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 卷积 卷积运算 convolution\n",
    "# 卷积神经网络，针对二维图像数据进行处理\n",
    "# feature maps\n",
    "# receptive field\n",
    "# weight sharing\n",
    "\n",
    "# 卷积神经网络\n",
    "# kernel size\n",
    "# padding & stride\n",
    "# channels\n",
    "# pyramid architecture\n",
    "# gradient\n",
    "\n",
    "# 池化和采样\n",
    "# reduce dim\n",
    "# max/avg pooling\n",
    "# upSampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets:  (50000, 32, 32, 3) (50000,) (10000, 32, 32, 3) (10000,) 0 255\n",
      "batch:  (64, 32, 32, 3) (64,)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              multiple                  1792      \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            multiple                  36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            multiple                  73856     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            multiple                  147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            multiple                  295168    \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            multiple                  590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            multiple                  1180160   \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            multiple                  2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            multiple                  2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            multiple                  2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 multiple                  0         \n",
      "=================================================================\n",
      "Total params: 9,404,992\n",
      "Trainable params: 9,404,992\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                multiple                  131328    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  32896     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  12900     \n",
      "=================================================================\n",
      "Total params: 177,124\n",
      "Trainable params: 177,124\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "0 0 loss:  4.605188369750977\n",
      "0 1 loss:  4.604963302612305\n",
      "0 2 loss:  4.6058125495910645\n",
      "0 3 loss:  4.605375289916992\n",
      "0 8 loss:  4.604780673980713\n",
      "0 9 loss:  4.604815483093262\n",
      "0 10 loss:  4.604935646057129\n",
      "0 11 loss:  4.60479736328125\n",
      "0 16 loss:  4.60684871673584\n",
      "0 17 loss:  4.60480260848999\n",
      "0 18 loss:  4.605024337768555\n",
      "0 19 loss:  4.6037702560424805\n",
      "0 24 loss:  4.604120254516602\n",
      "0 25 loss:  4.6052985191345215\n",
      "0 26 loss:  4.608879089355469\n",
      "0 27 loss:  4.606614589691162\n",
      "0 128 loss:  4.605820178985596\n",
      "0 129 loss:  4.60437536239624\n",
      "0 130 loss:  4.605249404907227\n",
      "0 131 loss:  4.605724334716797\n",
      "0 136 loss:  4.605238437652588\n",
      "0 137 loss:  4.604948997497559\n",
      "0 138 loss:  4.603646278381348\n",
      "0 139 loss:  4.605147361755371\n",
      "0 144 loss:  4.605351448059082\n",
      "0 145 loss:  4.60414981842041\n",
      "0 146 loss:  4.606561660766602\n",
      "0 147 loss:  4.605292320251465\n",
      "0 152 loss:  4.601627349853516\n",
      "0 153 loss:  4.604589462280273\n",
      "0 154 loss:  4.605579853057861\n",
      "0 155 loss:  4.605112075805664\n",
      "0 256 loss:  4.607682228088379\n",
      "0 257 loss:  4.605011940002441\n",
      "0 258 loss:  4.604797840118408\n",
      "0 259 loss:  4.604551315307617\n",
      "0 264 loss:  4.6062822341918945\n",
      "0 265 loss:  4.603349208831787\n",
      "0 266 loss:  4.603877544403076\n",
      "0 267 loss:  4.605881690979004\n",
      "0 272 loss:  4.603941917419434\n",
      "0 273 loss:  4.601945877075195\n",
      "0 274 loss:  4.605957984924316\n",
      "0 275 loss:  4.606285095214844\n",
      "0 280 loss:  4.6018476486206055\n",
      "0 281 loss:  4.610383033752441\n",
      "0 282 loss:  4.604269981384277\n",
      "0 283 loss:  4.604327201843262\n",
      "0 384 loss:  4.558099746704102\n",
      "0 385 loss:  4.568638324737549\n",
      "0 386 loss:  4.5523834228515625\n",
      "0 387 loss:  4.599523544311523\n"
     ]
    }
   ],
   "source": [
    "# CIFAR100 datasets\n",
    "# VGG 13\n",
    "\n",
    "# 13 layers\n",
    "# input -> conv 64 *2 -> max pool -> conv 128 *2 -> max pool -> conv 256 *2 -> max pool \n",
    "# -> conv 512 *2 -> max pool -> conv 512 *2 -> max pool -> Fully connected *3 -> output\n",
    "# [32, 32, 3] => [100]\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, losses, optimizers, Sequential, metrics\n",
    "\n",
    "# VGG\n",
    "conv_layers = [ # 5 units of conv + max pooling\n",
    "    # unit 1\n",
    "    layers.Conv2D(64, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.Conv2D(64, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.MaxPool2D(pool_size=[2, 2], strides=2, padding=\"same\"),\n",
    "    \n",
    "    # unit 2\n",
    "    layers.Conv2D(128, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.Conv2D(128, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.MaxPool2D(pool_size=[2, 2], strides=2, padding=\"same\"),\n",
    "    \n",
    "    # unit 3\n",
    "    layers.Conv2D(256, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.Conv2D(256, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.MaxPool2D(pool_size=[2, 2], strides=2, padding=\"same\"),\n",
    "    \n",
    "    # unit 4\n",
    "    layers.Conv2D(512, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.Conv2D(512, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.MaxPool2D(pool_size=[2, 2], strides=2, padding=\"same\"),\n",
    "    \n",
    "    # unit 5\n",
    "    layers.Conv2D(512, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.Conv2D(512, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.MaxPool2D(pool_size=[2, 2], strides=2, padding=\"same\")]\n",
    "\n",
    "# \n",
    "def preprocess(x, y):\n",
    "    # [0-255] => [0-1]\n",
    "    x = tf.cast(x, dtype=tf.float32) / 255.\n",
    "    y = tf.cast(y, dtype=tf.int32)\n",
    "    return x, y\n",
    "\n",
    "# [32, 32, 3]\n",
    "(x, y), (x_test, y_test) = datasets.cifar100.load_data()\n",
    "y = tf.squeeze(y, axis=1)\n",
    "y_test = tf.squeeze(y_test, axis=1)\n",
    "print(\"datasets: \", x.shape, y.shape, x_test.shape, y_test.shape, x.min(), x.max())\n",
    "\n",
    "train_db = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "train_db = train_db.shuffle(1000).map(preprocess).batch(64)\n",
    "test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_db = test_db.map(preprocess).batch(64)\n",
    "\n",
    "sample = next(iter(train_db))\n",
    "print(\"batch: \", sample[0].shape, sample[1].shape)\n",
    "\n",
    "\n",
    "\n",
    "# \n",
    "def main():\n",
    "    # [b, 32, 32, 3] => [b, 1, 1, 512]\n",
    "    conv_net = Sequential(conv_layers)\n",
    "    # conv_net.build(input_shape=[None, 32, 32, 3])\n",
    "    # test the conv_net\n",
    "    # x = tf.random.normal([4, 32, 32, 3])\n",
    "    # out = conv_net(x)\n",
    "    # print(out.shape)\n",
    "    \n",
    "    # Fully connected layer\n",
    "    fc_net = Sequential([\n",
    "        layers.Dense(256, activation=tf.nn.relu),\n",
    "        layers.Dense(128, activation=tf.nn.relu),\n",
    "        layers.Dense(100, activation=tf.nn.relu),])\n",
    "    #\n",
    "    conv_net.build(input_shape=[None, 32, 32, 3])\n",
    "    fc_net.build(input_shape=[None, 512])\n",
    "    conv_net.summary()\n",
    "    fc_net.summary()\n",
    "\n",
    "    optimizer = optimizers.Adam(lr=1e-4)\n",
    "    \n",
    "    # [1, 2] + [3, 4] = [1, 2, 3, 4]\n",
    "    variables = conv_net.trainable_variables + fc_net.trainable_variables\n",
    "    \n",
    "    #\n",
    "    for epoch in range(50):\n",
    "        #\n",
    "        for step, (x, y) in enumerate(train_db):\n",
    "            #\n",
    "            with tf.GradientTape() as tape:\n",
    "                # [b, 32, 32, 3] => [b, 1, 1, 512]\n",
    "                out = conv_net(x)\n",
    "                # squeeze/flatten\n",
    "                out = tf.reshape(out, [-1, 512])\n",
    "                # [b, 512] => [b, 100]\n",
    "                logits = fc_net(out)\n",
    "                # [b] => [b, 100]\n",
    "                y_onehot = tf.one_hot(y, depth=100)\n",
    "                # compute loss\n",
    "                loss = tf.losses.categorical_crossentropy(y_onehot, logits, from_logits=True)\n",
    "                loss = tf.reduce_mean(loss)\n",
    "            \n",
    "            # \n",
    "            grads = tape.gradient(loss, variables)\n",
    "            optimizer.apply_gradients(zip(grads, variables))\n",
    "            \n",
    "            #\n",
    "            if step & 100 == 0:\n",
    "                print(epoch, step, \"loss: \", float(loss))\n",
    "        \n",
    "        #\n",
    "        total_num, total_correct = 0., 0\n",
    "        for x, y in test_db:\n",
    "            \n",
    "            out = conv_net(x)\n",
    "            out = tf.reshape(out, [-1, 512])\n",
    "            logits = fc_net(out)\n",
    "            prob = tf.nn.softmax(logits, axis=1)\n",
    "            pred = tf.argmax(prob, axis=1)\n",
    "            pred = tf.cast(pred, dtype=tf.int32)\n",
    "\n",
    "            correct = tf.cast(tf.equal(pred, y), dtype=tf.int32)\n",
    "            correct += tf.reduce_sum(correct)\n",
    "            \n",
    "            total_num += x.shape[0]\n",
    "            total_correct += int(correct)\n",
    "            \n",
    "        # compute the accurate\n",
    "        print(epoch, \"Evaluate Acc : \", total_correct / total_num)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == \" __main__\":\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 经典卷积神经网络  CNN  CV方向（CNN）\n",
    "# ImageNet: LeNet5(80年代) => AlexNet(2012年) => VGG(2014年) => GoogLeNet(2014年) => ResNet(2015年)\n",
    "# ResNet  &  DenseNet\n",
    "#\n",
    "# =====================================================================================^_^ ^_^ ^-^\n",
    "# ^_^ ^_^ ^_^ ^_^  进入 search，而不是停留在看看别人的博客、跑跑别人的代码、并没有太大的意义，  \n",
    "# ^_^ ^_^ 要投入到一线的科研中，要么为学术界做出贡献，要么利用最新技术为工业界、工程界做出贡献，\n",
    "# ^_^ ^_^ ^_^ ^_^ 老是学别人的东西，这种行为其实没有产生价值   ^_^ ^_^ ^_^ ^_^ \n",
    "# ^_^ ^_^                                                      ^_^ ^_^ \n",
    "# ^_^                                                            ^_^ \n",
    "# ======================================================================================^_^ ^_^ ^-^\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, losses, optimizers, Sequential, metrics\n",
    "\n",
    "# basic block\n",
    "class BasicBlock(layers.Layer):\n",
    "    #\n",
    "    def __init__(self, filter_num, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = layers.Conv2D(filter_num, (3, 3), strides=stride, padding=\"same\")\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.relu = layers.Activation(\"relu\")\n",
    "        \n",
    "        self.conv2 = layers.Conv2D(filter_num, (3, 3), strides=1, padding=\"same\")\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        \n",
    "        if stride != 1:\n",
    "            self.downsample = Sequential()\n",
    "            self.downsample.add(layers.Conv2D(filter_num, (1, 1), strides=stride))\n",
    "        else:\n",
    "            self.downsample = lambda x:x\n",
    "            \n",
    "    #\n",
    "    def call(self, inputs, training=None):\n",
    "        # [b, h, w, c]\n",
    "        out = self.conv1(inputs)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        identity = self.downsample(inputs)\n",
    "        \n",
    "        output = layers.add([out, identity])\n",
    "        output = tf.nn.relu(output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "# Res block\n",
    "class ResNet(keras.Model):\n",
    "    #\n",
    "    def __init__(self, layer_dims, num_classes=100):\n",
    "        super(ResNet, self).__init__()\n",
    "        \n",
    "        self.stem = Sequential([layers.Conv2D(64, (3, 3), strides=(1, 1)),\n",
    "                                layers.BatchNormalization(),\n",
    "                                layers.Activation(\"relu\"),\n",
    "                                layers.MaxPool2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\")])\n",
    "        \n",
    "        self.layer1 = self.build_resblock(64,  layer_dims[0])\n",
    "        self.layer2 = self.build_resblock(128, layer_dims[1], stride=2)\n",
    "        self.layer3 = self.build_resblock(256, layer_dims[2], stride=2)\n",
    "        self.layer4 = self.build_resblock(512, layer_dims[3], stride=2)\n",
    "        \n",
    "        # output: [b, 512, h, w]\n",
    "        self.avgpool = layers.GlobalAveragePooling2D()\n",
    "        self.fc = layers.Dense(num_classes)\n",
    "    #\n",
    "    def call(self, inputs, training=None):\n",
    "        #\n",
    "        x = self.stem(inputs)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        # [b, c]\n",
    "        x = self.avgpool(x)\n",
    "        # [b, 100]\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    #\n",
    "    def build_resblock(self, filter_num, blocks, stride=1):\n",
    "        #\n",
    "        res_blocks = Sequential()\n",
    "        # may down sample\n",
    "        res_blocks.add(BasicBlock(filter_num, stride))\n",
    "        \n",
    "        for _ in range(1, blocks):\n",
    "            res_blocks.add(BasicBlock(filter_num, stride=1))\n",
    "        \n",
    "        return res_blocks\n",
    "    \n",
    "#\n",
    "def resnet18():\n",
    "    # 4 * 4 + 1 + 1 = 18\n",
    "    return ResNet([2, 2, 2, 2])\n",
    "\n",
    "#\n",
    "def resnet34():\n",
    "    return ResNet([3, 4, 6, 3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets:  (50000, 32, 32, 3) (50000,) (10000, 32, 32, 3) (10000,) 0 255\n",
      "batch:  (64, 32, 32, 3) (64,)\n",
      "Model: \"res_net_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_16 (Sequential)   multiple                  2048      \n",
      "_________________________________________________________________\n",
      "sequential_17 (Sequential)   multiple                  148736    \n",
      "_________________________________________________________________\n",
      "sequential_18 (Sequential)   multiple                  526976    \n",
      "_________________________________________________________________\n",
      "sequential_20 (Sequential)   multiple                  2102528   \n",
      "_________________________________________________________________\n",
      "sequential_22 (Sequential)   multiple                  8399360   \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_2 ( multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  51300     \n",
      "=================================================================\n",
      "Total params: 11,230,948\n",
      "Trainable params: 11,223,140\n",
      "Non-trainable params: 7,808\n",
      "_________________________________________________________________\n",
      "0 0 loss:  4.609555244445801\n",
      "0 1 loss:  4.603026390075684\n",
      "0 2 loss:  4.6128034591674805\n",
      "0 3 loss:  4.626725673675537\n"
     ]
    }
   ],
   "source": [
    "# ResNet 18\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, losses, optimizers, Sequential, metrics\n",
    "\n",
    "# loading ResNet18\n",
    "# from resnet import resnet18\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# \n",
    "def preprocess(x, y):\n",
    "    # [0-255] => [0-1]\n",
    "    x = tf.cast(x, dtype=tf.float32) / 255.\n",
    "    y = tf.cast(y, dtype=tf.int32)\n",
    "    return x, y\n",
    "\n",
    "# [32, 32, 3]\n",
    "(x, y), (x_test, y_test) = datasets.cifar100.load_data()\n",
    "y = tf.squeeze(y, axis=1)\n",
    "y_test = tf.squeeze(y_test, axis=1)\n",
    "print(\"datasets: \", x.shape, y.shape, x_test.shape, y_test.shape, x.min(), x.max())\n",
    "\n",
    "train_db = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "train_db = train_db.shuffle(1000).map(preprocess).batch(64)\n",
    "test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_db = test_db.map(preprocess).batch(64)\n",
    "\n",
    "sample = next(iter(train_db))\n",
    "print(\"batch: \", sample[0].shape, sample[1].shape)\n",
    "\n",
    "# \n",
    "def main():\n",
    "    # [b, 32, 32, 3] => [b, 1, 1, 512]\n",
    "    model = resnet18()\n",
    "    model.build(input_shape=(None, 32, 32, 3))\n",
    "    model.summary()\n",
    "\n",
    "    optimizer = optimizers.Adam(lr=1e-4)\n",
    "    \n",
    "    #\n",
    "    for epoch in range(50):\n",
    "        #\n",
    "        for step, (x, y) in enumerate(train_db):\n",
    "            #\n",
    "            with tf.GradientTape() as tape:\n",
    "                # [b, 32, 32, 3] => [b, 100]\n",
    "                logits = model(x)\n",
    "                # [b] => [b, 100]\n",
    "                y_onehot = tf.one_hot(y, depth=100)\n",
    "                # compute loss\n",
    "                loss = tf.losses.categorical_crossentropy(y_onehot, logits, from_logits=True)\n",
    "                loss = tf.reduce_mean(loss)\n",
    "            \n",
    "            # \n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "            \n",
    "            #\n",
    "            if step & 100 == 0:\n",
    "                print(epoch, step, \"loss: \", float(loss))\n",
    "        \n",
    "        #\n",
    "        total_num, total_correct = 0., 0\n",
    "        for x, y in test_db:\n",
    "            \n",
    "            logits = model(x)\n",
    "            prob = tf.nn.softmax(logits, axis=1)\n",
    "            pred = tf.argmax(prob, axis=1)\n",
    "            pred = tf.cast(pred, dtype=tf.int32)\n",
    "\n",
    "            correct = tf.cast(tf.equal(pred, y), dtype=tf.int32)\n",
    "            correct += tf.reduce_sum(correct)\n",
    "            \n",
    "            total_num += x.shape[0]\n",
    "            total_correct += int(correct)\n",
    "            \n",
    "        # compute the accurate\n",
    "        print(epoch, \"Evaluate Acc : \", total_correct / total_num)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == \" __main__\":\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 时间序列\n",
    "# sequence embedding\n",
    "# batch\n",
    "# 文本序列\n",
    "# word emmbedding\n",
    "# embedding layer\n",
    "# 图像、声音、视频、文本等等序列，转换为数字型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape:  (25000, 80) tf.Tensor(1, shape=(), dtype=int64) tf.Tensor(0, shape=(), dtype=int64)\n",
      "x_test shape:  (25000, 80)\n",
      "Epoch 1/4\n",
      "195/195 [==============================] - 30s 155ms/step - loss: 0.5179 - accuracy: 0.7275 - val_loss: 0.4049 - val_accuracy: 0.8238\n",
      "Epoch 2/4\n",
      "195/195 [==============================] - 27s 138ms/step - loss: 0.3056 - accuracy: 0.8738 - val_loss: 0.3960 - val_accuracy: 0.8336\n",
      "Epoch 3/4\n",
      "195/195 [==============================] - 27s 139ms/step - loss: 0.1906 - accuracy: 0.9285 - val_loss: 0.4417 - val_accuracy: 0.8162\n",
      "Epoch 4/4\n",
      "195/195 [==============================] - 27s 141ms/step - loss: 0.0887 - accuracy: 0.9702 - val_loss: 0.5652 - val_accuracy: 0.8161\n",
      "Model: \"my_rnn_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      multiple                  1000000   \n",
      "_________________________________________________________________\n",
      "simple_rnn_cell_7 (SimpleRNN multiple                  10560     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              multiple                  65        \n",
      "=================================================================\n",
      "Total params: 1,010,625\n",
      "Trainable params: 1,010,625\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 循环神经网络 RNN   NLP方向（RNN）\n",
    "# folded model\n",
    "# formulation\n",
    "# simple RNN cell\n",
    "# single layer RNN Cell\n",
    "# Multi-Layers RNN\n",
    "# RNN Layer\n",
    "\n",
    "\n",
    "# 情感分类 \n",
    "# 评论分析\n",
    "# sentiment analysis\n",
    "# two approaches\n",
    "#\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, losses, optimizers, Sequential, metrics\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "batchsz = 128\n",
    "\n",
    "# the most frequent words\n",
    "total_words = 10000\n",
    "max_review_len = 80\n",
    "embedding_len = 100\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(num_words=total_words)\n",
    "# x_train : [b, 80] , number\n",
    "# x_test : [b, 80]\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=max_review_len)\n",
    "x_test = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=max_review_len)\n",
    "\n",
    "db_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "db_train = db_train.shuffle(1000).batch(batchsz, drop_remainder=True)\n",
    "db_test = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "db_test = db_test.batch(batchsz, drop_remainder=True)\n",
    "print(\"x_train shape: \", x_train.shape, tf.reduce_max(y_train), tf.reduce_min(y_train))\n",
    "print(\"x_test shape: \", x_test.shape)\n",
    "\n",
    "# \n",
    "class MyRNN(keras.Model):\n",
    "    # \n",
    "    def __init__(self, units):\n",
    "        super(MyRNN, self).__init__()\n",
    "        \n",
    "        # [b, 64]\n",
    "        self.state0 = [tf.zeros([batchsz, units])]\n",
    "        \n",
    "        # transform text to embedding representation\n",
    "        # [b, 80] => [b, 80, 100]\n",
    "        self.embedding = layers.Embedding(total_words, embedding_len, input_length=max_review_len)\n",
    "        \n",
    "        # [b, 80, 100] , h_dim : 64\n",
    "        # RNN : cell1, cell2, cell3\n",
    "        # simpleRNN\n",
    "        self.rnn_cell0 = layers.SimpleRNNCell(units, dropout=0.2)\n",
    "\n",
    "        # fc, [b, 80, 100] => [b, 64] => [b, 1]\n",
    "        self.outlayer = layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        # [b, 80]\n",
    "        x = inputs \n",
    "        # embedding: [b, 80] => [b, 80, 100]\n",
    "        x = self.embedding(x)\n",
    "        # rnn cell compute,[b, 80, 100] => [b, 64]\n",
    "        state0 = self.state0\n",
    "        for word in tf.unstack(x, axis=1):\n",
    "            out, state1 = self.rnn_cell0(word, state0)\n",
    "            state0 = state1\n",
    "            \n",
    "        # out: [b, 64]\n",
    "        x = self.outlayer(out)\n",
    "        # p(y is pos|x)\n",
    "        prob = tf.sigmoid(x)\n",
    "\n",
    "        return prob\n",
    "    \n",
    "#\n",
    "def main():\n",
    "    units = 64\n",
    "    epochs = 4\n",
    "    \n",
    "    model = MyRNN(units)\n",
    "    model.compile(optimizer = keras.optimizers.Adam(0.001),\n",
    "                  loss = tf.losses.BinaryCrossentropy(),\n",
    "                  metrics = [\"accuracy\"])\n",
    "    model.fit(db_train, epochs=epochs, validation_data=db_test)\n",
    "    model.summary()\n",
    "    \n",
    "# if __name__ == \" __main__\":\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
