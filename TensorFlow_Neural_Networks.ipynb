{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow\n",
    "### Deep Learning\n",
    "### Neural Networks\n",
    "### 环境搭建详细教程\n",
    "- Windows 10\n",
    "- Ubuntu 18.04/16.04/20.04\n",
    "- TensorFlow\n",
    "- PyTorch\n",
    "- CUDA cuDNN\n",
    "- Jupyter Lab & Jupyter Notebook\n",
    "- PyCharm\n",
    "\n",
    "[环境搭建详细教程——GitHub-Blog](https://2694048168.github.io/)\n",
    "\n",
    "[环境搭建详细教程——Gitee-Blog](http://weili_yzzcq.gitee.io/)\n",
    "\n",
    "[环境搭建详细教程——CSDN-Blog](https://blog.csdn.net/weixin_46782218)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent at b = 0, w = 0, error = 5565.107834483211\n",
      "Running\n",
      "After 1000 iterations b = 0.08893651993741346, w = 1.4777440851894448, error = 112.61481011613473\n"
     ]
    }
   ],
   "source": [
    "# 线性回归\n",
    "# make decisions: discrete 离散值     continuous 连续值\n",
    "# continuous prediction：input data（x）；prediction（f，参数 θ theta）；real data，ground-truth（y）\n",
    "# linear equation 线性方程组 y = w * x + b + epsilon\n",
    "# with noise  epsilon 噪声 高斯分布\n",
    "# find the W and b\n",
    "# loss = (WX + b - Y)**2\n",
    "# miniize loss \n",
    "# gradient descent GD 梯度下降 一维和二维可视化\n",
    "# loss surface\n",
    "# linear regression\n",
    "# logistic regression\n",
    "# classification\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# 生成 csv 文件里面的数据\n",
    "# data = []\n",
    "# for i in range(100):\n",
    "# \tx = np.random.uniform(3., 12.)\n",
    "# \t# mean=0, std=0.1\n",
    "# \teps = np.random.normal(0., 0.1)\n",
    "# \ty = 1.477 * x + 0.089 + eps\n",
    "# \tdata.append([x, y])\n",
    "# data = np.array(data)\n",
    "# print(data.shape, data)\n",
    "\n",
    "# cpmputer loss\n",
    "# Y = W*X +b\n",
    "def computer_error_for_line_given_points(b, w, points):\n",
    "    total_error = 0\n",
    "    for i in range(0, len(points)):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        # computer mean-squared-error\n",
    "        total_error += ((w * x + b) - y) ** 2\n",
    "        # total_error += (y - (w * x + b)) ** 2\n",
    "    # average loss for each point\n",
    "    return total_error / float(len(points))\n",
    "\n",
    "# computer Gradient and update\n",
    "def step_gradient(b_current, w_current, points, learning_rate):\n",
    "    b_gradient = 0\n",
    "    w_gradient = 0\n",
    "    N = float(len(points))\n",
    "    for i in range(0, len(points)):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        # grad_b = 2(wx + b - y)\n",
    "        b_gradient += (2/N) * ((w_current * x + b_current) - y)\n",
    "        # grad_w = 2(wx + b - y) * x\n",
    "        w_gradient += (2/N) * x * ((w_current * x + b_current) - y)\n",
    "    # update the grad_w, and the grad_b\n",
    "    new_b = b_current - (learning_rate * b_gradient)\n",
    "    new_w = w_current - (learning_rate * w_gradient)\n",
    "    return [new_b, new_w]\n",
    "\n",
    "# set w = w_new and loop\n",
    "def gradient_descent_runner(points, starting_b, starting_w, learning_rate, num_iterations):\n",
    "    b = starting_b\n",
    "    w = starting_w\n",
    "    # update for several times\n",
    "    for i in range(num_iterations):\n",
    "        b, w = step_gradient(b, w, np.array(points), learning_rate)\n",
    "    return [b, w]\n",
    "\n",
    "\n",
    "# running the linear regression model\n",
    "# loading the data for numpy from the data.cvs file\n",
    "# the data shape is 100 * 2 (row * column)\n",
    "points = np.genfromtxt(\"./datasets/data.csv\", delimiter=\",\")\n",
    "# init the Hyperparameter\n",
    "learning_rate = 0.0001\n",
    "initial_b = 0\n",
    "initial_w = 0\n",
    "num_iterations = 1000\n",
    "# show the initial Hyperparameter for linear regression model and the error\n",
    "print(\"Starting gradient descent at b = {0}, w = {1}, error = {2}\"\n",
    "      .format(initial_b, initial_w, computer_error_for_line_given_points(initial_b, initial_w, points)))\n",
    "\n",
    "print(\"Running\")\n",
    "# iterations = 1000,computer the Hyperparameter for linear regression model\n",
    "[b, w] = gradient_descent_runner(points, initial_b, initial_w, learning_rate, num_iterations)\n",
    "\n",
    "# and show the Hyperparameter for linear regression model and the error\n",
    "print(\"After {0} iterations b = {1}, w = {2}, error = {3}\"\n",
    "      .format(num_iterations, b, w, computer_error_for_line_given_points(b, w, points)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000, 10)\n",
      "0 0 2.145007\n",
      "0 100 0.9779064\n",
      "0 200 0.8011861\n",
      "1 0 0.682101\n",
      "1 100 0.68953323\n",
      "1 200 0.59687996\n",
      "2 0 0.5544969\n",
      "2 100 0.60021234\n",
      "2 200 0.5208171\n",
      "3 0 0.49560317\n",
      "3 100 0.5510172\n",
      "3 200 0.4770015\n",
      "4 0 0.45830703\n",
      "4 100 0.51574916\n",
      "4 200 0.44660082\n",
      "5 0 0.4310113\n",
      "5 100 0.48866108\n",
      "5 200 0.42332447\n",
      "6 0 0.40955436\n",
      "6 100 0.46667328\n",
      "6 200 0.40453193\n",
      "7 0 0.3919871\n",
      "7 100 0.44810033\n",
      "7 200 0.38870093\n",
      "8 0 0.37698773\n",
      "8 100 0.4321654\n",
      "8 200 0.3749433\n",
      "9 0 0.3639257\n",
      "9 100 0.41827995\n",
      "9 200 0.3629041\n",
      "10 0 0.35239592\n",
      "10 100 0.40607452\n",
      "10 200 0.35226172\n",
      "11 0 0.34209\n",
      "11 100 0.39520663\n",
      "11 200 0.34277108\n",
      "12 0 0.33278885\n",
      "12 100 0.38533977\n",
      "12 200 0.33422118\n",
      "13 0 0.3244313\n",
      "13 100 0.37634808\n",
      "13 200 0.32637382\n",
      "14 0 0.3167131\n",
      "14 100 0.3681502\n",
      "14 200 0.3191441\n",
      "15 0 0.30964595\n",
      "15 100 0.36062518\n",
      "15 200 0.31245127\n",
      "16 0 0.30311885\n",
      "16 100 0.3536573\n",
      "16 200 0.30619872\n",
      "17 0 0.2970287\n",
      "17 100 0.34722012\n",
      "17 200 0.3003191\n",
      "18 0 0.29140472\n",
      "18 100 0.34124565\n",
      "18 200 0.29482606\n",
      "19 0 0.28617287\n",
      "19 100 0.33563197\n",
      "19 200 0.28969684\n",
      "20 0 0.28127086\n",
      "20 100 0.33038154\n",
      "20 200 0.28484973\n",
      "21 0 0.27660605\n",
      "21 100 0.32542306\n",
      "21 200 0.280265\n",
      "22 0 0.27215695\n",
      "22 100 0.32068706\n",
      "22 200 0.27590075\n",
      "23 0 0.26799193\n",
      "23 100 0.31619698\n",
      "23 200 0.27178058\n",
      "24 0 0.26402566\n",
      "24 100 0.3118977\n",
      "24 200 0.26785937\n",
      "25 0 0.26028073\n",
      "25 100 0.307815\n",
      "25 200 0.26411417\n",
      "26 0 0.25673878\n",
      "26 100 0.3039063\n",
      "26 200 0.26061907\n",
      "27 0 0.25335112\n",
      "27 100 0.3001855\n",
      "27 200 0.25728583\n",
      "28 0 0.25014195\n",
      "28 100 0.29665297\n",
      "28 200 0.25410765\n",
      "29 0 0.2471003\n",
      "29 100 0.29328233\n",
      "29 200 0.25104755\n"
     ]
    }
   ],
   "source": [
    "# Image Classification\n",
    "# hand-written digits recognition\n",
    "# MNIST: 7,000 images per category = 70,000\n",
    "# train / test splitting = 60k / 10k\n",
    "# [28, 28, 1] = [rows,columns, gray_value]\n",
    "# 28 * 28 = 784 转换为一维数组\n",
    "# input and output\n",
    "# coding the features for data: one-hot\n",
    "# regression VS classification\n",
    "# y = w * x + b\n",
    "# y 属于空间 R^d\n",
    "# out = X @ W + b\n",
    "# out:[0.1, 0.8, 0.02,.0.08] = 1 概率，最大值概率称之为置信度\n",
    "# pred = argmax(out)\n",
    "\n",
    "# out = X @ W + b\n",
    "# X = [b, 784]\n",
    "# W = [784, 10]\n",
    "# b = [10]\n",
    "\n",
    "# it is linear !\n",
    "# out = X @ W + b\n",
    "# 变成 non-linear !\n",
    "# out = f(X @ W + b)\n",
    "# 引入非线性因子，这个 f 函数称之为激活函数，\n",
    "# f = ReLu function and sigmoid function\n",
    "# out = relu(X @ W + b)\n",
    "\n",
    "# it is simple\n",
    "# out = relu(X @ W + b)\n",
    "# 添加隐藏层 hide layer\n",
    "# h1 = relu(X @ W1 + b1)\n",
    "# h2 = relu(X @ W2 + b2)\n",
    "# out = relu(X @ W3 + b3)\n",
    "\n",
    "# particularly\n",
    "# 每一层都是类似降维的过程，直到从 [1, 784] ---- [1, 10]\n",
    "# X = [v1, v2, ... , v784]\n",
    "# h1 = relu(X @ W1 + b1)\n",
    "#    W1: [784, 512]\n",
    "#    b1: [1, 512]\n",
    "# h2 = relu(X @ W2 + b2)\n",
    "#    W2: [512, 256]\n",
    "#    b2: [1, 256]\n",
    "# out = relu(X @ W3 + b3)\n",
    "#    W2: [256, 10]\n",
    "#    b2: [1, 10]\n",
    "\n",
    "# loss\n",
    "# out:[1, 10]\n",
    "# Y/label: 0-9 （one-hot）\n",
    "# euclidean distance 计算欧式距离 使得 out 接近 label   MSE\n",
    "# loss = (Y - out) **2\n",
    "\n",
    "# in a nutshell\n",
    "# out = relu{relu{relu[X @ W1 + b1] @ W2 + b2 } @ W3 + b3}\n",
    "# pred = argmax(out)\n",
    "# loss = MSE(out, label)\n",
    "# minimize loss\n",
    "# [w1_new, b2_new, w2_new, b2_new, w3_new, b3_new]\n",
    "\n",
    "# Deep Learning\n",
    "# classification precedure\n",
    "# step1 compute [h1, h2, out]\n",
    "# step2 compute loss\n",
    "# step3 compute gradient and update [w1_new, b2_new, w2_new, b2_new, w3_new, b3_new]\n",
    "# step4 loop\n",
    "# need TensorFlow\n",
    "\n",
    "# 以下两条语句，使得 TensorFlow 少打印出一些无关紧要的信息\n",
    "# import  os\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "\n",
    "# step 0 loading data X and Y\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, optimizers\n",
    "\n",
    "(x, y), (x_val, y_val) = datasets.mnist.load_data() \n",
    "x = tf.convert_to_tensor(x, dtype=tf.float32) / 255.\n",
    "y = tf.convert_to_tensor(y, dtype=tf.int32)\n",
    "y = tf.one_hot(y, depth=10)\n",
    "print(x.shape, y.shape)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "# batch 概念\n",
    "train_dataset = train_dataset.batch(200)\n",
    "    \n",
    "# step 0 model NN\n",
    "model = tf.keras.Sequential([layers.Dense(512, activation=\"relu\"),\n",
    "                          layers.Dense(256, activation=\"relu\"),\n",
    "                          layers.Dense(10)])\n",
    "\n",
    "optimizer = optimizers.SGD(learning_rate=0.001)\n",
    "\n",
    "# 对整个训练集 训练 30 次，即就是 epoch = 30 \n",
    "# 将训练集数据划分为每次读取 200 ，即就是 batch = 200\n",
    "# 那么 step = train_data / batch = 60k / 200 = 300\n",
    "# 每训练 100 个样本数据，就查看一次训练情况 loss\n",
    "# 实际显示的 loss 情况次数 = epoch * (step / 100) = 30 * (300/100) = 90\n",
    "\n",
    "# step4 loop\n",
    "# 对一个数据集 dataset 训练一次称之为 epoch\n",
    "def train_epoch(epoch):\n",
    "    # 对一个 batch 训练一次称之为 step\n",
    "    for step, (x, y) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # [b， 28， 28] => [b, 784]\n",
    "            x = tf.reshape(x, (-1, 28*28))\n",
    "            # step 1 compute output\n",
    "            # [b, 784] => [b, 10]\n",
    "            out = model(x)\n",
    "            # step2 compute loss\n",
    "            loss = tf.reduce_sum(tf.square(out - y)) / x.shape[0]\n",
    "            \n",
    "        # step 3 optimize and update w1, w2, w3, b1, b2, b3\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        # w_new = w - lr * grad\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables)) \n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            # print(epoch, step, 'loss:', loss.numpy())\n",
    "            print(\"第 {0} 次 epoch，第 {1} step，该次训练 loss = {2}\".format(epoch, step+100, loss.numpy()))\n",
    "\n",
    "# 对整个数据集训练多次 30 次\n",
    "for epoch in range(30):\n",
    "    train_epoch(epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=1>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TensorFlow 数据类型\n",
    "# data container: list, np.array, tf.Tensor\n",
    "# what is Tensor\n",
    "# - scalar 标量 1.1\n",
    "# - vector 向量 [1.1], [1.1, 2.2]\n",
    "# - matrix 矩阵 [[1.1, 2.2], [3.3, 4.4], [5.5, 6.6]]\n",
    "# - tensor rank > 2 矩阵的秩\n",
    "# TF is a computing lib\n",
    "# - int, float, double, bool, string\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# create the int, float, double, bool, string\n",
    "tf.constant(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=1.0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.constant(1.2, dtype=tf.int32)\n",
    "# TypeError: Cannot convert 1.2 to EagerTensor of dtype int32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True, False])>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant([True, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'Hello World'>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/job:localhost/replica:0/task:0/device:CPU:0'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensor Property 常见属性\n",
    "with tf.device(\"cpu\"):\n",
    "    const_cpu = tf.constant([1])\n",
    "\n",
    "with tf.device(\"gpu\"):\n",
    "    const_gpu = tf.range(4)\n",
    "    \n",
    "# device 属性，查看 Tensor 在哪一个设备上 CPU or GPU\n",
    "const_cpu.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/job:localhost/replica:0/task:0/device:CPU:0'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const_gpu.device\n",
    "# 因为本地没有安装 GPU 加速驱动 CUDA 和 cuDNN\n",
    "# 无法提供 GPU 加速环境，故此工作环境依旧还在 CPU 上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/job:localhost/replica:0/task:0/device:CPU:0'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 相互转换 \n",
    "# const_cpu_to_gpu = const_cpu.gpu()\n",
    "# const_cpu_to_gpu.device\n",
    "\n",
    "const_gpu_to_cpu = tf.identity(const_gpu)\n",
    "const_gpu_to_cpu.device\n",
    "\n",
    "# tensor 的计算必须在同一设备进行，全部都是 CPU 或者全部都是 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy 属性，将 Tensor 转换为 numpy\n",
    "# numpy 是在 CPU 上的\n",
    "const_gpu.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看 Tensor 的 shape 维度\n",
    "# ndim 属性，返回 Tensor 的维度信息\n",
    "# rank 方法，返回 Tensor 的维度信息\n",
    "const_cpu.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=1>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.rank(const_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=3>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.rank(tf.ones([3, 2, 4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 判断是否 Tensor\n",
    "tf.is_tensor(const_cpu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=int32, numpy=array([0, 1, 2, 3, 4])>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 转换为 Tensor\n",
    "const = np.arange(5)\n",
    "const_tensor = tf.convert_to_tensor(const)\n",
    "const_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf.int32"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert dtype\n",
    "# 数值类型之间的转换\n",
    "const_cpu.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=float32, numpy=array([0., 1., 2., 3., 4.], dtype=float32)>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.cast(const_tensor, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=float64, numpy=array([0., 1., 2., 3., 4.])>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.cast(const_tensor, dtype=tf.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=int32, numpy=array([0, 1, 2, 3, 4])>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.cast(const_tensor, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=bool, numpy=array([False,  True])>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# int <==> bool\n",
    "bool_num = tf.constant([0, 1])\n",
    "bool_num_bool = tf.cast(bool_num, dtype=tf.bool)\n",
    "bool_num_bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=int32, numpy=array([0, 1])>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.cast(bool_num_bool, tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=int32, numpy=array([0, 1, 2, 3, 4])>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf.Variable\n",
    "# 一般是需要优化的变量参数\n",
    "a = tf.range(5)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(5,) dtype=int32, numpy=array([0, 1, 2, 3, 4])>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = tf.Variable(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf.int32"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Variable:0'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.trainable\n",
    "# 需要梯度信息，可以自动求导\n",
    "# 神经网络的参数需求"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.is_tensor(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建 Tensor\n",
    "# from numpy, list\n",
    "# zeros, ones\n",
    "# fill\n",
    "# random\n",
    "# constant\n",
    "# Applicatin\n",
    "\n",
    "# meta-learning\n",
    "\n",
    "# Tensor 索引和切片\n",
    "# selecetive indexing\n",
    "# tf.gather() , tf.gather_nd(), tf.boolean_mask()\n",
    "\n",
    "# 维度变换\n",
    "# shape, ndim\n",
    "# reshape\n",
    "# expand_dims, squeeze\n",
    "# transpose\n",
    "# broadcast_to\n",
    "\n",
    "# 数学计算\n",
    "# +, -, *, /, **, pow, square, sqrt, //, %, exp, log, @, matmul\n",
    "# element_wise, matrix_wise, dim_wise, \n",
    "# 元素级计算，矩阵级计算，维度级计算\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets: (60000, 28, 28) (60000,) 0 255\n",
      "(60000, 28, 28) (60000,) <dtype: 'float32'> <dtype: 'int32'>\n",
      "tf.Tensor(0.0, shape=(), dtype=float32) tf.Tensor(255.0, shape=(), dtype=float32)\n",
      "tf.Tensor(0, shape=(), dtype=int32) tf.Tensor(9, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# import  os\n",
    "# 复制 0， 1， 2，打印 c++ 一些信息\n",
    "# 0 打印全部信息\n",
    "# 2 只打印 error 信息\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, optimizers, Sequential, metrics\n",
    "\n",
    "# loading the dataset MNIST\n",
    "(x, y), _ = datasets.mnist.load_data()\n",
    "print('datasets:', x.shape, y.shape, x.min(), x.max())\n",
    "\n",
    "# x : [0-255] -> [0-1.]\n",
    "x = tf.convert_to_tensor(x, dtype=tf.float32) / 255.\n",
    "y = tf.convert_to_tensor(y, dtype=tf.int32)\n",
    "\n",
    "print(x.shape, y.shape, x.dtype, y.dtype)\n",
    "print(tf.reduce_min(x), tf.reduce_max(x))\n",
    "print(tf.reduce_min(y), tf.reduce_max(y))\n",
    "\n",
    "# set the batch\n",
    "train_db = tf.data.Dataset.from_tensor_slices((x, y)).batch(128)\n",
    "\n",
    "# set the iteration\n",
    "train_iter = iter(train_db)\n",
    "sample = next(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,) <dtype: 'float32'> <dtype: 'int32'>\n",
      "tf.Tensor(0.0, shape=(), dtype=float32) tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "tf.Tensor(0, shape=(), dtype=int32) tf.Tensor(9, shape=(), dtype=int32)\n",
      "batch (128, 28, 28) (128,)\n",
      "0 0 loss 0.42511042952537537\n",
      "0 100 loss 0.1789942979812622\n",
      "0 200 loss 0.16580988466739655\n",
      "0 300 loss 0.1560211181640625\n",
      "0 400 loss 0.1542573869228363\n",
      "1 0 loss 0.1479978859424591\n",
      "1 100 loss 0.1321682631969452\n",
      "1 200 loss 0.13824641704559326\n",
      "1 300 loss 0.13372421264648438\n",
      "1 400 loss 0.13392488658428192\n",
      "2 0 loss 0.12848277390003204\n",
      "2 100 loss 0.11817771196365356\n",
      "2 200 loss 0.12388703972101212\n",
      "2 300 loss 0.11938042938709259\n",
      "2 400 loss 0.12050290405750275\n",
      "3 0 loss 0.11577292531728745\n",
      "3 100 loss 0.10877491533756256\n",
      "3 200 loss 0.11369021236896515\n",
      "3 300 loss 0.1092468723654747\n",
      "3 400 loss 0.11090860515832901\n",
      "4 0 loss 0.10678441822528839\n",
      "4 100 loss 0.10189224779605865\n",
      "4 200 loss 0.10604660212993622\n",
      "4 300 loss 0.10167856514453888\n",
      "4 400 loss 0.10369563102722168\n",
      "5 0 loss 0.0999135747551918\n",
      "5 100 loss 0.09662076085805893\n",
      "5 200 loss 0.10006420314311981\n",
      "5 300 loss 0.0957786813378334\n",
      "5 400 loss 0.09803612530231476\n",
      "6 0 loss 0.0944289118051529\n",
      "6 100 loss 0.09243079274892807\n",
      "6 200 loss 0.09519816935062408\n",
      "6 300 loss 0.09105338156223297\n",
      "6 400 loss 0.09347577393054962\n",
      "7 0 loss 0.08995360136032104\n",
      "7 100 loss 0.08898767083883286\n",
      "7 200 loss 0.09114708006381989\n",
      "7 300 loss 0.08714453876018524\n",
      "7 400 loss 0.0896892100572586\n",
      "8 0 loss 0.08621992915868759\n",
      "8 100 loss 0.08607572317123413\n",
      "8 200 loss 0.08771105855703354\n",
      "8 300 loss 0.08386079967021942\n",
      "8 400 loss 0.08645957708358765\n",
      "9 0 loss 0.08304877579212189\n",
      "9 100 loss 0.08354615420103073\n",
      "9 200 loss 0.08477957546710968\n",
      "9 300 loss 0.08105111122131348\n",
      "9 400 loss 0.08372876793146133\n"
     ]
    }
   ],
   "source": [
    "# 前向传播 forward\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets\n",
    "\n",
    "# loading data\n",
    "(x, y), _ = datasets.mnist.load_data()\n",
    "\n",
    "# convert to Tensor\n",
    "# x : [0-255] -> [0-1.]\n",
    "x = tf.convert_to_tensor(x, dtype=tf.float32) / 255.\n",
    "y = tf.convert_to_tensor(y, dtype=tf.int32)\n",
    "\n",
    "# show the info\n",
    "print(x.shape, y.shape, x.dtype, y.dtype)\n",
    "print(tf.reduce_min(x), tf.reduce_max(x))\n",
    "print(tf.reduce_min(y), tf.reduce_max(y))\n",
    "\n",
    "# set the dataset and split the batch\n",
    "train_db = tf.data.Dataset.from_tensor_slices((x, y)).batch(128)\n",
    "train_iter = iter(train_db)\n",
    "sample = next(train_iter)\n",
    "print(\"batch\", sample[0].shape, sample[1].shape)\n",
    "\n",
    "# 权值\n",
    "# [b, 784] => [b, 256] => [b, 128] => [b, 10]\n",
    "# [dim_in, dim_out], [dim_out]\n",
    "# tf.Variable 才会自动记录梯度信息，tf.Tensor 不会记录梯度信息，故此值为 None，类型为 NoneType\n",
    "# 需要给一个好点的初始值，否则容易 loss 爆炸，出现 nan(not a number) 情况,stddev=0.1 方差给小一点\n",
    "w1 = tf.Variable(tf.random.truncated_normal([784, 256], stddev=0.1))\n",
    "b1 = tf.Variable(tf.zeros([256]))\n",
    "w2 = tf.Variable(tf.random.truncated_normal([256, 128], stddev=0.1))\n",
    "b2 = tf.Variable(tf.zeros([128]))\n",
    "w3 = tf.Variable(tf.random.truncated_normal([128, 10], stddev=0.1))\n",
    "b3 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# 学习率 ，步长 10^-3\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# 对数据集迭代 10 次\n",
    "for epoch in range(10):\n",
    "    # 对数据集一次循环\n",
    "    for step, (x, y) in enumerate(train_db):\n",
    "        # x [128, 28, 28]\n",
    "        # y [128]\n",
    "        # 进行维度转换 [128, 28, 28] => [b, 28*28]\n",
    "        x = tf.reshape(x, [-1, 28*28])   \n",
    "    \n",
    "        # 自动求导过程,梯度\n",
    "        with tf.GradientTape() as tape:\n",
    "            # x [b, 28*28]\n",
    "            # h1 = x@w1 + b1\n",
    "            # [b, 784]@[784, 256] + [256] => [b, 256] + [256] => [b, 256] + [b, 256]\n",
    "            h1 = x@w1 + tf.broadcast_to(b1, [x.shape[0], 256])\n",
    "            # 添加非线性，relu 激活函数\n",
    "            h1 = tf.nn.relu(h1)\n",
    "            # [b, 256] => [b, 128]\n",
    "            h2 = h1@w2 + b2\n",
    "            h2 = tf.nn.relu(h2)\n",
    "            # [b, 128] => [b, 10]\n",
    "            out = h2@w3 +b3\n",
    "        \n",
    "            # compute loss\n",
    "            # out [b, 10]\n",
    "            # y [b] => [b, 10]\n",
    "            y_one_hot = tf.one_hot(y, depth=10)\n",
    "        \n",
    "            # MSE = mean((y - out)**2)\n",
    "            loss = tf.square(y_one_hot - out)\n",
    "            # mean: scalar\n",
    "            loss = tf.reduce_mean(loss)\n",
    "    \n",
    "        # compute gradients\n",
    "        grads = tape.gradient(loss, [w1, b1, w2, b2, w3, b3])\n",
    "        # print(grads)\n",
    "        # w1 = w1 - lr * w1_grad\n",
    "        # 原地更新，否则就返回 tf.Tensor 类型了，无法进行下一个梯度更新\n",
    "        # w1 = w1 - learning_rate * grads[0]\n",
    "        w1.assign_sub(learning_rate * grads[0])\n",
    "        b1.assign_sub(learning_rate * grads[1])\n",
    "        w2.assign_sub(learning_rate * grads[2])\n",
    "        b2.assign_sub(learning_rate * grads[3])\n",
    "        w3.assign_sub(learning_rate * grads[4])\n",
    "        b3.assign_sub(learning_rate * grads[5])\n",
    "    \n",
    "        if step % 100 == 0:\n",
    "            print(epoch, step, \"loss\", float(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
