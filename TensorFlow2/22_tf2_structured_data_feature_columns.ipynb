{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "copyrighted-professional",
   "metadata": {},
   "source": [
    "# Classify structured data with feature columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integrated-progressive",
   "metadata": {},
   "source": [
    "## 1. The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "potential-latest",
   "metadata": {},
   "source": [
    "## 2. Import TensorFlow and other libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "attached-direction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The version of Tensorflow: 2.4.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"The version of Tensorflow: {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completed-fourth",
   "metadata": {},
   "source": [
    "## 3. Use Pandas to create a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "australian-highway",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/petfinder-mini.zip\n",
      "1671168/1668792 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "\n",
    "dataset_url = 'http://storage.googleapis.com/download.tensorflow.org/data/petfinder-mini.zip'\n",
    "csv_file = 'datasets/petfinder-mini/petfinder-mini.csv'\n",
    "\n",
    "tf.keras.utils.get_file('petfinder_mini.zip', dataset_url,\n",
    "                        extract=True, cache_dir='.')\n",
    "dataframe = pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "colored-financing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Age</th>\n",
       "      <th>Breed1</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Color1</th>\n",
       "      <th>Color2</th>\n",
       "      <th>MaturitySize</th>\n",
       "      <th>FurLength</th>\n",
       "      <th>Vaccinated</th>\n",
       "      <th>Sterilized</th>\n",
       "      <th>Health</th>\n",
       "      <th>Fee</th>\n",
       "      <th>Description</th>\n",
       "      <th>PhotoAmt</th>\n",
       "      <th>AdoptionSpeed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cat</td>\n",
       "      <td>3</td>\n",
       "      <td>Tabby</td>\n",
       "      <td>Male</td>\n",
       "      <td>Black</td>\n",
       "      <td>White</td>\n",
       "      <td>Small</td>\n",
       "      <td>Short</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Healthy</td>\n",
       "      <td>100</td>\n",
       "      <td>Nibble is a 3+ month old ball of cuteness. He ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cat</td>\n",
       "      <td>1</td>\n",
       "      <td>Domestic Medium Hair</td>\n",
       "      <td>Male</td>\n",
       "      <td>Black</td>\n",
       "      <td>Brown</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Not Sure</td>\n",
       "      <td>Not Sure</td>\n",
       "      <td>Healthy</td>\n",
       "      <td>0</td>\n",
       "      <td>I just found it alone yesterday near my apartm...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dog</td>\n",
       "      <td>1</td>\n",
       "      <td>Mixed Breed</td>\n",
       "      <td>Male</td>\n",
       "      <td>Brown</td>\n",
       "      <td>White</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Healthy</td>\n",
       "      <td>0</td>\n",
       "      <td>Their pregnant mother was dumped by her irresp...</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dog</td>\n",
       "      <td>4</td>\n",
       "      <td>Mixed Breed</td>\n",
       "      <td>Female</td>\n",
       "      <td>Black</td>\n",
       "      <td>Brown</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Short</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Healthy</td>\n",
       "      <td>150</td>\n",
       "      <td>Good guard dog, very alert, active, obedience ...</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dog</td>\n",
       "      <td>1</td>\n",
       "      <td>Mixed Breed</td>\n",
       "      <td>Male</td>\n",
       "      <td>Black</td>\n",
       "      <td>No Color</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Short</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Healthy</td>\n",
       "      <td>0</td>\n",
       "      <td>This handsome yet cute boy is up for adoption....</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Type  Age                Breed1  Gender Color1    Color2 MaturitySize  \\\n",
       "0  Cat    3                 Tabby    Male  Black     White        Small   \n",
       "1  Cat    1  Domestic Medium Hair    Male  Black     Brown       Medium   \n",
       "2  Dog    1           Mixed Breed    Male  Brown     White       Medium   \n",
       "3  Dog    4           Mixed Breed  Female  Black     Brown       Medium   \n",
       "4  Dog    1           Mixed Breed    Male  Black  No Color       Medium   \n",
       "\n",
       "  FurLength Vaccinated Sterilized   Health  Fee  \\\n",
       "0     Short         No         No  Healthy  100   \n",
       "1    Medium   Not Sure   Not Sure  Healthy    0   \n",
       "2    Medium        Yes         No  Healthy    0   \n",
       "3     Short        Yes         No  Healthy  150   \n",
       "4     Short         No         No  Healthy    0   \n",
       "\n",
       "                                         Description  PhotoAmt  AdoptionSpeed  \n",
       "0  Nibble is a 3+ month old ball of cuteness. He ...         1              2  \n",
       "1  I just found it alone yesterday near my apartm...         2              0  \n",
       "2  Their pregnant mother was dumped by her irresp...         7              3  \n",
       "3  Good guard dog, very alert, active, obedience ...         8              2  \n",
       "4  This handsome yet cute boy is up for adoption....         3              2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "right-minneapolis",
   "metadata": {},
   "source": [
    "## 4. Create target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "entire-judge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After modifying the label column, 0 will indicate the pet was not adopted, and 1 will indicate it was.\n",
    "\n",
    "# In the original dataset \"4\" indicates the pet was not adopted.\n",
    "dataframe['target'] = np.where(dataframe['AdoptionSpeed']==4, 0, 1)\n",
    "\n",
    "# Drop un-used columns.\n",
    "dataframe = dataframe.drop(columns=['AdoptionSpeed', 'Description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "harmful-overview",
   "metadata": {},
   "source": [
    "## 5. Split the dataframe into train, validation, and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "scheduled-oxide",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7383 train examples\n",
      "1846 validation examples\n",
      "2308 test examples\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(dataframe, test_size=0.2)\n",
    "train, val = train_test_split(train, test_size=0.2)\n",
    "print(len(train), 'train examples')\n",
    "print(len(val), 'validation examples')\n",
    "print(len(test), 'test examples')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "located-korean",
   "metadata": {},
   "source": [
    "## 6. Create an input pipeline using tf.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "embedded-budget",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A utility method to create a tf.data dataset from a Pandas Dataframe\n",
    "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
    "  dataframe = dataframe.copy()\n",
    "  labels = dataframe.pop('target')\n",
    "  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "  ds = ds.batch(batch_size)\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "elect-attachment",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5 # A small batch sized is used for demonstration purposes\n",
    "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
    "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
    "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nervous-cemetery",
   "metadata": {},
   "source": [
    "## 7. Understand the input pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "french-sentence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every feature: ['Type', 'Age', 'Breed1', 'Gender', 'Color1', 'Color2', 'MaturitySize', 'FurLength', 'Vaccinated', 'Sterilized', 'Health', 'Fee', 'PhotoAmt']\n",
      "A batch of ages: tf.Tensor([1 2 4 3 2], shape=(5,), dtype=int64)\n",
      "A batch of targets: tf.Tensor([1 1 1 0 1], shape=(5,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Now that we have created the input pipeline, let's call it to see the format of the data it returns.\n",
    "# We have used a small batch size to keep the output readable.\n",
    "\n",
    "for feature_batch, label_batch in train_ds.take(1):\n",
    "  print('Every feature:', list(feature_batch.keys()))\n",
    "  print('A batch of ages:', feature_batch['Age'])\n",
    "  print('A batch of targets:', label_batch )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precise-spokesman",
   "metadata": {},
   "source": [
    "## 8. Demonstrate several types of feature columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "front-raise",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use this batch to demonstrate several types of feature columns\n",
    "example_batch = next(iter(train_ds))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "referenced-leadership",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A utility method to create a feature column\n",
    "# and to transform a batch of data\n",
    "def demo(feature_column):\n",
    "  feature_layer = layers.DenseFeatures(feature_column)\n",
    "  print(feature_layer(example_batch).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utility-extraction",
   "metadata": {},
   "source": [
    "### i. Numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "optional-mobile",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.]\n",
      " [10.]\n",
      " [ 6.]\n",
      " [ 3.]\n",
      " [ 4.]]\n"
     ]
    }
   ],
   "source": [
    "photo_count = feature_column.numeric_column('PhotoAmt')\n",
    "demo(photo_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "political-academy",
   "metadata": {},
   "source": [
    "### ii. Bucketized columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "played-winning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "age = feature_column.numeric_column('Age')\n",
    "age_buckets = feature_column.bucketized_column(age, boundaries=[1, 3, 5])\n",
    "demo(age_buckets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brown-image",
   "metadata": {},
   "source": [
    "### iii. Categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "patent-enforcement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "animal_type = feature_column.categorical_column_with_vocabulary_list(\n",
    "      'Type', ['Cat', 'Dog'])\n",
    "\n",
    "animal_type_one_hot = feature_column.indicator_column(animal_type)\n",
    "demo(animal_type_one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorrect-caution",
   "metadata": {},
   "source": [
    "### iv. Embedding columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "damaged-alloy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.17847468  0.49610293 -0.5808373  -0.20088199  0.39659172  0.50982726\n",
      "   0.06663885  0.00095745]\n",
      " [ 0.29709777  0.20287935 -0.03885686 -0.24133116  0.46994448 -0.2738055\n",
      "   0.01688367  0.06424612]\n",
      " [-0.00995925  0.0822737   0.3941166  -0.23942663 -0.23538984  0.01253001\n",
      "   0.49396482 -0.17121339]\n",
      " [-0.45432663 -0.5248258   0.51606387  0.41490018  0.30444112 -0.07294352\n",
      "   0.42028347 -0.17130357]\n",
      " [ 0.29709777  0.20287935 -0.03885686 -0.24133116  0.46994448 -0.2738055\n",
      "   0.01688367  0.06424612]]\n"
     ]
    }
   ],
   "source": [
    "# The size of the embedding (8, in the example below) is a parameter that must be tuned.\n",
    "# Key Point: using an embedding column is best when a categorical column has many possible values. \n",
    "# We are using one here for demonstration purposes, \n",
    "# so you have a complete example you can modify for a different dataset in the future.\n",
    "\n",
    "\n",
    "# Notice the input to the embedding column is the categorical column\n",
    "# we previously created\n",
    "breed1 = feature_column.categorical_column_with_vocabulary_list(\n",
    "      'Breed1', dataframe.Breed1.unique())\n",
    "breed1_embedding = feature_column.embedding_column(breed1, dimension=8)\n",
    "demo(breed1_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-expansion",
   "metadata": {},
   "source": [
    "### v. Hashed feature columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "contrary-montgomery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Key Point: An important downside of this technique is that there may be collisions \n",
    "# in which different strings are mapped to the same bucket. \n",
    "# In practice, this can work well for some datasets regardless.\n",
    "\n",
    "\n",
    "breed1_hashed = feature_column.categorical_column_with_hash_bucket(\n",
    "      'Breed1', hash_bucket_size=10)\n",
    "demo(feature_column.indicator_column(breed1_hashed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfactory-exhibition",
   "metadata": {},
   "source": [
    "### vi. Crossed feature columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "premier-musical",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9223372036854775807"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.maxsize"
   ]
  },
  {
   "cell_type": "raw",
   "id": "registered-notification",
   "metadata": {},
   "source": [
    "crossed_feature = feature_column.crossed_column([age_buckets, animal_type], hash_bucket_size=10)\n",
    "demo(feature_column.indicator_column(crossed_feature))\n",
    "\n",
    "# OverflowError: Python int too large to convert to C long"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "north-rebate",
   "metadata": {},
   "source": [
    "## 9. Choose which columns to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "facial-score",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key Point: If your aim is to build an accurate model, try a larger dataset of your own, \n",
    "# and think carefully about which features are the most meaningful to include, and how they should be represented.\n",
    "\n",
    "\n",
    "feature_columns = []\n",
    "\n",
    "# numeric cols\n",
    "for header in ['PhotoAmt', 'Fee', 'Age']:\n",
    "  feature_columns.append(feature_column.numeric_column(header))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "organizational-daisy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bucketized cols\n",
    "age = feature_column.numeric_column('Age')\n",
    "age_buckets = feature_column.bucketized_column(age, boundaries=[1, 2, 3, 4, 5])\n",
    "feature_columns.append(age_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "vulnerable-fairy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# indicator_columns\n",
    "indicator_column_names = ['Type', 'Color1', 'Color2', 'Gender', 'MaturitySize',\n",
    "                          'FurLength', 'Vaccinated', 'Sterilized', 'Health']\n",
    "for col_name in indicator_column_names:\n",
    "  categorical_column = feature_column.categorical_column_with_vocabulary_list(\n",
    "      col_name, dataframe[col_name].unique())\n",
    "  indicator_column = feature_column.indicator_column(categorical_column)\n",
    "  feature_columns.append(indicator_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "corrected-metro",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding columns\n",
    "breed1 = feature_column.categorical_column_with_vocabulary_list(\n",
    "      'Breed1', dataframe.Breed1.unique())\n",
    "breed1_embedding = feature_column.embedding_column(breed1, dimension=8)\n",
    "feature_columns.append(breed1_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "accompanied-nursery",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crossed columns\n",
    "age_type_feature = feature_column.crossed_column([age_buckets, animal_type], hash_bucket_size=100)\n",
    "feature_columns.append(feature_column.indicator_column(age_type_feature))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspended-career",
   "metadata": {},
   "source": [
    "### i. Create a feature layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "religious-dictionary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have defined our feature columns, we will use a DenseFeatures layer to input them to our Keras model.\n",
    "\n",
    "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "taken-annotation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Earlier, we used a small batch size to demonstrate how feature columns worked. \n",
    "# We create a new input pipeline with a larger batch size.\n",
    "\n",
    "batch_size = 32\n",
    "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
    "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
    "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-longer",
   "metadata": {},
   "source": [
    "## 10. Create, compile, and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "parliamentary-guide",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'Type': <tf.Tensor 'ExpandDims_11:0' shape=(None, 1) dtype=string>, 'Age': <tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=int64>, 'Breed1': <tf.Tensor 'ExpandDims_1:0' shape=(None, 1) dtype=string>, 'Gender': <tf.Tensor 'ExpandDims_6:0' shape=(None, 1) dtype=string>, 'Color1': <tf.Tensor 'ExpandDims_2:0' shape=(None, 1) dtype=string>, 'Color2': <tf.Tensor 'ExpandDims_3:0' shape=(None, 1) dtype=string>, 'MaturitySize': <tf.Tensor 'ExpandDims_8:0' shape=(None, 1) dtype=string>, 'FurLength': <tf.Tensor 'ExpandDims_5:0' shape=(None, 1) dtype=string>, 'Vaccinated': <tf.Tensor 'ExpandDims_12:0' shape=(None, 1) dtype=string>, 'Sterilized': <tf.Tensor 'ExpandDims_10:0' shape=(None, 1) dtype=string>, 'Health': <tf.Tensor 'ExpandDims_7:0' shape=(None, 1) dtype=string>, 'Fee': <tf.Tensor 'ExpandDims_4:0' shape=(None, 1) dtype=int64>, 'PhotoAmt': <tf.Tensor 'ExpandDims_9:0' shape=(None, 1) dtype=int64>}\n",
      "Consider rewriting this model with the Functional API.\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'Type': <tf.Tensor 'ExpandDims_11:0' shape=(None, 1) dtype=string>, 'Age': <tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=int64>, 'Breed1': <tf.Tensor 'ExpandDims_1:0' shape=(None, 1) dtype=string>, 'Gender': <tf.Tensor 'ExpandDims_6:0' shape=(None, 1) dtype=string>, 'Color1': <tf.Tensor 'ExpandDims_2:0' shape=(None, 1) dtype=string>, 'Color2': <tf.Tensor 'ExpandDims_3:0' shape=(None, 1) dtype=string>, 'MaturitySize': <tf.Tensor 'ExpandDims_8:0' shape=(None, 1) dtype=string>, 'FurLength': <tf.Tensor 'ExpandDims_5:0' shape=(None, 1) dtype=string>, 'Vaccinated': <tf.Tensor 'ExpandDims_12:0' shape=(None, 1) dtype=string>, 'Sterilized': <tf.Tensor 'ExpandDims_10:0' shape=(None, 1) dtype=string>, 'Health': <tf.Tensor 'ExpandDims_7:0' shape=(None, 1) dtype=string>, 'Fee': <tf.Tensor 'ExpandDims_4:0' shape=(None, 1) dtype=int64>, 'PhotoAmt': <tf.Tensor 'ExpandDims_9:0' shape=(None, 1) dtype=int64>}\n",
      "Consider rewriting this model with the Functional API.\n",
      "227/231 [============================>.] - ETA: 0s - loss: 0.8001 - accuracy: 0.6366WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'Type': <tf.Tensor 'ExpandDims_11:0' shape=(None, 1) dtype=string>, 'Age': <tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=int64>, 'Breed1': <tf.Tensor 'ExpandDims_1:0' shape=(None, 1) dtype=string>, 'Gender': <tf.Tensor 'ExpandDims_6:0' shape=(None, 1) dtype=string>, 'Color1': <tf.Tensor 'ExpandDims_2:0' shape=(None, 1) dtype=string>, 'Color2': <tf.Tensor 'ExpandDims_3:0' shape=(None, 1) dtype=string>, 'MaturitySize': <tf.Tensor 'ExpandDims_8:0' shape=(None, 1) dtype=string>, 'FurLength': <tf.Tensor 'ExpandDims_5:0' shape=(None, 1) dtype=string>, 'Vaccinated': <tf.Tensor 'ExpandDims_12:0' shape=(None, 1) dtype=string>, 'Sterilized': <tf.Tensor 'ExpandDims_10:0' shape=(None, 1) dtype=string>, 'Health': <tf.Tensor 'ExpandDims_7:0' shape=(None, 1) dtype=string>, 'Fee': <tf.Tensor 'ExpandDims_4:0' shape=(None, 1) dtype=int64>, 'PhotoAmt': <tf.Tensor 'ExpandDims_9:0' shape=(None, 1) dtype=int64>}\n",
      "Consider rewriting this model with the Functional API.\n",
      "231/231 [==============================] - 4s 12ms/step - loss: 0.7985 - accuracy: 0.6375 - val_loss: 0.5406 - val_accuracy: 0.7546\n",
      "Epoch 2/10\n",
      "231/231 [==============================] - 2s 10ms/step - loss: 0.5876 - accuracy: 0.7147 - val_loss: 0.5184 - val_accuracy: 0.6891\n",
      "Epoch 3/10\n",
      "231/231 [==============================] - 2s 10ms/step - loss: 0.5288 - accuracy: 0.7233 - val_loss: 0.4956 - val_accuracy: 0.7394\n",
      "Epoch 4/10\n",
      "231/231 [==============================] - 2s 10ms/step - loss: 0.5077 - accuracy: 0.7284 - val_loss: 0.4806 - val_accuracy: 0.7432\n",
      "Epoch 5/10\n",
      "231/231 [==============================] - 2s 10ms/step - loss: 0.4964 - accuracy: 0.7421 - val_loss: 0.4791 - val_accuracy: 0.7595\n",
      "Epoch 6/10\n",
      "231/231 [==============================] - 2s 10ms/step - loss: 0.4957 - accuracy: 0.7329 - val_loss: 0.4943 - val_accuracy: 0.7378\n",
      "Epoch 7/10\n",
      "231/231 [==============================] - 2s 10ms/step - loss: 0.4828 - accuracy: 0.7304 - val_loss: 0.5025 - val_accuracy: 0.6901\n",
      "Epoch 8/10\n",
      "231/231 [==============================] - 2s 10ms/step - loss: 0.4715 - accuracy: 0.7469 - val_loss: 0.4963 - val_accuracy: 0.7107\n",
      "Epoch 9/10\n",
      "231/231 [==============================] - 2s 10ms/step - loss: 0.4839 - accuracy: 0.7403 - val_loss: 0.4876 - val_accuracy: 0.7465\n",
      "Epoch 10/10\n",
      "231/231 [==============================] - 2s 10ms/step - loss: 0.4620 - accuracy: 0.7509 - val_loss: 0.4922 - val_accuracy: 0.7102\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2712f0b4940>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  feature_layer,\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dropout(.1),\n",
    "  layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_ds,\n",
    "          validation_data=val_ds,\n",
    "          epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "pressing-seminar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 0s 6ms/step - loss: 0.5110 - accuracy: 0.7123\n",
      "Accuracy 0.7123050093650818\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_ds)\n",
    "print(\"Accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "enhanced-constitution",
   "metadata": {},
   "source": [
    "Key Point: You will typically see best results with deep learning with much larger and more complex datasets. \n",
    "          When working with a small dataset like this one, we recommend using a decision tree or random forest as a strong                   baseline. The goal of this tutorial is not to train an accurate model, but to demonstrate the mechanics of working with           structured data, so you have code to use as a starting point when working with your own datasets in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-manitoba",
   "metadata": {},
   "source": [
    "# reference\n",
    "\n",
    "### https://tensorflow.google.cn/tutorials/structured_data/feature_columns#create_compile_and_train_the_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "russian-honduras",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
