{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 感知机\n",
    "\n",
    "- 0、单层感知机模型结构图\n",
    "\n",
    "- 1、定义**输出单元** sigmoid = 1 / (1 + exp(-z)) \n",
    "\n",
    "- 2、定义**仿射变换** Z = WX + b\n",
    "\n",
    "- 3、数据集 X 有多少实例样本 N，y 有多少种类别 2\n",
    "\n",
    "- 4、**前向传播**计算  y_hat = sigmoid(z) = sigmoid(WX + b) = 1 / (1 + exp(-WX + b)) \n",
    "\n",
    "- 5、**损失函数**计算 LogLoss = -1/N \\* sum(y\\*log(y_hat) + (1-y)\\*log(1-y_hat))\n",
    "\n",
    "- 6、**梯度计算** dW = X(y_hat - y) ; db = y_hat - y\n",
    "\n",
    "- 7、**反向传播，更新梯度**  W <— W - learning_rate \\* dW; b <— b - learning_rate \\* db\n",
    "\n",
    "- 8、可视化训练过程中 Loss 下降过程\n",
    "\n",
    "- 9、验证集测试预测准确率 accuracy = correct_number / total_number\n",
    "\n",
    "![title](./image/singlePerceptron.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def Sigmoid(Z):\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "def Initialize(dims):\n",
    "    W = np.zeros((dims, 1))\n",
    "    b = 0.0\n",
    "    return W, b\n",
    "\n",
    "def Forward_Propagate(W, b, X, y):\n",
    "    y_hat = Sigmoid(np.dot(W.T, X) + b)\n",
    "    log_loss = -1 / X.shape[1] * np.sum( y*np.log(y_hat) + (1-y)*np.log(1-y_hat) )\n",
    "    \n",
    "    dW = np.dot(X, (y_hat - y).T) / X.shape[1]\n",
    "    db = np.sum(y_hat - y) / X.shape[1]\n",
    "    \n",
    "    grads = { \"dW\" : dW, \"db\" : db}\n",
    "    \n",
    "    return grads, log_loss\n",
    "\n",
    "def Back_Propagate(W, b, X, y, iterations, learning_rate, print_logloss=False):\n",
    "    logloss = []\n",
    "    epoch = []\n",
    "    for epoch in range(iterations):\n",
    "        grads, log_loss = Forward_Propagate(W, b, X, y)\n",
    "        dW = grads['dW']\n",
    "        db = grads['db']\n",
    "        \n",
    "        W -= learning_rate * dW\n",
    "        b -= learning_rate * db\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            logloss.append(log_loss)\n",
    "            epoch.append(epoch)\n",
    "        if print_logloss && epoch % 100 == 0:\n",
    "            print(\"epoch {} , Logloss {} \".format(epoch, log_loss))\n",
    "            \n",
    "    grads = { \"dW\" : dW, \"db\" : db}\n",
    "    params = { \"W\" : W, \"b\" : b}\n",
    "    \n",
    "    return params, grads, logloss, epoch\n",
    "\n",
    "def Visualize_LogLoss(epoch, logloss):\n",
    "    plt.plot(epoch, logloss)\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"logloss\")\n",
    "    plt.title(\"LogLoss for training\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "def Predict_Accuracy(W, b, X, y):\n",
    "    y_pred = np.zeros((1, X.shape[1]))\n",
    "    \n",
    "    W = W.reshape(x.shape[0], 1)\n",
    "    probability = Sigmoid(np.dot(W.T, W) + b)\n",
    "    for index in range(probability.shape[1]):\n",
    "        if probability[:, index] > 0.5:\n",
    "            y_pred[:, index] = 1\n",
    "        else:\n",
    "            y_pred[:, index] = 0\n",
    "    \n",
    "    accuracy = (1 - np.mean(np.abs(y_pred - y))) * 100\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# pythonic\n",
    "def Single_Perceptron_Model(X_train, y_train, X_test, y_test, iterations=1000, learning_rate=1e-2, print_logloss=False):\n",
    "    W, b = Initialize(X_train.shape[0])\n",
    "    \n",
    "    params, _, logloss, epoch = Back_Propagate(W, b, X_train, y_train, iterations, learning_rate, print_logloss)\n",
    "    \n",
    "    W = params[\"W\"]\n",
    "    b = params[\"b\"]\n",
    "    \n",
    "    print(\"Train accuracy: \", Predict_Accuracy(W, b, X_train, y_train))\n",
    "    print(\"Test accuracy: \", Predict_Accuracy(W, b, X_test, y_train))\n",
    "    \n",
    "    Visualize_LogLoss(epoch, logloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多层感知机\n",
    "\n",
    "- 多层感知机模型结构图\n",
    "\n",
    "- 定义**输出单元** sigmoid = 1 / (1 + exp(-Z2)) \n",
    "\n",
    "- 隐藏层**激活函数** tanh(Z1) = ( exp(Z1) - exp(-Z1) ) / ( exp(Z1) + exp(-Z1) ) \n",
    "\n",
    "- 定义**仿射变换** Z1 = W1X + b1；A1 = tanh(Z1); Z2 = W2A2 + b2 \n",
    "\n",
    "\n",
    "- 数据集 X 有多少实例样本 N，y 有多少种类别 2\n",
    "- **前向传播**计算  y_hat = sigmoid(Z2) = sigmoid(W2A2 + b2) = sigmoid(W2(tanh(W1X + b)) + b2) \n",
    "\n",
    "- **损失函数**计算 LogLoss = -1/N \\* sum(y\\*log(y_hat) + (1-y)\\*log(1-y_hat))\n",
    "\n",
    "- **梯度计算** dW = X(y_hat - y) ; db = y_hat - y\n",
    "- **梯度计算** 先将损失函数对中间变量进行求导，然后根据数学的链式法则求解参数梯度\n",
    "\n",
    "- **反向传播，更新梯度**  W1 <— W1 - learning_rate \\* dW1; b1 <— b1 - learning_rate \\* db1\n",
    "- **反向传播，更新梯度**  W2 <— W2 - learning_rate \\* dW2; b2 <— b2 - learning_rate \\* db2\n",
    "\n",
    "- 可视化训练过程中 Loss 下降过程\n",
    "\n",
    "![title](./image/perceptron.jpg)\n",
    "\n",
    "\n",
    "**参数 parameter**\n",
    "- 神经元之间连接权重 W，偏置值 b\n",
    "\n",
    "**超参数 hyperparameter**\n",
    "- learning_rate, 学习率或者步长alpha\n",
    "- iterations, 迭代次数\n",
    "- hidden layer number, 隐藏层的层数L\n",
    "- hidden layer Neurons number, 隐藏层的神经元数量 unit\n",
    "- choice of activation function,  激活函数的选择\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def Layer_Size(X, y):\n",
    "    X_dims = X.shape[0]\n",
    "    hidden_neurons = 3\n",
    "    y_dims = y.shape[0]\n",
    "    return (X_dims, hidden_neurons, y_dims)\n",
    "\n",
    "def Initialize(X_dims, hidden_neurons, y_dims):\n",
    "    W1 = np.random.randn(hidden_neurons, X_dims) * 0.01\n",
    "    b1 = np.zeros((hidden_neurons, 1))\n",
    "    \n",
    "    W2 = np.random.randn(y_dims, hidden_neurons) * 0.01\n",
    "    b2 = np.zeros((y_dims, 1))\n",
    "    \n",
    "    parameters = {\"W1\" : W1, \"b1\" : b1,\n",
    "                  \"W2\" : W2, \"b2\" : b2}\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "def Feed_Forward_Propagation(X, parameters):\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    Z1 = np.dot(W1, x) + b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    output = sigmoid(Z2)\n",
    "    \n",
    "    cache = {\"Z1\" : Z1, \"A1\" : A1,\n",
    "             \"Z2\" : Z2, \"output\" : output}\n",
    "    \n",
    "    return output, cache\n",
    "\n",
    "def Compute_Logloss(output, y, parameters):\n",
    "\n",
    "    log_probs = np.multiply(np.log(output), y) + np.multiply(np.log(1-output), 1-y)\n",
    "    \n",
    "    cost = -1/y.shape[1] * np.sum(logprobs)\n",
    "    \n",
    "    cost = np.sequeeze(cost)\n",
    "    \n",
    "    return cost\n",
    "\n",
    "def Backward_Propagate(parameters, cache, X, y):\n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    \n",
    "    A1 = cache[\"A1\"]\n",
    "    output = cache[\"output\"]\n",
    "    \n",
    "    dZ2 = output - y\n",
    "    dW2 = 1/X.shape[1] * np.dot(dZ2, A1.T)\n",
    "    db2 = 1/X.shape[1] * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    \n",
    "    dZ1 = np.dot(W2.T, dZ2) * (1 - np.power(A1, 2))\n",
    "    dW1 = 1/X.shape[1] * np.dot(dZ1, X.T)\n",
    "    db1 = 1/X.shape[1] * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    grads = {\"dW1\" : dW1, \"db1\" : db1\n",
    "             \"dW2\" : dW2, \"db2\" : db2}\n",
    "    \n",
    "    return grads\n",
    "\n",
    "def Update_Parameters(parameters, grads, learning_rate = 1e-2):\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    dW1 = grads[\"dW1\"]\n",
    "    db1 = grads[\"db1\"]\n",
    "    dW2 = grads[\"dW2\"]\n",
    "    db2 = grads[\"db2\"]\n",
    "    \n",
    "    W1 -= dW1 * learning_rate\n",
    "    b1 -= db1 * learning_rete\n",
    "    W2 -= dW2 * learning_rate\n",
    "    b2 -= db2 * learning_rete\n",
    "    \n",
    "    parameters = {\"W1\" : W1, \"b1\" : b1,\n",
    "                  \"W2\" : W2, \"b2\" : b2}\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "def Visualize_LogLoss(epoch, logloss):\n",
    "    plt.plot(epoch, logloss)\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"logloss\")\n",
    "    plt.title(\"LogLoss for training\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# pythonic\n",
    "def Perceptron_Model(X, y, iterations=1000, learning_rate=1e-2, print_logloss=False):\n",
    "    np.random.seed(42)\n",
    "    logloss = []\n",
    "    epoch = []\n",
    "    \n",
    "    X_dims, hidden_neurons, y_dims = Layer_Size(X, y)\n",
    "    \n",
    "    parameters = Initialize(X_dims, hidden_neurons, y_dims)\n",
    "    \n",
    "    for epoch in range(iterations):\n",
    "        output, cache = Feed_Forward_Propagation(X, parameters)\n",
    "        \n",
    "        cost = Compute_Logloss(output, y, parameters)\n",
    "        \n",
    "        grads = Backward_Propagate(parameters, cache, X, y)\n",
    "        \n",
    "        parameters = Update_Parameters(parameters, grads, learning_rate = 1e-2)\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            logloss.append(log_loss)\n",
    "            epoch.append(epoch)\n",
    "        if print_logloss && epoch % 100 == 0:\n",
    "            print(\"epoch {} , Logloss {} \".format(epoch, log_loss))\n",
    "        \n",
    "    Visualize_LogLoss(epoch, logloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN 深度神经网络\n",
    "\n",
    "**参数 parameter**\n",
    "- 神经元之间连接权重 W，偏置值 b\n",
    "\n",
    "**超参数 hyperparameter**\n",
    "- learning_rate, 学习率或者步长alpha\n",
    "- iterations, 迭代次数\n",
    "- hidden layer number, 隐藏层的层数L\n",
    "- hidden layer Neurons number, 隐藏层的神经元数量 unit\n",
    "- choice of activation function,  激活函数的选择\n",
    "\n",
    "**网络结构**\n",
    "- 输入神经元个数，数据实例样本数\n",
    "- 隐藏层的层数L\n",
    "- 隐藏层的神经元数量 unit\n",
    "- 输出神经元个数，数据输出分类数\n",
    "\n",
    "**激活函数 activation function**\n",
    "- ReLu\n",
    "- sigmoid\n",
    "- tanh\n",
    "\n",
    "**输出单元 output unit**\n",
    "- 高斯输出分布的线性单元，即对隐藏特征不做非线性变换，直接产 y_hat = z = W.⊤ * h + b\n",
    "- 伯努利输出分布的 sigmoid 单元，即对隐藏特征先线性层求 z = W.⊤ * h + b，然后对这个值做 sigmoid 变换将其映射到[0, 1]，转化成概率值\n",
    "- 多元伯努利输出分布的 softmax 单元，可以看作是伯努利输出分布的 sigmoid 单元在多分类问题上的推广。softmax(Zi) = exp(Zi) / sum( exp(Zj) )\n",
    "\n",
    "**损失函数 cost function**\n",
    "- 二次损失函数 MSE\n",
    "- 最大似然函数 LogLoss\n",
    "\n",
    "**优化损失函数，梯度下降 GD**\n",
    "- 单个样本计算梯度\n",
    "- 先计算中间变量导数，然后链式法则计算参数梯度\n",
    "\n",
    "**训练网络**\n",
    "- 初始化权重参数以及方法\n",
    "- 前向传播计算/预测\n",
    "- 反向传播/更新梯度，更新权值参数/最优权值参数\n",
    "\n",
    "\n",
    "\n",
    "![title](./image/DNN.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    np.random.seed(42)\n",
    "    parameters = {}\n",
    "    \n",
    "    L = len(layer_dims)\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        parameters['w' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1] * 0.01)\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    \n",
    "    cache = (linear_cache, activation_cache)\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def L_model_forward(X, parameters):\n",
    "    caches = []\n",
    "    A = X\n",
    "    \n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        A, cache = linear_activation_forward(A_prev, parameters[\"W\" + str(l)], parameters[\"b\" + str(l)], \"relu\")\n",
    "        caches.append(cache)\n",
    "        \n",
    "    AL, cache = linear_activation_forward(A, parameters[\"W\" + str(L)], parameters[\"b\" + str(L)], \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "        \n",
    "    return AL, caches\n",
    "    \n",
    "def compute_cost(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    cost = -np.sum(np.multiply(Y, np.log(AL)) + np.multiply(1-Y, np.log(1-AL))) / m\n",
    "    \n",
    "    cost = np.sequeeze(cost)\n",
    "    \n",
    "    return cost\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    dW = np.dot(dZ, A_prev.T) / m\n",
    "    db = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA,activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    \n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    \n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, \"sigmoid\") \n",
    "    \n",
    "    for l in reversed(range(L - 1)):\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 2)], current_cache, \"relu\")\n",
    "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp    \n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "        \n",
    "    return grads\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * grads[\"db\" + str(l + 1)]\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "def Visualize_LogLoss(epoch, costs):\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel(\"cost\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.title(\"Learning rete = \" + str(learning_rate))\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# pythonic\n",
    "def L_layer_model(X, Y, layers_dims, learning_rate = 1e-4, iterations=3000, print_cost=False):\n",
    "    np.random.seed(42)\n",
    "    costs = []\n",
    "    \n",
    "    parameters = initialize_parameters_deep(layer_dims)\n",
    "    \n",
    "    for epoch in range(iterations):\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        \n",
    "        cost = compute_cost(AL, Y)\n",
    "        \n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        \n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            epoch.append(epoch)\n",
    "        if print_logloss && epoch % 100 == 0:\n",
    "            print(\"epoch {} , Logloss {} \".format(epoch, log_loss))\n",
    "        \n",
    "    Visualize_LogLoss(epoch, costs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度神经网络优化\n",
    "\n",
    "**监督机器学习**核心思想：正则化参数的同时最小化经验误差函数\n",
    "- 最小化经验误差是为了极大程度的拟合训练数据\n",
    "- 正则化参数是为了防止过分拟合训练数据\n",
    "\n",
    "\n",
    "\n",
    "**正则化**：正则化系数、正则化项（L1 and L2 范数）\n",
    "\n",
    "**dropout**：在神经网络训练过程中，对所有神经元按照一定的概率进行消除的处理方式。\n",
    "简单概述，dropout 就是在正常的神经网络基础之上给每一层的每一个神经元添加一道概率流程来随机丢弃某些神经元以达到防止过拟合的目的。\n",
    "\n",
    "\n",
    "**梯度下降法 Gradient Descent**\n",
    "- mini-batch Gradient Descent, 小批量梯度下降法\n",
    "- Stochastic Gradient Descent, 随机梯度下降法\n",
    "- momentum，带动量的梯度下降法\n",
    "\n",
    "**Adam 算法**\n",
    "- Adam，Adaptive Moment Estimation\n",
    "- 是在带动量的梯度下降法的基础上融合一种称之为 RMSprop（加速梯度下降）的算法而成的\n",
    "\n",
    "\n",
    "**CNN，卷积神经网络**\n",
    "\n",
    "**CapsNet 胶囊网络**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
