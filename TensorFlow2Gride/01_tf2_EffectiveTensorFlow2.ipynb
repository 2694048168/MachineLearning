{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fresh-procedure",
   "metadata": {},
   "source": [
    "# Effective TensorFlow 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impossible-receipt",
   "metadata": {},
   "source": [
    "## 1. A brief summary of major changes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flying-bosnia",
   "metadata": {},
   "source": [
    "### i. API Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "willing-contamination",
   "metadata": {},
   "source": [
    "### ii. Eager execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifteen-density",
   "metadata": {},
   "source": [
    "### iii. No more globals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baking-allocation",
   "metadata": {},
   "source": [
    "### iv. Functions, not sessions"
   ]
  },
  {
   "cell_type": "raw",
   "id": "defined-elephant",
   "metadata": {},
   "source": [
    "# TensorFlow 1.X\n",
    "outputs = session.run(f(placeholder), feed_dict={placeholder: input})\n",
    "# TensorFlow 2.0\n",
    "outputs = f(input)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "wrong-raleigh",
   "metadata": {},
   "source": [
    "AutoGraph supports arbitrary nestings of control flow, which makes it possible to performantly and concisely implement many complex ML programs such as sequence models, reinforcement learning, custom training loops, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latin-graph",
   "metadata": {},
   "source": [
    "## 2. Recommendations for idiomatic TensorFlow 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alien-demographic",
   "metadata": {},
   "source": [
    "### i. Refactor your code into smaller functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approximate-catholic",
   "metadata": {},
   "source": [
    "### ii. Use Keras layers and models to manage variables"
   ]
  },
  {
   "cell_type": "raw",
   "id": "funded-miracle",
   "metadata": {},
   "source": [
    "# Contrast:\n",
    "\n",
    "def dense(x, W, b):\n",
    "  return tf.nn.sigmoid(tf.matmul(x, W) + b)\n",
    "\n",
    "@tf.function\n",
    "def multilayer_perceptron(x, w0, b0, w1, b1, w2, b2 ...):\n",
    "  x = dense(x, w0, b0)\n",
    "  x = dense(x, w1, b1)\n",
    "  x = dense(x, w2, b2)\n",
    "  ...\n",
    "\n",
    "# You still have to manage w_i and b_i, and their shapes are defined far away from the code."
   ]
  },
  {
   "cell_type": "raw",
   "id": "transparent-appreciation",
   "metadata": {},
   "source": [
    "# with the Keras version:\n",
    "\n",
    "# Each layer can be called, with a signature equivalent to linear(x)\n",
    "layers = [tf.keras.layers.Dense(hidden_size, activation=tf.nn.sigmoid) for _ in range(n)]\n",
    "perceptron = tf.keras.Sequential(layers)\n",
    "\n",
    "# layers[3].trainable_variables => returns [w3, b3]\n",
    "# perceptron.trainable_variables => returns [w0, b0, ...]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "intense-sensitivity",
   "metadata": {},
   "source": [
    "# Here's a transfer learning example that demonstrates how Keras makes it easy to collect a subset of relevant variables. \n",
    "# Let's say you're training a multi-headed model with a shared trunk:\n",
    "\n",
    "trunk = tf.keras.Sequential([...])\n",
    "head1 = tf.keras.Sequential([...])\n",
    "head2 = tf.keras.Sequential([...])\n",
    "\n",
    "path1 = tf.keras.Sequential([trunk, head1])\n",
    "path2 = tf.keras.Sequential([trunk, head2])\n",
    "\n",
    "# Train on primary dataset\n",
    "for x, y in main_dataset:\n",
    "  with tf.GradientTape() as tape:\n",
    "    # training=True is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "    prediction = path1(x, training=True)\n",
    "    loss = loss_fn_head1(prediction, y)\n",
    "  # Simultaneously optimize trunk and head1 weights.\n",
    "  gradients = tape.gradient(loss, path1.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(gradients, path1.trainable_variables))\n",
    "\n",
    "# Fine-tune second head, reusing the trunk\n",
    "for x, y in small_dataset:\n",
    "  with tf.GradientTape() as tape:\n",
    "    # training=True is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "    prediction = path2(x, training=True)\n",
    "    loss = loss_fn_head2(prediction, y)\n",
    "  # Only optimize head2 weights, not trunk weights\n",
    "  gradients = tape.gradient(loss, head2.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(gradients, head2.trainable_variables))\n",
    "\n",
    "# You can publish just the trunk computation for other people to reuse.\n",
    "tf.saved_model.save(trunk, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "through-continuity",
   "metadata": {},
   "source": [
    "### iii. Combine tf.data.Datasets and @tf.function"
   ]
  },
  {
   "cell_type": "raw",
   "id": "acknowledged-stadium",
   "metadata": {},
   "source": [
    "When iterating over training data that fits in memory, feel free to use regular Python iteration. Otherwise, tf.data.Dataset is the best way to stream training data from disk. Datasets are iterables (not iterators), and work just like other Python iterables in Eager mode. You can fully utilize dataset async prefetching/streaming features by wrapping your code in tf.function(), which replaces Python iteration with the equivalent graph operations using AutoGraph"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fancy-forwarding",
   "metadata": {},
   "source": [
    "@tf.function\n",
    "def train(model, dataset, optimizer):\n",
    "  for x, y in dataset:\n",
    "    with tf.GradientTape() as tape:\n",
    "      # training=True is only needed if there are layers with different\n",
    "      # behavior during training versus inference (e.g. Dropout).\n",
    "      prediction = model(x, training=True)\n",
    "      loss = loss_fn(prediction, y)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "academic-improvement",
   "metadata": {},
   "source": [
    "# If you use the Keras .fit() API, you won't have to worry about dataset iteration.\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_fn)\n",
    "model.fit(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integral-wyoming",
   "metadata": {},
   "source": [
    "### iv. Take advantage of AutoGraph with Python control flow"
   ]
  },
  {
   "cell_type": "raw",
   "id": "sublime-hearing",
   "metadata": {},
   "source": [
    "AutoGraph provides a way to convert data-dependent control flow into graph-mode equivalents like tf.cond and tf.while_loop.\n",
    "\n",
    "One common place where data-dependent control flow appears is in sequence models. tf.keras.layers.RNN wraps an RNN cell, allowing you to either statically or dynamically unroll the recurrence. For demonstration's sake, you could reimplement dynamic unroll as follows:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "american-acrylic",
   "metadata": {},
   "source": [
    "class DynamicRNN(tf.keras.Model):\n",
    "\n",
    "  def __init__(self, rnn_cell):\n",
    "    super(DynamicRNN, self).__init__(self)\n",
    "    self.cell = rnn_cell\n",
    "\n",
    "  def call(self, input_data):\n",
    "    # [batch, time, features] -> [time, batch, features]\n",
    "    input_data = tf.transpose(input_data, [1, 0, 2])\n",
    "    outputs = tf.TensorArray(tf.float32, input_data.shape[0])\n",
    "    state = self.cell.zero_state(input_data.shape[1], dtype=tf.float32)\n",
    "    for i in tf.range(input_data.shape[0]):\n",
    "      output, state = self.cell(input_data[i], state)\n",
    "      outputs = outputs.write(i, output)\n",
    "    return tf.transpose(outputs.stack(), [1, 0, 2]), state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laden-involvement",
   "metadata": {},
   "source": [
    "### v. tf.metrics aggregates data and tf.summary logs them"
   ]
  },
  {
   "cell_type": "raw",
   "id": "quick-transportation",
   "metadata": {},
   "source": [
    "To log summaries, use tf.summary.(scalar|histogram|...) and redirect it to a writer using a context manager. (If you omit the context manager, nothing happens.) Unlike TF 1.x, the summaries are emitted directly to the writer; there is no separate \"merge\" op and no separate add_summary() call, which means that the step value must be provided at the callsite."
   ]
  },
  {
   "cell_type": "raw",
   "id": "applicable-hardwood",
   "metadata": {},
   "source": [
    "summary_writer = tf.summary.create_file_writer('/tmp/summaries')\n",
    "with summary_writer.as_default():\n",
    "  tf.summary.scalar('loss', 0.1, step=42)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "lasting-webmaster",
   "metadata": {},
   "source": [
    "To aggregate data before logging them as summaries, use tf.metrics. Metrics are stateful: They accumulate values and return a cumulative result when you call .result(). Clear accumulated values with .reset_states()."
   ]
  },
  {
   "cell_type": "raw",
   "id": "other-finance",
   "metadata": {},
   "source": [
    "def train(model, optimizer, dataset, log_freq=10):\n",
    "  avg_loss = tf.keras.metrics.Mean(name='loss', dtype=tf.float32)\n",
    "  for images, labels in dataset:\n",
    "    loss = train_step(model, optimizer, images, labels)\n",
    "    avg_loss.update_state(loss)\n",
    "    if tf.equal(optimizer.iterations % log_freq, 0):\n",
    "      tf.summary.scalar('loss', avg_loss.result(), step=optimizer.iterations)\n",
    "      avg_loss.reset_states()\n",
    "\n",
    "def test(model, test_x, test_y, step_num):\n",
    "  # training=False is only needed if there are layers with different\n",
    "  # behavior during training versus inference (e.g. Dropout).\n",
    "  loss = loss_fn(model(test_x, training=False), test_y)\n",
    "  tf.summary.scalar('loss', loss, step=step_num)\n",
    "\n",
    "train_summary_writer = tf.summary.create_file_writer('/tmp/summaries/train')\n",
    "test_summary_writer = tf.summary.create_file_writer('/tmp/summaries/test')\n",
    "\n",
    "with train_summary_writer.as_default():\n",
    "  train(model, optimizer, dataset)\n",
    "\n",
    "with test_summary_writer.as_default():\n",
    "  test(model, test_x, test_y, optimizer.iterations)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "synthetic-accreditation",
   "metadata": {},
   "source": [
    "# Visualize the generated summaries by pointing TensorBoard at the summary log directory:\n",
    "\n",
    "tensorboard --logdir /tmp/summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "committed-transfer",
   "metadata": {},
   "source": [
    "### vi. Use tf.config.experimental_run_functions_eagerly() when debugging"
   ]
  },
  {
   "cell_type": "raw",
   "id": "integral-essence",
   "metadata": {},
   "source": [
    "In TensorFlow 2.0, Eager execution lets you run the code step-by-step to inspect shapes, data types and values. Certain APIs, like tf.function, tf.keras, etc. are designed to use Graph execution, for performance and portability. When debugging, use tf.config.experimental_run_functions_eagerly(True) to use Eager execution inside this code.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "clinical-rental",
   "metadata": {},
   "source": [
    "@tf.function\n",
    "def f(x):\n",
    "  if x > 0:\n",
    "    import pdb\n",
    "    pdb.set_trace()\n",
    "    x = x + 1\n",
    "  return x\n",
    "\n",
    "tf.config.experimental_run_functions_eagerly(True)\n",
    "f(tf.constant(1))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "medium-marketplace",
   "metadata": {},
   "source": [
    ">>> f()\n",
    "-> x = x + 1\n",
    "(Pdb) l\n",
    "  6     @tf.function\n",
    "  7     def f(x):\n",
    "  8       if x > 0:\n",
    "  9         import pdb\n",
    " 10         pdb.set_trace()\n",
    " 11  ->     x = x + 1\n",
    " 12       return x\n",
    " 13\n",
    " 14     tf.config.experimental_run_functions_eagerly(True)\n",
    " 15     f(tf.constant(1))\n",
    "[EOF]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "built-apple",
   "metadata": {},
   "source": [
    "# This also works inside Keras models and other APIs that support Eager execution:\n",
    "\n",
    "class CustomModel(tf.keras.models.Model):\n",
    "\n",
    "  @tf.function\n",
    "  def call(self, input_data):\n",
    "    if tf.reduce_mean(input_data) > 0:\n",
    "      return input_data\n",
    "    else:\n",
    "      import pdb\n",
    "      pdb.set_trace()\n",
    "      return input_data // 2\n",
    "\n",
    "\n",
    "tf.config.experimental_run_functions_eagerly(True)\n",
    "model = CustomModel()\n",
    "model(tf.constant([-2, -4]))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "legendary-wealth",
   "metadata": {},
   "source": [
    ">>> call()\n",
    "-> return input_data // 2\n",
    "(Pdb) l\n",
    " 10         if tf.reduce_mean(input_data) > 0:\n",
    " 11           return input_data\n",
    " 12         else:\n",
    " 13           import pdb\n",
    " 14           pdb.set_trace()\n",
    " 15  ->       return input_data // 2\n",
    " 16\n",
    " 17\n",
    " 18     tf.config.experimental_run_functions_eagerly(True)\n",
    " 19     model = CustomModel()\n",
    " 20     model(tf.constant([-2, -4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bronze-moore",
   "metadata": {},
   "source": [
    "# reference\n",
    "\n",
    "### https://tensorflow.google.cn/guide/effective_tf2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bright-kentucky",
   "metadata": {},
   "source": [
    "# Migrate your TensorFlow 1 code to TensorFlow 2\n",
    "### https://tensorflow.google.cn/guide/migrate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprised-cartoon",
   "metadata": {},
   "source": [
    "# Automatically upgrade code to TensorFlow 2\n",
    "\n",
    "### https://tensorflow.google.cn/guide/upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passive-envelope",
   "metadata": {},
   "source": [
    "# Community testing FAQ\n",
    "### https://github.com/tensorflow/community/blob/master/sigs/testing/faq.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weighted-imperial",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
