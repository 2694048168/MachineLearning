{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "joint-transformation",
   "metadata": {},
   "source": [
    "# Introduction to Variables"
   ]
  },
  {
   "cell_type": "raw",
   "id": "republican-carry",
   "metadata": {},
   "source": [
    "A TensorFlow variable is the recommended way to represent shared, persistent state your program manipulates. This guide covers how to create, update, and manage instances of tf.Variable in TensorFlow.\n",
    "\n",
    "Variables are created and tracked via the tf.Variable class. A tf.Variable represents a tensor whose value can be changed by running ops on it. Specific ops allow you to read and modify the values of this tensor. Higher level libraries like tf.keras use tf.Variable to store model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparable-attendance",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "certain-producer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The version of Tensorflow: 2.4.1\n"
     ]
    }
   ],
   "source": [
    "# This notebook discusses variable placement. If you want to see on what device your variables are placed, uncomment this line.\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"The version of Tensorflow: {}\".format(tf.__version__))\n",
    "\n",
    "# Uncomment to see where your variables get placed (see below)\n",
    "tf.debugging.set_log_device_placement(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-greensboro",
   "metadata": {},
   "source": [
    "## 2. Create a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "color-accreditation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# To create a variable, provide an initial value. The tf.Variable will have the same dtype as the initialization value.\n",
    "\n",
    "my_tensor = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
    "my_variable = tf.Variable(my_tensor)\n",
    "\n",
    "# Variables can be all kinds of types, just like tensors\n",
    "bool_variable = tf.Variable([False, False, False, True])\n",
    "complex_variable = tf.Variable([5 + 4j, 6 + 1j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "continuous-guess",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:  (2, 2)\n",
      "DType:  <dtype: 'float32'>\n",
      "Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "As NumPy:  [[1. 2.]\n",
      " [3. 4.]]\n"
     ]
    }
   ],
   "source": [
    "# A variable looks and acts like a tensor, and, in fact, is a data structure backed by a tf.Tensor. \n",
    "# Like tensors, they have a dtype and a shape, and can be exported to NumPy.\n",
    "\n",
    "print(\"Shape: \", my_variable.shape)\n",
    "print(\"DType: \", my_variable.dtype)\n",
    "print(\"As NumPy: \", my_variable.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "private-sydney",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A variable: Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "<tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=\n",
      "array([[1., 2.],\n",
      "       [3., 4.]], dtype=float32)>\n",
      "Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "\n",
      "Viewed as a tensor: tf.Tensor(\n",
      "[[1. 2.]\n",
      " [3. 4.]], shape=(2, 2), dtype=float32)\n",
      "Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op ArgMax in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "\n",
      "Index of highest value: tf.Tensor([1 1], shape=(2,), dtype=int64)\n",
      "Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Reshape in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "\n",
      "Copying and reshaping:  tf.Tensor([[1. 2. 3. 4.]], shape=(1, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Most tensor operations work on variables as expected, although variables cannot be reshaped.\n",
    "\n",
    "print(\"A variable:\", my_variable)\n",
    "print(\"\\nViewed as a tensor:\", tf.convert_to_tensor(my_variable))\n",
    "print(\"\\nIndex of highest value:\", tf.argmax(my_variable))\n",
    "\n",
    "# This creates a new tensor; it does not reshape the variable.\n",
    "print(\"\\nCopying and reshaping: \", tf.reshape(my_variable, ([1,4])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "middle-swedish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "ValueError: Cannot assign to variable Variable:0 due to variable shape (2,) and value shape (3,) are incompatible\n"
     ]
    }
   ],
   "source": [
    "# As noted above, variables are backed by tensors. \n",
    "# You can reassign the tensor using tf.Variable.assign. \n",
    "# Calling assign does not (usually) allocate a new tensor; \n",
    "# instead, the existing tensor's memory is reused.\n",
    "\n",
    "a = tf.Variable([2.0, 3.0])\n",
    "# This will keep the same dtype, float32\n",
    "a.assign([1, 2]) \n",
    "# Not allowed as it resizes the variable: \n",
    "try:\n",
    "  a.assign([1.0, 2.0, 3.0])\n",
    "except Exception as e:\n",
    "  print(f\"{type(e).__name__}: {e}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "arabic-sheet",
   "metadata": {},
   "source": [
    "If you use a variable like a tensor in operations, you will usually operate on the backing tensor.\n",
    "\n",
    "Creating new variables from existing variables duplicates the backing tensors. Two variables will not share the same memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "lesser-browse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "[5. 6.]\n",
      "Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "[2. 3.]\n",
      "Executing op AssignAddVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "[7. 9.]\n",
      "Executing op AssignSubVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "[0. 0.]\n"
     ]
    }
   ],
   "source": [
    "a = tf.Variable([2.0, 3.0])\n",
    "# Create b based on the value of a\n",
    "b = tf.Variable(a)\n",
    "a.assign([5, 6])\n",
    "\n",
    "# a and b are different\n",
    "print(a.numpy())\n",
    "print(b.numpy())\n",
    "\n",
    "# There are other versions of assign\n",
    "print(a.assign_add([2,3]).numpy())  # [7. 9.]\n",
    "print(a.assign_sub([7,9]).numpy())  # [0. 0.]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pursuant-favorite",
   "metadata": {},
   "source": [
    "## 3. Lifecycles, naming, and watching"
   ]
  },
  {
   "cell_type": "raw",
   "id": "affecting-middle",
   "metadata": {},
   "source": [
    "In Python-based TensorFlow, tf.Variable instance have the same lifecycle as other Python objects. When there are no references to a variable it is automatically deallocated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bridal-claim",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AddV2 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Equal in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "tf.Tensor(\n",
      "[[False False]\n",
      " [False False]], shape=(2, 2), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "# Variables can also be named which can help you track and debug them. You can give two variables the same name.\n",
    "\n",
    "# Create a and b; they will have the same name but will be backed by\n",
    "# different tensors.\n",
    "a = tf.Variable(my_tensor, name=\"Mark\")\n",
    "# A new variable with the same name, but different value\n",
    "# Note that the scalar add is broadcast\n",
    "b = tf.Variable(my_tensor + 1, name=\"Mark\")\n",
    "\n",
    "# These are elementwise-unequal, despite having the same name\n",
    "print(a == b)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "radical-spanking",
   "metadata": {},
   "source": [
    "Variable names are preserved when saving and loading models. By default, variables in models will acquire unique variable names automatically, so you don't need to assign them yourself unless you want to.\n",
    "\n",
    "Although variables are important for differentiation, some variables will not need to be differentiated. You can turn off gradients for a variable by setting trainable to false at creation. An example of a variable that would not need gradients is a training step counter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "opened-scotland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\n"
     ]
    }
   ],
   "source": [
    "step_counter = tf.Variable(1, trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premium-judgment",
   "metadata": {},
   "source": [
    "## 4. Placing variables and tensors"
   ]
  },
  {
   "cell_type": "raw",
   "id": "canadian-converter",
   "metadata": {},
   "source": [
    "For better performance, TensorFlow will attempt to place tensors and variables on the fastest device compatible with its dtype. This means most variables are placed on a GPU if one is available.\n",
    "\n",
    "However, you can override this. In this snippet, place a float tensor and a variable on the CPU, even if a GPU is available. By turning on device placement logging (see Setup), you can see where the variable is placed.\n",
    "\n",
    "Note: Although manual placement works, using distribution strategies can be a more convenient and scalable way to optimize your computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "overhead-indian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op MatMul in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "tf.Tensor(\n",
      "[[22. 28.]\n",
      " [49. 64.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# If you run this notebook on different backends with and without a GPU you will see different logging. \n",
    "# Note that logging device placement must be turned on at the start of the session.\n",
    "\n",
    "with tf.device('CPU:0'):\n",
    "\n",
    "  # Create some tensors\n",
    "  a = tf.Variable([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "  b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "  c = tf.matmul(a, b)\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "normal-management",
   "metadata": {},
   "source": [
    "It's possible to set the location of a variable or tensor on one device and do the computation on another device. This will introduce delay, as data needs to be copied between the devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "oriented-mountain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "tf.Tensor(\n",
      "[[ 1.  4.  9.]\n",
      " [ 4. 10. 18.]], shape=(2, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# You might do this, however, if you had multiple GPU workers but only want one copy of the variables.\n",
    "\n",
    "with tf.device('CPU:0'):\n",
    "  a = tf.Variable([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "  b = tf.Variable([[1.0, 2.0, 3.0]])\n",
    "\n",
    "with tf.device('GPU:0'):\n",
    "  # Element-wise multiply\n",
    "  k = a * b\n",
    "\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "reduced-furniture",
   "metadata": {},
   "source": [
    "Note: Because tf.config.set_soft_device_placement is turned on by default, even if you run this code on a device without a GPU, it will still run. The multiplication step will happen on the CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-thing",
   "metadata": {},
   "source": [
    "# reference\n",
    "\n",
    "### https://tensorflow.google.cn/guide/variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "molecular-assistant",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
