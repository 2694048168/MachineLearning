{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "universal-anderson",
   "metadata": {},
   "source": [
    "# Working with preprocessing layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operating-passing",
   "metadata": {},
   "source": [
    "## 0. Keras preprocessing layers"
   ]
  },
  {
   "cell_type": "raw",
   "id": "paperback-dining",
   "metadata": {},
   "source": [
    "The Keras preprocessing layers API allows developers to build Keras-native input processing pipelines. These input processing pipelines can be used as independent preprocessing code in non-Keras workflows, combined directly with Keras models, and exported as part of a Keras SavedModel."
   ]
  },
  {
   "cell_type": "raw",
   "id": "classified-bonus",
   "metadata": {},
   "source": [
    "With Keras preprocessing layers, you can build and export models that are truly end-to-end: models that accept raw images or raw structured data as input; models that handle feature normalization or feature value indexing on their own."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artistic-checkout",
   "metadata": {},
   "source": [
    "## 1. Available preprocessing layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-creator",
   "metadata": {},
   "source": [
    "### i. Core preprocessing layers"
   ]
  },
  {
   "cell_type": "raw",
   "id": "supported-spouse",
   "metadata": {},
   "source": [
    "- TextVectorization layer: turns raw strings into an encoded representation that can be read by an Embedding layer or Dense layer.\n",
    "- Normalization layer: performs feature-wise normalize of input features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attached-northwest",
   "metadata": {},
   "source": [
    "### ii. Structured data preprocessing layers"
   ]
  },
  {
   "cell_type": "raw",
   "id": "viral-appearance",
   "metadata": {},
   "source": [
    "These layers are for structured data encoding and feature engineering.\n",
    "\n",
    "- CategoryEncoding layer: turns integer categorical features into one-hot, multi-hot, or TF-IDF dense representations.\n",
    "- Hashing layer: performs categorical feature hashing, also known as the \"hashing trick\".\n",
    "- Discretization layer: turns continuous numerical features into integer categorical features.\n",
    "- StringLookup layer: turns string categorical values into integers indices.\n",
    "- IntegerLookup layer: turns integer categorical values into integers indices.\n",
    "- CategoryCrossing layer: combines categorical features into co-occurrence features. E.g. if you have feature values \"a\" and \"b\", it can provide with the combination feature \"a and b are present at the same time\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "critical-cylinder",
   "metadata": {},
   "source": [
    "### iii. Image preprocessing layers"
   ]
  },
  {
   "cell_type": "raw",
   "id": "functioning-salad",
   "metadata": {},
   "source": [
    "These layers are for standardizing the inputs of an image model.\n",
    "\n",
    "- Resizing layer: resizes a batch of images to a target size.\n",
    "- Rescaling layer: rescales and offsets the values of a batch of image (e.g. go from inputs in the [0, 255] range to inputs in the [0, 1] range.\n",
    "- CenterCrop layer: returns a center crop of a batch of images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sublime-provision",
   "metadata": {},
   "source": [
    "### iv. Image data augmentation layers"
   ]
  },
  {
   "cell_type": "raw",
   "id": "short-marathon",
   "metadata": {},
   "source": [
    "These layers apply random augmentation transforms to a batch of images. They are only active during training.\n",
    "\n",
    "- RandomCrop layer\n",
    "- RandomFlip layer\n",
    "- RandomTranslation layer\n",
    "- RandomRotation layer\n",
    "- RandomZoom layer\n",
    "- RandomHeight layer\n",
    "- RandomWidth layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "according-payroll",
   "metadata": {},
   "source": [
    "## 2. The adapt() method"
   ]
  },
  {
   "cell_type": "raw",
   "id": "configured-comparison",
   "metadata": {},
   "source": [
    "Some preprocessing layers have an internal state that must be computed based on a sample of the training data. The list of stateful preprocessing layers is:\n",
    "\n",
    "- TextVectorization: holds a mapping between string tokens and integer indices\n",
    "- Normalization: holds the mean and standard deviation of the features\n",
    "- StringLookup and IntegerLookup: hold a mapping between input values and output indices.\n",
    "- CategoryEncoding: holds an index of input values.\n",
    "- Discretization: holds information about value bucket boundaries.\n",
    "\n",
    "\n",
    "Crucially, these layers are non-trainable. Their state is not set during training; it must be set before training, a step called \"adaptation\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "instrumental-match",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features mean: 0.00\n",
      "Features std: 1.00\n"
     ]
    }
   ],
   "source": [
    "# You set the state of a preprocessing layer by exposing it to training data, via the adapt() method:\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "data = np.array([[0.1, 0.2, 0.3], [0.8, 0.9, 1.0], [1.5, 1.6, 1.7],])\n",
    "layer = preprocessing.Normalization()\n",
    "layer.adapt(data)\n",
    "normalized_data = layer(data)\n",
    "\n",
    "print(\"Features mean: %.2f\" % (normalized_data.numpy().mean()))\n",
    "print(\"Features std: %.2f\" % (normalized_data.numpy().std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "biblical-imagination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[37 12 25  5  9 20 21  0  0]\n",
      " [51 34 27 33 29 18  0  0  0]\n",
      " [49 52 30 31 19 46 10  0  0]\n",
      " [ 7  5 50 43 28  7 47 17  0]\n",
      " [24 35 39 40  3  6 32 16  0]\n",
      " [ 4  2 15 14 22 23  0  0  0]\n",
      " [36 48  6 38 42  3 45  0  0]\n",
      " [ 4  2 13 41 53  8 44 26 11]], shape=(8, 9), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# The adapt() method takes either a Numpy array or a tf.data.Dataset object. \n",
    "# In the case of StringLookup and TextVectorization, \n",
    "# you can also pass a list of strings:\n",
    "\n",
    "data = [\n",
    "    \"ξεῖν᾽, ἦ τοι μὲν ὄνειροι ἀμήχανοι ἀκριτόμυθοι\",\n",
    "    \"γίγνοντ᾽, οὐδέ τι πάντα τελείεται ἀνθρώποισι.\",\n",
    "    \"δοιαὶ γάρ τε πύλαι ἀμενηνῶν εἰσὶν ὀνείρων:\",\n",
    "    \"αἱ μὲν γὰρ κεράεσσι τετεύχαται, αἱ δ᾽ ἐλέφαντι:\",\n",
    "    \"τῶν οἳ μέν κ᾽ ἔλθωσι διὰ πριστοῦ ἐλέφαντος,\",\n",
    "    \"οἵ ῥ᾽ ἐλεφαίρονται, ἔπε᾽ ἀκράαντα φέροντες:\",\n",
    "    \"οἱ δὲ διὰ ξεστῶν κεράων ἔλθωσι θύραζε,\",\n",
    "    \"οἵ ῥ᾽ ἔτυμα κραίνουσι, βροτῶν ὅτε κέν τις ἴδηται.\",\n",
    "]\n",
    "layer = preprocessing.TextVectorization()\n",
    "layer.adapt(data)\n",
    "vectorized_text = layer(data)\n",
    "print(vectorized_text)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "polar-weapon",
   "metadata": {},
   "source": [
    "In addition, adaptable layers always expose an option to directly set state via constructor arguments or weight assignment. If the intended state values are known at layer construction time, or are calculated outside of the adapt() call, they can be set without relying on the layer's internal computation. For instance, if external vocabulary files for the TextVectorization, StringLookup, or IntegerLookup layers already exist, those can be loaded directly into the lookup tables by passing a path to the vocabulary file in the layer's constructor arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "western-primary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[2 4 5]\n",
      " [5 1 3]], shape=(2, 3), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# Here's an example where we instantiate a StringLookup layer with precomputed vocabulary:\n",
    "\n",
    "vocab = [\"a\", \"b\", \"c\", \"d\"]\n",
    "data = tf.constant([[\"a\", \"c\", \"d\"], [\"d\", \"z\", \"b\"]])\n",
    "layer = preprocessing.StringLookup(vocabulary=vocab)\n",
    "vectorized_data = layer(data)\n",
    "print(vectorized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subsequent-pathology",
   "metadata": {},
   "source": [
    "## 3. Preprocessing data before the model or inside the model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "funny-pregnancy",
   "metadata": {},
   "source": [
    "There are two ways you could be using preprocessing layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "necessary-aviation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Make them part of the model, like this:\n",
    "from tensorflow import keras\n",
    "\n",
    "inputs = keras.Input(shape=input_shape)\n",
    "x = preprocessing_layer(inputs)\n",
    "outputs = rest_of_the_model(x)\n",
    "model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fresh-design",
   "metadata": {},
   "source": [
    "With this option, preprocessing will happen on device, synchronously with the rest of the model execution, meaning that it will benefit from GPU acceleration. If you're training on GPU, this is the best option for the Normalization layer, and for all image preprocessing and data augmentation layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-visibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: apply it to your tf.data.Dataset, \n",
    "# so as to obtain a dataset that yields batches of preprocessed data, like this:\n",
    "\n",
    "dataset = dataset.map(\n",
    "  lambda x, y: (preprocessing_layer(x), y))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "decreased-square",
   "metadata": {},
   "source": [
    "With this option, your preprocessing will happen on CPU, asynchronously, and will be buffered before going into the model."
   ]
  },
  {
   "cell_type": "raw",
   "id": "enabling-characteristic",
   "metadata": {},
   "source": [
    "This is the best option for TextVectorization, and all structured data preprocessing layers. It can also be a good option if you're training on CPU and you use image preprocessing layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frozen-latest",
   "metadata": {},
   "source": [
    "## 4. Benefits of doing preprocessing inside the model at inference time"
   ]
  },
  {
   "cell_type": "raw",
   "id": "whole-joining",
   "metadata": {},
   "source": [
    "Even if you go with option 2, you may later want to export an inference-only end-to-end model that will include the preprocessing layers. The key benefit to doing this is that it makes your model portable and it helps reduce the training/serving skew."
   ]
  },
  {
   "cell_type": "raw",
   "id": "certified-disclaimer",
   "metadata": {},
   "source": [
    "When all data preprocessing is part of the model, other people can load and use your model without having to be aware of how each feature is expected to be encoded & normalized. Your inference model will be able to process raw images or raw structured data, and will not require users of the model to be aware of the details of e.g. the tokenization scheme used for text, the indexing scheme used for categorical features, whether image pixel values are normalized to [-1, +1] or to [0, 1], etc. This is especially powerful if you're exporting your model to another runtime, such as TensorFlow.js: you won't have to reimplement your preprocessing pipeline in JavaScript."
   ]
  },
  {
   "cell_type": "raw",
   "id": "decimal-robertson",
   "metadata": {},
   "source": [
    "If you initially put your preprocessing layers in your tf.data pipeline, you can export an inference model that packages the preprocessing. Simply instantiate a new model that chains your preprocessing layers and your training model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complimentary-hazard",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=input_shape)\n",
    "x = preprocessing_layer(inputs)\n",
    "outputs = training_model(x)\n",
    "inference_model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "descending-bridal",
   "metadata": {},
   "source": [
    "## 5. Quick recipes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "early-curve",
   "metadata": {},
   "source": [
    "### i. Image data augmentation (on-device)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "compact-jurisdiction",
   "metadata": {},
   "source": [
    "Note that image data augmentation layers are only active during training (similarly to the Dropout layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "verified-saskatchewan",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Create a data augmentation stage with horizontal flipping, rotations, zooms\n",
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        preprocessing.RandomFlip(\"horizontal\"),\n",
    "        preprocessing.RandomRotation(0.1),\n",
    "        preprocessing.RandomZoom(0.1),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a model that includes the augmentation stage\n",
    "input_shape = (32, 32, 3)\n",
    "classes = 10\n",
    "inputs = keras.Input(shape=input_shape)\n",
    "# Augment images\n",
    "x = data_augmentation(inputs)\n",
    "# Rescale image values to [0, 1]\n",
    "x = preprocessing.Rescaling(1.0 / 255)(x)\n",
    "# Add the rest of the model\n",
    "outputs = keras.applications.ResNet50(\n",
    "    weights=None, input_shape=input_shape, classes=classes\n",
    ")(x)\n",
    "model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "asian-internship",
   "metadata": {},
   "source": [
    "You can see a similar setup in action in the example image classification from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limited-performer",
   "metadata": {},
   "source": [
    "### ii. Normalizing numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "roman-pattern",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 3s 1ms/step - loss: 2.1754\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f04c9f77c0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load some data\n",
    "(x_train, y_train), _ = keras.datasets.cifar10.load_data()\n",
    "x_train = x_train.reshape((len(x_train), -1))\n",
    "input_shape = x_train.shape[1:]\n",
    "classes = 10\n",
    "\n",
    "# Create a Normalization layer and set its internal state using the training data\n",
    "normalizer = preprocessing.Normalization()\n",
    "normalizer.adapt(x_train)\n",
    "\n",
    "# Create a model that include the normalization layer\n",
    "inputs = keras.Input(shape=input_shape)\n",
    "x = normalizer(inputs)\n",
    "outputs = layers.Dense(classes, activation=\"softmax\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "# Train the model\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\")\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thirty-metadata",
   "metadata": {},
   "source": [
    "### iii. Encoding string categorical features via one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "personal-inquiry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]], shape=(6, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Define some toy data\n",
    "data = tf.constant([\"a\", \"b\", \"c\", \"b\", \"c\", \"a\"])\n",
    "\n",
    "# Use StringLookup to build an index of the feature values\n",
    "indexer = preprocessing.StringLookup()\n",
    "indexer.adapt(data)\n",
    "\n",
    "# Use CategoryEncoding to encode the integer indices to a one-hot vector\n",
    "encoder = preprocessing.CategoryEncoding(output_mode=\"binary\")\n",
    "encoder.adapt(indexer(data))\n",
    "\n",
    "# Convert new test data (which includes unknown feature values)\n",
    "test_data = tf.constant([\"a\", \"b\", \"c\", \"d\", \"e\", \"\"])\n",
    "encoded_data = encoder(indexer(test_data))\n",
    "print(encoded_data)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "significant-gauge",
   "metadata": {},
   "source": [
    "Note that index 0 is reserved for missing values (which you should specify as the empty string \"\"), and index 1 is reserved for out-of-vocabulary values (values that were not seen during adapt()). You can configure this by using the mask_token and oov_token constructor arguments of StringLookup."
   ]
  },
  {
   "cell_type": "raw",
   "id": "valid-accused",
   "metadata": {},
   "source": [
    "You can see the StringLookup and CategoryEncoding layers in action in the example structured data classification from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serious-improvement",
   "metadata": {},
   "source": [
    "### iv. Encoding integer categorical features via one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "incoming-setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]], shape=(6, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Define some toy data\n",
    "data = tf.constant([10, 20, 20, 10, 30, 0])\n",
    "\n",
    "# Use IntegerLookup to build an index of the feature values\n",
    "indexer = preprocessing.IntegerLookup()\n",
    "indexer.adapt(data)\n",
    "\n",
    "# Use CategoryEncoding to encode the integer indices to a one-hot vector\n",
    "encoder = preprocessing.CategoryEncoding(output_mode=\"binary\")\n",
    "encoder.adapt(indexer(data))\n",
    "\n",
    "# Convert new test data (which includes unknown feature values)\n",
    "test_data = tf.constant([10, 10, 20, 50, 60, 0])\n",
    "encoded_data = encoder(indexer(test_data))\n",
    "print(encoded_data)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "still-produce",
   "metadata": {},
   "source": [
    "Note that index 0 is reserved for missing values (which you should specify as the value 0), and index 1 is reserved for out-of-vocabulary values (values that were not seen during adapt()). You can configure this by using the mask_value and oov_value constructor arguments of IntegerLookup."
   ]
  },
  {
   "cell_type": "raw",
   "id": "productive-bristol",
   "metadata": {},
   "source": [
    "You can see the IntegerLookup and CategoryEncoding layers in action in the example structured data classification from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlling-identifier",
   "metadata": {},
   "source": [
    "### v. Applying the hashing trick to an integer categorical feature"
   ]
  },
  {
   "cell_type": "raw",
   "id": "statewide-process",
   "metadata": {},
   "source": [
    "If you have a categorical feature that can take many different values (on the order of 10e3 or higher), where each value only appears a few times in the data, it becomes impractical and ineffective to index and one-hot encode the feature values. Instead, it can be a good idea to apply the \"hashing trick\": hash the values to a vector of fixed size. This keeps the size of the feature space manageable, and removes the need for explicit indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "conventional-fighter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 64)\n"
     ]
    }
   ],
   "source": [
    "# Sample data: 10,000 random integers with values between 0 and 100,000\n",
    "data = np.random.randint(0, 100000, size=(10000, 1))\n",
    "\n",
    "# Use the Hashing layer to hash the values to the range [0, 64]\n",
    "hasher = preprocessing.Hashing(num_bins=64, salt=1337)\n",
    "\n",
    "# Use the CategoryEncoding layer to one-hot encode the hashed values\n",
    "encoder = preprocessing.CategoryEncoding(max_tokens=64, output_mode=\"binary\")\n",
    "encoded_data = encoder(hasher(data))\n",
    "print(encoded_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interim-corner",
   "metadata": {},
   "source": [
    "### vi. Encoding text as a sequence of token indices"
   ]
  },
  {
   "cell_type": "raw",
   "id": "affiliated-pakistan",
   "metadata": {},
   "source": [
    "This is how you should preprocess text to be passed to an Embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "conscious-senegal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['', '[UNK]', 'the', 'side', 'you', 'with', 'will', 'wider', 'them', 'than', 'sky', 'put', 'other', 'one', 'is', 'for', 'ease', 'contain', 'by', 'brain', 'beside', 'and']\n"
     ]
    }
   ],
   "source": [
    "# Define some text data to adapt the layer\n",
    "data = tf.constant(\n",
    "    [\n",
    "        \"The Brain is wider than the Sky\",\n",
    "        \"For put them side by side\",\n",
    "        \"The one the other will contain\",\n",
    "        \"With ease and You beside\",\n",
    "    ]\n",
    ")\n",
    "# Instantiate TextVectorization with \"int\" output_mode\n",
    "text_vectorizer = preprocessing.TextVectorization(output_mode=\"int\")\n",
    "# Index the vocabulary via `adapt()`\n",
    "text_vectorizer.adapt(data)\n",
    "\n",
    "# You can retrieve the vocabulary we indexed via get_vocabulary()\n",
    "vocab = text_vectorizer.get_vocabulary()\n",
    "print(\"Vocabulary:\", vocab)\n",
    "\n",
    "# Create an Embedding + LSTM model\n",
    "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
    "x = text_vectorizer(inputs)\n",
    "x = layers.Embedding(input_dim=len(vocab), output_dim=64)(x)\n",
    "outputs = layers.LSTM(1)(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "# Call the model on test data (which includes unknown tokens)\n",
    "test_data = tf.constant([\"The Brain is deeper than the sea\"])\n",
    "test_output = model(test_data)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "artistic-paragraph",
   "metadata": {},
   "source": [
    "You can see the TextVectorization layer in action, combined with an Embedding mode, in the example text classification from scratch."
   ]
  },
  {
   "cell_type": "raw",
   "id": "driven-lafayette",
   "metadata": {},
   "source": [
    "Note that when training such a model, for best performance, you should use the TextVectorization layer as part of the input pipeline (which is what we do in the text classification example above)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "correct-increase",
   "metadata": {},
   "source": [
    "### vii Encoding text as a dense matrix of ngrams with multi-hot encoding"
   ]
  },
  {
   "cell_type": "raw",
   "id": "interpreted-source",
   "metadata": {},
   "source": [
    "This is how you should preprocess text to be passed to a Dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "modified-hypothesis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded text:\n",
      " [[1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.]] \n",
      "\n",
      "Model output: tf.Tensor([[-0.05050808]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Define some text data to adapt the layer\n",
    "data = tf.constant(\n",
    "    [\n",
    "        \"The Brain is wider than the Sky\",\n",
    "        \"For put them side by side\",\n",
    "        \"The one the other will contain\",\n",
    "        \"With ease and You beside\",\n",
    "    ]\n",
    ")\n",
    "# Instantiate TextVectorization with \"binary\" output_mode (multi-hot)\n",
    "# and ngrams=2 (index all bigrams)\n",
    "text_vectorizer = preprocessing.TextVectorization(output_mode=\"binary\", ngrams=2)\n",
    "# Index the bigrams via `adapt()`\n",
    "text_vectorizer.adapt(data)\n",
    "\n",
    "print(\n",
    "    \"Encoded text:\\n\",\n",
    "    text_vectorizer([\"The Brain is deeper than the sea\"]).numpy(),\n",
    "    \"\\n\",\n",
    ")\n",
    "\n",
    "# Create a Dense model\n",
    "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
    "x = text_vectorizer(inputs)\n",
    "outputs = layers.Dense(1)(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "# Call the model on test data (which includes unknown tokens)\n",
    "test_data = tf.constant([\"The Brain is deeper than the sea\"])\n",
    "test_output = model(test_data)\n",
    "\n",
    "print(\"Model output:\", test_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accurate-tissue",
   "metadata": {},
   "source": [
    "### viii Encoding text as a dense matrix of ngrams with TF-IDF weighting"
   ]
  },
  {
   "cell_type": "raw",
   "id": "opposite-mitchell",
   "metadata": {},
   "source": [
    "This is an alternative way of preprocessing text before passing it to a Dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "elder-special",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded text:\n",
      " [[8.04719   1.6945957 0.        0.        0.        0.        0.\n",
      "  0.        0.        0.        0.        0.        0.        0.\n",
      "  0.        0.        1.0986123 1.0986123 1.0986123 0.        0.\n",
      "  0.        0.        0.        0.        0.        0.        0.\n",
      "  1.0986123 0.        0.        0.        0.        0.        0.\n",
      "  0.        1.0986123 1.0986123 0.        0.        0.       ]] \n",
      "\n",
      "Model output: tf.Tensor([[1.2472827]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Define some text data to adapt the layer\n",
    "data = tf.constant(\n",
    "    [\n",
    "        \"The Brain is wider than the Sky\",\n",
    "        \"For put them side by side\",\n",
    "        \"The one the other will contain\",\n",
    "        \"With ease and You beside\",\n",
    "    ]\n",
    ")\n",
    "# Instantiate TextVectorization with \"tf-idf\" output_mode\n",
    "# (multi-hot with TF-IDF weighting) and ngrams=2 (index all bigrams)\n",
    "text_vectorizer = preprocessing.TextVectorization(output_mode=\"tf-idf\", ngrams=2)\n",
    "# Index the bigrams and learn the TF-IDF weights via `adapt()`\n",
    "text_vectorizer.adapt(data)\n",
    "\n",
    "print(\n",
    "    \"Encoded text:\\n\",\n",
    "    text_vectorizer([\"The Brain is deeper than the sea\"]).numpy(),\n",
    "    \"\\n\",\n",
    ")\n",
    "\n",
    "# Create a Dense model\n",
    "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
    "x = text_vectorizer(inputs)\n",
    "outputs = layers.Dense(1)(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "# Call the model on test data (which includes unknown tokens)\n",
    "test_data = tf.constant([\"The Brain is deeper than the sea\"])\n",
    "test_output = model(test_data)\n",
    "print(\"Model output:\", test_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revolutionary-gauge",
   "metadata": {},
   "source": [
    "# reference\n",
    "\n",
    "### https://tensorflow.google.cn/guide/keras/preprocessing_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chubby-concrete",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
