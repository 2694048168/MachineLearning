{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "tracked-talent",
   "metadata": {},
   "source": [
    "# Random number generation"
   ]
  },
  {
   "cell_type": "raw",
   "id": "atmospheric-switzerland",
   "metadata": {},
   "source": [
    "TensorFlow provides a set of pseudo-random number generators (RNG), in the tf.random module. This document describes how you can control the random number generators, and how these generators interact with other tensorflow sub-systems.\n",
    "\n",
    "TensorFlow provides two approaches for controlling the random number generation process:\n",
    "\n",
    "1. Through the explicit use of tf.random.Generator objects. Each such object maintains a state (in tf.Variable) that will be changed after each number generation.\n",
    "\n",
    "2. Through the purely-functional stateless random functions like tf.random.stateless_uniform. Calling these functions with the same arguments (which include the seed) and on the same device will always produce the same results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nasty-casino",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "frequent-mississippi",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The version of Tensorflow: 2.4.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"The version of Tensorflow: {}\".format(tf.__version__))\n",
    "\n",
    "# Creates 2 virtual devices cpu:0 and cpu:1 for using distribution strategy\n",
    "physical_devices = tf.config.list_physical_devices(\"CPU\")\n",
    "tf.config.experimental.set_virtual_device_configuration(\n",
    "    physical_devices[0], [\n",
    "        tf.config.experimental.VirtualDeviceConfiguration(),\n",
    "        tf.config.experimental.VirtualDeviceConfiguration()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southeast-carry",
   "metadata": {},
   "source": [
    "## 2. The tf.random.Generator class"
   ]
  },
  {
   "cell_type": "raw",
   "id": "accurate-morning",
   "metadata": {},
   "source": [
    "The tf.random.Generator class is used in cases where you want each RNG call to produce different results. It maintains an internal state (managed by a tf.Variable object) which will be updated every time random numbers are generated. Because the state is managed by tf.Variable, it enjoys all facilities provided by tf.Variable such as easy checkpointing, automatic control-dependency and thread safety."
   ]
  },
  {
   "cell_type": "raw",
   "id": "employed-chile",
   "metadata": {},
   "source": [
    "You can get a tf.random.Generator by manually creating an object of the class or call tf.random.get_global_generator() to get the default global generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "constitutional-joyce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.43842277 -0.53439844 -0.07710262]\n",
      " [ 1.5658046  -0.1012345  -0.2744976 ]], shape=(2, 3), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[-0.34302068  1.2457008   0.9001127 ]\n",
      " [ 1.1962626  -0.3957007   0.77666587]], shape=(2, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "g1 = tf.random.Generator.from_seed(1)\n",
    "print(g1.normal(shape=[2, 3]))\n",
    "g2 = tf.random.get_global_generator()\n",
    "print(g2.normal(shape=[2, 3]))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "refined-exercise",
   "metadata": {},
   "source": [
    "There are multiple ways to create a generator object. The easiest is Generator.from_seed, as shown above, that creates a generator from a seed. A seed is any non-negative integer. from_seed also takes an optional argument alg which is the RNG algorithm that will be used by this generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "improving-plenty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.43842277 -0.53439844 -0.07710262]\n",
      " [ 1.5658046  -0.1012345  -0.2744976 ]], shape=(2, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "g1 = tf.random.Generator.from_seed(1, alg='philox')\n",
    "print(g1.normal(shape=[2, 3]))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "nearby-disorder",
   "metadata": {},
   "source": [
    "See the Algorithms section below for more information about it."
   ]
  },
  {
   "cell_type": "raw",
   "id": "closing-humidity",
   "metadata": {},
   "source": [
    "Another way to create a generator is with Generator.from_non_deterministic_state. A generator created this way will start from a non-deterministic state, depending on e.g. time and OS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "challenging-preliminary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-1.7864108  -1.0207369   1.8253692 ]\n",
      " [-0.6798147   0.9397597   0.50946605]], shape=(2, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "g = tf.random.Generator.from_non_deterministic_state()\n",
    "print(g.normal(shape=[2, 3]))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "olive-diagnosis",
   "metadata": {},
   "source": [
    "There are yet other ways to create generators, such as from explicit states, which are not covered by this guide."
   ]
  },
  {
   "cell_type": "raw",
   "id": "homeless-bathroom",
   "metadata": {},
   "source": [
    "When using tf.random.get_global_generator to get the global generator, you need to be careful about device placement. The global generator is created (from a non-deterministic state) at the first time tf.random.get_global_generator is called, and placed on the default device at that call. So, for example, if the first site you call tf.random.get_global_generator is within a tf.device(\"gpu\") scope, the global generator will be placed on the GPU, and using the global generator later on from the CPU will incur a GPU-to-CPU copy.\n",
    "\n",
    "There is also a function tf.random.set_global_generator for replacing the global generator with another generator object. This function should be used with caution though, because the old global generator may have been captured by a tf.function (as a weak reference), and replacing it will cause it to be garbage collected, breaking the tf.function. A better way to reset the global generator is to use one of the \"reset\" functions such as Generator.reset_from_seed, which won't create new generator objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "centered-reaction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.43842277, shape=(), dtype=float32)\n",
      "tf.Tensor(1.6272374, shape=(), dtype=float32)\n",
      "tf.Tensor(0.43842277, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "g = tf.random.Generator.from_seed(1)\n",
    "print(g.normal([]))\n",
    "print(g.normal([]))\n",
    "g.reset_from_seed(1)\n",
    "print(g.normal([]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automated-census",
   "metadata": {},
   "source": [
    "### i. Creating independent random-number streams"
   ]
  },
  {
   "cell_type": "raw",
   "id": "recognized-personal",
   "metadata": {},
   "source": [
    "In many applications one needs multiple independent random-number streams, independent in the sense that they won't overlap and won't have any statistically detectable correlations. This is achieved by using Generator.split to create multiple generators that are guaranteed to be independent of each other (i.e. generating independent streams)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "constant-pennsylvania",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.43842277, shape=(), dtype=float32)\n",
      "tf.Tensor(2.536413, shape=(), dtype=float32)\n",
      "tf.Tensor(0.33186463, shape=(), dtype=float32)\n",
      "tf.Tensor(-0.07144657, shape=(), dtype=float32)\n",
      "tf.Tensor(-0.79253083, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "g = tf.random.Generator.from_seed(1)\n",
    "print(g.normal([]))\n",
    "new_gs = g.split(3)\n",
    "for new_g in new_gs:\n",
    "  print(new_g.normal([]))\n",
    "print(g.normal([]))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "great-nutrition",
   "metadata": {},
   "source": [
    "split will change the state of the generator on which it is called (g in the above example), similar to an RNG method such as normal. In addition to being independent of each other, the new generators (new_gs) are also guaranteed to be independent of the old one (g)."
   ]
  },
  {
   "cell_type": "raw",
   "id": "generic-desert",
   "metadata": {},
   "source": [
    "Spawning new generators is also useful when you want to make sure the generator you use is on the same device as other computations, to avoid the overhead of cross-device copy. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "stylish-narrative",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(3.0575864, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.device(\"cpu\"):  # change \"cpu\" to the device you want\n",
    "  g = tf.random.get_global_generator().split(1)[0]  \n",
    "  print(g.normal([]))  # use of g won't cause cross-device copy, unlike the global generator"
   ]
  },
  {
   "cell_type": "raw",
   "id": "strange-worker",
   "metadata": {},
   "source": [
    "Note: In theory, you can use constructors such as from_seed instead of split here to obtain a new generator, but by doing so you lose the guarantee that the new generator is independent of the global generator. You will also run the risk that you may accidentally create two generators with the same seed or with seeds that lead to overlapping random-number streams."
   ]
  },
  {
   "cell_type": "raw",
   "id": "qualified-saudi",
   "metadata": {},
   "source": [
    "You can do splitting recursively, calling split on splitted generators. There are no limits (barring integer overflow) on the depth of recursions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colored-reason",
   "metadata": {},
   "source": [
    "### ii. Interaction with tf.function"
   ]
  },
  {
   "cell_type": "raw",
   "id": "understanding-compression",
   "metadata": {},
   "source": [
    "tf.random.Generator obeys the same rules as tf.Variable when used with tf.function. This includes three aspects.\n",
    "\n",
    "Creating generators outside tf.function\n",
    "tf.function can use a generator created outside of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fifteen-request",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.43842277, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "g = tf.random.Generator.from_seed(1)\n",
    "@tf.function\n",
    "def foo():\n",
    "  return g.normal([])\n",
    "print(foo())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ordinary-toddler",
   "metadata": {},
   "source": [
    "The user needs to make sure that the generator object is still alive (not garbage-collected) when the function is called.\n",
    "\n",
    "Creating generators inside tf.function\n",
    "Creation of generators inside a tf.function can only happend during the first run of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "experimental-statistics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.43842277, shape=(), dtype=float32)\n",
      "tf.Tensor(1.6272374, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "g = None\n",
    "@tf.function\n",
    "def foo():\n",
    "  global g\n",
    "  if g is None:\n",
    "    g = tf.random.Generator.from_seed(1)\n",
    "  return g.normal([])\n",
    "print(foo())\n",
    "print(foo())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emotional-sunset",
   "metadata": {},
   "source": [
    "### Passing generators as arguments to tf.function"
   ]
  },
  {
   "cell_type": "raw",
   "id": "regulation-sacrifice",
   "metadata": {},
   "source": [
    "When used as an argument to a tf.function, different generator objects with the same state size (state size is determined by the RNG algorithm) won't cause retracing of the tf.function, while those with different state sizes will."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "realistic-grade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "num_traces = 0\n",
    "@tf.function\n",
    "def foo(g):\n",
    "  global num_traces\n",
    "  num_traces += 1\n",
    "  return g.normal([])\n",
    "foo(tf.random.Generator.from_seed(1))\n",
    "foo(tf.random.Generator.from_seed(2))\n",
    "print(num_traces)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "established-purple",
   "metadata": {},
   "source": [
    "### iii. Interaction with distribution strategies"
   ]
  },
  {
   "cell_type": "raw",
   "id": "descending-helmet",
   "metadata": {},
   "source": [
    "There are three ways in which Generator interacts with distribution strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nonprofit-murder",
   "metadata": {},
   "source": [
    "### Creating generators outside distribution strategies"
   ]
  },
  {
   "cell_type": "raw",
   "id": "accomplished-river",
   "metadata": {},
   "source": [
    "If a generator is created outside strategy scopes, all replicasâ€™ access to the generator will be serialized, and hence the replicas will get different random numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "animal-local",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0', '/job:localhost/replica:0/task:0/device:CPU:1')\n",
      "WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.\n",
      "tf.Tensor(0.43842277, shape=(), dtype=float32)\n",
      "tf.Tensor(1.6272374, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "g = tf.random.Generator.from_seed(1)\n",
    "strat = tf.distribute.MirroredStrategy(devices=[\"cpu:0\", \"cpu:1\"])\n",
    "with strat.scope():\n",
    "  def f():\n",
    "    print(g.normal([]))\n",
    "  results = strat.run(f)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "light-billion",
   "metadata": {},
   "source": [
    "Note that this usage may have performance issues because the generator's device is different from the replicas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "buried-hamburg",
   "metadata": {},
   "source": [
    "### Creating generators inside distribution strategies"
   ]
  },
  {
   "cell_type": "raw",
   "id": "located-mirror",
   "metadata": {},
   "source": [
    "Creating generators inside strategy scopes is disallowed, because there is ambiguity on how to replicate a generator (e.g. should it be copied so that each replica gets the same random numbers, or 'split' so that each replica gets different random numbers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "rental-brother",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0', '/job:localhost/replica:0/task:0/device:CPU:1')\n",
      "ValueError: Creating a generator within a strategy scope is disallowed, because there is ambiguity on how to replicate a generator (e.g. should it be copied so that each replica gets the same random numbers, or 'split' so that each replica gets different random numbers).\n"
     ]
    }
   ],
   "source": [
    "strat = tf.distribute.MirroredStrategy(devices=[\"cpu:0\", \"cpu:1\"])\n",
    "with strat.scope():\n",
    "  try:\n",
    "    tf.random.Generator.from_seed(1)\n",
    "  except ValueError as e:\n",
    "    print(\"ValueError:\", e)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "higher-bicycle",
   "metadata": {},
   "source": [
    "Note that Strategy.run will run its argument function in a strategy scope implicitly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "heavy-month",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0', '/job:localhost/replica:0/task:0/device:CPU:1')\n",
      "WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.\n",
      "INFO:tensorflow:Error reported to Coordinator: Creating a generator within a strategy scope is disallowed, because there is ambiguity on how to replicate a generator (e.g. should it be copied so that each replica gets the same random numbers, or 'split' so that each replica gets different random numbers).\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\miniconda\\install\\lib\\site-packages\\tensorflow\\python\\training\\coordinator.py\", line 297, in stop_on_exception\n",
      "    yield\n",
      "  File \"d:\\miniconda\\install\\lib\\site-packages\\tensorflow\\python\\distribute\\mirrored_run.py\", line 323, in run\n",
      "    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\n",
      "  File \"d:\\miniconda\\install\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 572, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"<ipython-input-13-2cd7806456bd>\", line 3, in f\n",
      "    tf.random.Generator.from_seed(1)\n",
      "  File \"d:\\miniconda\\install\\lib\\site-packages\\tensorflow\\python\\ops\\stateful_random_ops.py\", line 453, in from_seed\n",
      "    return cls(state=state, alg=alg)\n",
      "  File \"d:\\miniconda\\install\\lib\\site-packages\\tensorflow\\python\\ops\\stateful_random_ops.py\", line 374, in __init__\n",
      "    self._state_var = self._create_variable(state, dtype=STATE_TYPE,\n",
      "  File \"d:\\miniconda\\install\\lib\\site-packages\\tensorflow\\python\\ops\\stateful_random_ops.py\", line 389, in _create_variable\n",
      "    raise ValueError(\n",
      "ValueError: Creating a generator within a strategy scope is disallowed, because there is ambiguity on how to replicate a generator (e.g. should it be copied so that each replica gets the same random numbers, or 'split' so that each replica gets different random numbers).\n",
      "ValueError: Creating a generator within a strategy scope is disallowed, because there is ambiguity on how to replicate a generator (e.g. should it be copied so that each replica gets the same random numbers, or 'split' so that each replica gets different random numbers).\n"
     ]
    }
   ],
   "source": [
    "strat = tf.distribute.MirroredStrategy(devices=[\"cpu:0\", \"cpu:1\"])\n",
    "def f():\n",
    "  tf.random.Generator.from_seed(1)\n",
    "try:\n",
    "  strat.run(f)\n",
    "except ValueError as e:\n",
    "  print(\"ValueError:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "veterinary-clarity",
   "metadata": {},
   "source": [
    "### Passing generators as arguments to Strategy.run"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cubic-quest",
   "metadata": {},
   "source": [
    "If you want each replica to use its own generator, you need to make n generators (either by copying or splitting), where n is the number of replicas, and then pass them as arguments to Strategy.run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "falling-graph",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0', '/job:localhost/replica:0/task:0/device:CPU:1')\n",
      "WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.\n",
      "WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.\n",
      "tf.Tensor(-0.14334793, shape=(), dtype=float32)\n",
      "tf.Tensor(1.0381967, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "strat = tf.distribute.MirroredStrategy(devices=[\"cpu:0\", \"cpu:1\"])\n",
    "gs = tf.random.get_global_generator().split(2)\n",
    "# to_args is a workaround for the absence of APIs to create arguments for \n",
    "# run. It will be replaced when such APIs are available.\n",
    "def to_args(gs):  \n",
    "  with strat.scope():\n",
    "    def f():\n",
    "      return [gs[tf.distribute.get_replica_context().replica_id_in_sync_group]]\n",
    "    return strat.run(f)\n",
    "args = to_args(gs)\n",
    "def f(g):\n",
    "  print(g.normal([]))\n",
    "results = strat.run(f, args=args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "whole-particle",
   "metadata": {},
   "source": [
    "## 3. Stateless RNGs"
   ]
  },
  {
   "cell_type": "raw",
   "id": "portable-belfast",
   "metadata": {},
   "source": [
    "Usage of stateless RNGs is simple. Since they are just pure functions, there is no state or side effect involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "broke-updating",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.5441101   0.20738031  0.07356433]\n",
      " [ 0.04643455 -1.30159    -0.95385665]], shape=(2, 3), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ 0.5441101   0.20738031  0.07356433]\n",
      " [ 0.04643455 -1.30159    -0.95385665]], shape=(2, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.random.stateless_normal(shape=[2, 3], seed=[1, 2]))\n",
    "print(tf.random.stateless_normal(shape=[2, 3], seed=[1, 2]))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "normal-playback",
   "metadata": {},
   "source": [
    "Every stateless RNG requires a seed argument, which needs to be an integer Tensor of shape [2]. The results of the op are fully determined by this seed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clear-capture",
   "metadata": {},
   "source": [
    "## 4. Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coral-hawaii",
   "metadata": {},
   "source": [
    "### i. General"
   ]
  },
  {
   "cell_type": "raw",
   "id": "suspected-publicity",
   "metadata": {},
   "source": [
    "Both the tf.random.Generator class and the stateless functions support the Philox algorithm (written as \"philox\" or tf.random.Algorithm.PHILOX) on all devices.\n",
    "\n",
    "Different devices will generate the same integer numbers, if using the same algorithm and starting from the same state. They will also generate \"almost the same\" float-point numbers, though there may be small numerical discrepancies caused by the different ways the devices carry out the float-point computation (e.g. reduction order)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polar-charge",
   "metadata": {},
   "source": [
    "### ii. XLA devices"
   ]
  },
  {
   "cell_type": "raw",
   "id": "distinguished-index",
   "metadata": {},
   "source": [
    "On XLA-driven devices (such as TPU, and also CPU/GPU when XLA is enabled) the ThreeFry algorithm (written as \"threefry\" or tf.random.Algorithm.THREEFRY) is also supported. This algorithm is fast on TPU but slow on CPU/GPU compared to Philox."
   ]
  },
  {
   "cell_type": "raw",
   "id": "enhanced-routine",
   "metadata": {},
   "source": [
    "See paper 'Parallel Random Numbers: As Easy as 1, 2, 3' for more details about these algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floating-implementation",
   "metadata": {},
   "source": [
    "# reference\n",
    "\n",
    "### https://tensorflow.google.cn/guide/random_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operating-arrow",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
